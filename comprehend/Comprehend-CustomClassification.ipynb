{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Comprehend - Custom Text Classification\n",
    "\n",
    "This lab is based off the blog post found [here](https://aws.amazon.com/blogs/machine-learning/building-a-custom-classifier-using-amazon-comprehend/).  This uses some data that has been pre-parsed and split in a public facing S3 bucket.  You will need to update this notebook with your own s3 output location and IAM user policy.\n",
    "\n",
    "Furthermore, we've reduced the number of documents to speed up the classification training time.\n",
    "\n",
    "Please email awsaaron@amazon.com for questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Typically you'll copy data from S3 into the Sagemaker notebook instance, however, in this example we are not really using the power of Sagemaker for custom model training but using the AI Service - Amazon Comprehend.  To use that, we'll need our data in S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "bucket_name = '<enter bucket name here>'\n",
    "prefix = 'NLP.Classification'\n",
    "os.environ[\"AWS_REGION\"] = region\n",
    "role = '<enter_role_here>'\n",
    "\n",
    "print(region)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = 's3://aws-ml-blog/artifacts/comprehend-custom-classification/comprehend-train.csv'\n",
    "testing_data = 's3://aws-ml-blog/artifacts/comprehend-custom-classification/comprehend-test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the data from the public bucket to your local instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {training_data} .\n",
    "!aws s3 cp {testing_data} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(training_data,header=None,names=['class','text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of the dataset, let's downsample it for the purposes of this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the downsampled dataset to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "a['text'] = '\"' + a['text'] + '\"'\n",
    "a.to_csv('limited_dataset.csv',header=None,index=None,quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_dataset_path = 's3://'+bucket_name+'/'+prefix+'/limited_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp limited_dataset.csv {limited_dataset_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classifier\n",
    "Create a custom document classifier, supply the name, location of training data, access role ARN, language, and output S3 bucket location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Instantiate Boto3 SDK:\n",
    "client = boto3.client('comprehend', region_name='us-east-1')\n",
    "classifier_name = '<enter name here>'\n",
    "\n",
    "# Create a document classifier\n",
    "create_response = client.create_document_classifier(\n",
    "      DocumentClassifierName=classifier_name,\n",
    "      DataAccessRoleArn=role,\n",
    "      InputDataConfig={\n",
    "          'S3Uri': limited_dataset.csv,\n",
    "      },\n",
    "      LanguageCode='en',\n",
    "  )\n",
    "\n",
    "print(create_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the status of the custom classifier.  You can run the following cell's multiple times to check the status if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_response = client.describe_document_classifier(\n",
    "    DocumentClassifierArn=create_response['DocumentClassifierArn'])\n",
    "print(\"Describe response: \\n\",describe_response)\n",
    "print()\n",
    "\n",
    "# List all classifiers in account\n",
    "list_response = client.list_document_classifiers()\n",
    "print(\"List response: \\n\", list_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_response['DocumentClassifierProperties']['DocumentClassifierArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions!\n",
    "\n",
    "Once the custom classification model is trained, now you can use if for batch or real-time predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an end point for real time model prediction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create end point\n",
    "response = client.create_endpoint(\n",
    "    EndpointName='my-custom-classification-endpoint2',\n",
    "    ModelArn=describe_response['DocumentClassifierProperties']['DocumentClassifierArn'],\n",
    "    DesiredInferenceUnits=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a222d6c54ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EndpointArn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "response['EndpointArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'After my most recent doctors appointment, I came down with the flu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real-time\n",
    "real_time_response = client.classify_document(\n",
    "    Text=txt,\n",
    "    EndpointArn=response['EndpointArn']\n",
    ")\n",
    "print(real_time_response['Classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try a batch async prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "start_response = client.start_document_classification_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': testing_data,\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': s3_output_bucket\n",
    "    },\n",
    "    DataAccessRoleArn=data_access_arn,\n",
    "    DocumentClassifierArn=describe_response['DocumentClassifierProperties']['DocumentClassifierArn']\n",
    ")\n",
    "\n",
    "print(\"Start response: %s\\n\", start_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the job\n",
    "describe_response = client.describe_document_classification_job(JobId=start_response['JobId'])\n",
    "print(\"Describe response: %s\\n\", describe_response)\n",
    "\n",
    "# List all classification jobs in account\n",
    "list_response = client.list_document_classification_jobs()\n",
    "print(\"List response: %s\\n\", list_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
