{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem:  Predicting remaining useful life for Turbofan engines \n",
    "\n",
    "### Introduction\n",
    "\n",
    "In many real life situations, you will encounter where you do not have labeled data and still you need to detect anomalous activites. This technique walks you through how to learn features, build a model using unlabeled dataset.  We will start by converting to problem from an unsupervised learning problem to a supervised learning problem.\n",
    "\n",
    "The data for this notebook comes from a well known NASA competition in 2008. The dataset was generated via simulation using C-MAPSS. Four different sets simulated under different combinations of operational conditions and fault modes. Recording several sensor channels to characterize fault evolution. The data set was provided by the Prognostics CoE at NASA Ames.  We will use this dataset to learn the behavior of a normal engine.  \n",
    "\n",
    "### Background\n",
    "\n",
    "<img src=\"https://aws-machine-learning-immersion-day.s3.amazonaws.com/resources/engine_failure.gif\" width=\"600\" align=\"center\">\n",
    "<p style=\"text-align: center;\">fan blade containment failure test</p>\n",
    "\n",
    "\n",
    "Predictive maintenance is important for safety systems and systems where unexpected maintenace and downtime impacts the business objectives.  A turbofan engine is both a safety system and one where unexpected downtime impacts the operation of the airline.  \n",
    "\n",
    "For predictive maintenance there are typically 3 approaches, it depends on how much you know about the failures and systems: \n",
    "1. Similarity - Use this approach if your data captures degridation from the healthy state to the failed state.  \n",
    "2. Survival - Use this approach when you only have data from the failure event.\n",
    "3. Degradation - Use this approach when you want the operation of the machine to operate above some limit threshold.\n",
    "\n",
    "### Contact\n",
    "\n",
    "* Aaron Sengstacken\n",
    "* awsaaron@amazon.com\n",
    "\n",
    "### References\n",
    "* [Predictive Maintenance - wikipedia](https://en.wikipedia.org/wiki/Predictive_maintenance)\n",
    "* A. Saxena and K. Goebel (2008). \"Turbofan Engine Degradation Simulation Data Set\", NASA Ames Prognostics Data Repository (http://ti.arc.nasa.gov/project/prognostic-data-repository), NASA Ames Research Center, Moffett Field, CA\n",
    "* https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "%matplotlib inline\n",
    "\n",
    "# general python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import io\n",
    "import os\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score,accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve,cohen_kappa_score\n",
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support)\n",
    "\n",
    "# aws\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import KMeans\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.models import Model\n",
    "\n",
    "RANDOM_SEED = 1234 #used to help randomly select the data points\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data for this notebook comes from a well known NASA competition in 2008.  The dataset was generated via simulation using C-MAPSS. Four different sets simulated under different combinations of operational conditions and fault modes. Recording several sensor channels to characterize fault evolution. The data set was provided by the Prognostics CoE at NASA Ames.\n",
    "\n",
    "The dataset consists of multiple multivariate time series. Each data set is further divided into training and test subsets. Each time series is from a different engine i.e., the data can be considered to be from a fleet of engines of the same type. Each engine starts with different degrees of initial wear and manufacturing variation which is unknown to the user. This wear and variation is considered normal, i.e., it is not considered a fault condition. There are three operational settings that have a substantial effect on engine performance. These settings are also included in the data. The data is contaminated with sensor noise.\n",
    "\n",
    "The engine is operating normally at the start of each time series, and develops a fault at some point during the series. In the training set, the fault grows in magnitude until system failure. In the test set, the time series ends some time prior to system failure. The objective of the competition is to predict the number of remaining operational cycles before failure in the test set, i.e., the number of operational cycles after the last cycle that the engine will continue to operate. Also provided a vector of true Remaining Useful Life (RUL) values for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from URL\n",
    "!wget https://ti.arc.nasa.gov/c/6/ -O CMAPSSData.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y -c conda-forge unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack zip file\n",
    "!unzip -o CMAPSSData.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "session = sagemaker.session.Session()\n",
    "bucket_name = session.default_bucket()\n",
    "bucket = 's3://{}'.format(session.default_bucket())\n",
    "print('default_s3_bucket: {}'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets don't contain any headers so we'll have to manually define those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"unit\",\"cycle\",\"op1\",\"op2\",\"op3\",\"sensor1\",\"sensor2\",\"sensor3\",\"sensor4\",\"sensor5\",\"sensor6\",\"sensor7\",\"sensor8\",\"sensor9\",\"sensor10\",\"sensor11\",\"sensor12\",\"sensor13\",\"sensor14\",\"sensor15\",\"sensor16\",\"sensor17\",\"sensor18\",\"sensor19\",\"sensor20\",\"sensor21\",\"sensor22\",\"sensor23\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_FD001.txt',sep=' ',header=None,names=cols)\n",
    "rul_df = pd.read_csv('RUL_FD001.txt',header=None,names=['rul'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Data')\n",
    "display(train_df.head())\n",
    "print('Remaining Useful Life Data')\n",
    "display(rul_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Data')\n",
    "print(train_df.shape)\n",
    "print('Remaining Useful Life Data')\n",
    "print(rul_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['remaining_cycles']=train_df.groupby('unit')['cycle'].transform(max)-train_df['cycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take Away:** Data was loaded successfully, we have some null values in the last two sensors that we'll need to address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. How many engines are in the dataset?\n",
    "2. How long was each engine run?\n",
    "3. What is the distribution of total cycles on each engine?\n",
    "4. What are the operational settings for the engines and how do they vary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 - How many engines are in the dataset?\n",
    "train_df['unit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 - How long was each engine run for?\n",
    "max_cycles = train_df.groupby('unit')['cycle'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "plt.bar(max_cycles.index,max_cycles.values)\n",
    "plt.xlabel('Engine Unit #')\n",
    "plt.ylabel('Max Cycles')\n",
    "plt.title('Total Cycles Per Engine')\n",
    "plt.xlim((0,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3 - What is the distribution of cycles across engines?\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "plt.hist(max_cycles.values,bins=20)\n",
    "plt.title('Histogram of Max Cycles')\n",
    "plt.xlabel('Number of Cycles')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cycles.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4. What are the operational settings for the engines and how do they vary?\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "plt.hist(train_df['op1'].values,bins=30)\n",
    "plt.title('Histogram of Op Setting #1')\n",
    "plt.xlabel('Op Setting #1')\n",
    "plt.ylabel('Count')\n",
    "train_df['op1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "plt.hist(train_df['op2'].values,bins=30)\n",
    "plt.title('Histogram of Op Setting #2')\n",
    "plt.xlabel('Op Setting #2')\n",
    "plt.ylabel('Count')\n",
    "train_df['op2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "plt.hist(train_df['op3'].values,bins=30)\n",
    "plt.title('Histogram of Op Setting #3')\n",
    "plt.xlabel('Op Setting #3')\n",
    "plt.ylabel('Count')\n",
    "train_df['op3'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take Away** The Op3 setting is constant for all data in the dataset, suggest removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.hist(figsize=(20,20),bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take Away** \n",
    "1. Sensor 1, 5, 6, 10, 16, 18, 19, Op3 have constant values across the dataset, suggest removal.  \n",
    "2. Sensor 22, 23 are missing, suggest removal\n",
    "\n",
    "Note:  Unit and Cycle histograms are not informative since they contain the engine number and cycle (recall from above that max cycle per engine was useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['unit','cycle','op3','sensor1','sensor5','sensor6','sensor10','sensor16','sensor18','sensor19','sensor22','sensor23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(drop_cols,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Target Variable\n",
    "\n",
    "For this task we want to predict that the engine is near failure, before it fails.  To do that we will convert the cycle count column to remaining cycles.  Next, we will add an additonal column that is a binary variable when the engine has X remaining cycles left.  X can be defined by the user.  In this example we will use 10 remaining cycles as the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_limit = 10\n",
    "train_df['failed'] = train_df['remaining_cycles'].apply(lambda x: 1 if x <= cycle_limit else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of failed cases in training dataset:  '+str(train_df['failed'].sum()))\n",
    "print('Fraction of total in training dataset:  '+str(train_df['failed'].sum()/train_df['failed'].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Dataset Shape:', train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save off the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('cleaned_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL - Load the saved datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('cleaned_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle / Randomize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1,random_state=RANDOM_SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select only non-failed rows and drop other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.loc[train_df['failed'] == 0]\n",
    "train_x = train_x.drop(['failed','remaining_cycles'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% for the training set and 20% for testing set\n",
    "TEST_PCT = 0.2 # 20% of the data\n",
    "train_x, val_x = train_test_split(train_x, test_size=TEST_PCT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale Data\n",
    "\n",
    "IMPORTANT! - You must apply the same scaling from the training dataset to the testing / validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = scaler.transform(train_x)\n",
    "val = scaler.transform(val_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"./data\", exist_ok = True)\n",
    "\n",
    "# save\n",
    "with open('./data/val.npy', 'wb') as f: np.save(f, val)\n",
    "with open('./data/train.npy', 'wb') as f: np.save(f, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open('./data/val.npy', 'rb') as f: val = np.load(f)\n",
    "with open('./data/train.npy', 'rb') as f: train = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'turbofan-RUL-AE'\n",
    "\n",
    "training_input_path   = sess.upload_data('data/train.npy', key_prefix=prefix+'/training')\n",
    "validation_input_path = sess.upload_data('data/val.npy', key_prefix=prefix+'/validation')\n",
    "\n",
    "print(training_input_path)\n",
    "print(validation_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoEncoders are special kind of neural networks, where your input is 'x' and you have your output as 'x' as well. What this really means is that we are trying to learn a function, where the input and output are the same.\n",
    "\n",
    "Few things to note. \n",
    "\n",
    "- We are reducing the number of nodes, which will force network to learn the features from the dataset. Intuition being that this \"code\" is a set of abstracted features which represents or creates a fingerprint for \"failures\" or a \"non-failure\" activitiy.\n",
    "- Since we are starting with the input 'x', reducing into a abstracted features and then reconstructing back the 'x' means we really don't need a labeled dataset. \n",
    "- The \"code\" is intutively a representation of abstracted features. \n",
    "\n",
    "For our engine dataset, we are going to get all the non-failed data and will try to re-create the same. During this process the network should try to learn a unique representation of what's a non-failed activity. Once the model is trained with whats 'normal' that means anything which does not match this normal representation can be declared as abnormal. \n",
    "\n",
    "For inference, we are going to give both failed and non-failed data to the model. Model prediction will give us the  reconstruction error. This is where we set the threshold which let's domain expert define what tolerance is ok consider normal and when to declare as abnormal data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize turbofan_autoencoder_keras_tf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Tensorflow on the notebook instance (aka 'local mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point='turbofan_autoencoder_keras_tf.py', \n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='local',\n",
    "                          framework_version='1.12', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={'epochs': 2}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Tensorflow on a GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point='turbofan_autoencoder_keras_tf.py', \n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='ml.p3.2xlarge',\n",
    "                          framework_version='1.12', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={\n",
    "                              'epochs': 30,\n",
    "                              'batch-size': 256,\n",
    "                              'learning-rate': 0.001}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot validation and training progress\n",
    "client = boto3.client('logs')\n",
    "BASE_LOG_NAME = '/aws/sagemaker/TrainingJobs'\n",
    "\n",
    "def plot_log(model):\n",
    "    logs = client.describe_log_streams(logGroupName=BASE_LOG_NAME, logStreamNamePrefix=model._current_job_name)\n",
    "    cw_log = client.get_log_events(logGroupName=BASE_LOG_NAME, logStreamName=logs['logStreams'][0]['logStreamName'])\n",
    "\n",
    "    val = []\n",
    "    train = []\n",
    "    iteration = []\n",
    "    count = 0\n",
    "    for e in cw_log['events']:\n",
    "        msg = e['message']\n",
    "        if '/step' in msg:\n",
    "            msg = msg.split(' ')\n",
    "            #print(msg)\n",
    "            train.append(float(msg[-4]))\n",
    "            val.append(float(msg[-1]))\n",
    "            iteration.append(count)\n",
    "            count+=1\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    train_plot,   = ax.plot(iteration,   train,   label='train')\n",
    "    val_plot,   = ax.plot(iteration,   val,   label='validation')\n",
    "    plt.legend(handles=[train_plot,val_plot])\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log(tf_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the trained model with Elastic Inference and Data Capture\n",
    "\n",
    "Pricing for SageMaker by region is found [here](https://aws.amazon.com/sagemaker/pricing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "tf_endpoint_name = 'turbo-fan-RUL-AE-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "s3_capture_upload_path = 's3://{}/{}/monitoring/datacapture'.format(bucket_name, prefix)\n",
    "print(s3_capture_upload_path)\n",
    "\n",
    "#predictor = tf_estimator.deploy(initial_instance_count=1,\n",
    "#                                   instance_type='ml.p2.xlarge')      # $1.125/hour in us-east-1\n",
    "\n",
    "predictor = tf_estimator.deploy(initial_instance_count=1,\n",
    "                         instance_type='ml.c5.large',        # $0.119/hour in us-east-1\n",
    "                         accelerator_type='ml.eia1.medium',  # + $0.168/hour in us-east-1\n",
    "                         endpoint_name=tf_endpoint_name,     # = 67% discount!\n",
    "                         data_capture_config=DataCaptureConfig(\n",
    "                                enable_capture=True,\n",
    "                                sampling_percentage=100,\n",
    "                                destination_s3_uri=s3_capture_upload_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL - Connect to deployed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do you connect to an already deployed endpoint\n",
    "end_point_name = 'ENDPOINT-NAME'\n",
    "predictor = sagemaker.tensorflow.model.TensorFlowPredictor(end_point_name,sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to calculate the mean squared error between predicted and the expected values. This will be our reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single predictions\n",
    "test1 = np.array([ 0.23181664, -1.031415  ,  0.0200345 ,  0.49338209,  1.00451033,\n",
    "       -1.45819216,  0.0041503 ,  0.03854353,  0.54958493, -0.95076123,\n",
    "        0.16660914, -0.18772613,  0.64182997,  0.67217714, -0.82575155,\n",
    "       -0.16161417])\n",
    "\n",
    "result = predictor.predict(test1)\n",
    "print(result)\n",
    "\n",
    "test2 = np.array([ 1.05574627,  1.35571432, -0.57110458,  0.74633819,  0.67483162,\n",
    "       -0.51925459,  0.77610555, -0.76845284, -0.2522413 , -0.04313786,\n",
    "        0.31915843, -0.43000929,  0.683185  ,  0.67217714,  0.20974389,\n",
    "        0.09380091])\n",
    "\n",
    "result = predictor.predict(test2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's list the data capture files stored in S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred.\n",
    "\n",
    "**Note that the delivery of capture data to Amazon S3 can require a couple of minutes so next cell might error. If this happens, please retry after a minute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}/monitoring/datacapture/{}'.format(prefix, tf_endpoint_name)\n",
    "\n",
    "result = s3_client.list_objects(Bucket=bucket_name, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = ['s3://{0}/{1}'.format(bucket_name, capture_file.get(\"Key\")) for capture_file in result.get('Contents')]\n",
    "\n",
    "print(\"Capture Files: \")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read the contents of one of these files and see how capture records are organized in JSON lines format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {capture_files[0]} datacapture/captured_data_example.jsonl\n",
    "\n",
    "import json\n",
    "with open (\"datacapture/captured_data_example.jsonl\", \"r\") as myfile:\n",
    "    data=myfile.read()\n",
    "\n",
    "print(json.dumps(json.loads(data.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each inference request, we get input data, output data and some metadata like the inference time captured and saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruction error without failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the validation data through the trained model\n",
    "y_pred_val = predictor.predict(val)['predictions']\n",
    "mse = np.mean(np.power(val - y_pred_val, 2), axis=1)\n",
    "\n",
    "error_df_val = pd.DataFrame({'reconstruction_error': mse,'true_class': np.zeros(len(mse))})\n",
    "error_df_val.describe(percentiles=[.50,.90,.95,.99,.999,.9999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruction error with failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the failed examples through the trained model\n",
    "train_fail = train_df.loc[train_df['failed'] == 1]\n",
    "train_fail = train_fail.drop(['failed','remaining_cycles'], axis=1)\n",
    "train_fail_scaled = scaler.transform(train_fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(train_fail_scaled)['predictions']\n",
    "mse = np.mean(np.power(train_fail_scaled - y_pred, 2), axis=1)\n",
    "\n",
    "error_df_val_fail = pd.DataFrame({'reconstruction_error': mse,'true_class': np.ones(len(mse))})\n",
    "error_df_val_fail.describe(percentiles=[.50,.90,.95,.99,.999,.9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.concat([error_df_val,error_df_val_fail],ignore_index=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "_ = ax.hist(val_df[val_df['true_class']==0]['reconstruction_error'].values, bins=20,density=True,color='blue',edgecolor='black',alpha=0.5,label='normal')\n",
    "_ = ax.hist(val_df[val_df['true_class']==1]['reconstruction_error'], bins=50,density=True,color='red',edgecolor='black',alpha=0.5,label='failed')\n",
    "plt.legend()\n",
    "plt.xlabel('reconstruction error, MSE')\n",
    "plt.ylabel('normalized count')\n",
    "#plt.ylim((0,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(val_df.true_class, val_df.reconstruction_error)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.001, 1])\n",
    "plt.ylim([0, 1.001])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, th = precision_recall_curve(val_df.true_class, val_df.reconstruction_error)\n",
    "plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n",
    "plt.title('Recall vs Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\n",
    "plt.title('Precision for different threshold values')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')\n",
    "plt.title('Recall for different threshold values')\n",
    "plt.xlabel('Reconstruction error')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = val_df.groupby('true_class')\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
    "            label= \"Fail\" if name == 1 else \"Normal\")\n",
    "ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "ax.legend()\n",
    "plt.title(\"Reconstruction error for different classes\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define thresholds\n",
    "thresholds = np.arange(0, 2, 0.001)\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(val_df['true_class'], to_labels(val_df.reconstruction_error, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = np.argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.965\n",
    "pd.crosstab(index=val_df['true_class'], columns=to_labels(val_df.reconstruction_error,t), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Monitor \n",
    "\n",
    "The generic steps for configuring the Model Monitor are:\n",
    "\n",
    "1.  Deploy a model with data capture enabled\n",
    "2.  Make predictions on the deployed model\n",
    "3.  Create a baseline\n",
    "4.  Create a monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "From our validation dataset let's ask Amazon SageMaker to suggest a set of baseline constraints and generate descriptive statistics for our features. Note that we are using the validation dataset for this workshop to make sure baselining time is short, and that file extension needs to be changed since the baselining jobs require .CSV file extension as default.\n",
    "\n",
    "In reality, you might be willing to use a larger dataset as baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "df = pd.DataFrame(val)\n",
    "df.to_csv('val.csv',index=False)\n",
    "baseline_path = sess.upload_data('val.csv', key_prefix=prefix+'/monitoring/baselining/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data_path = 's3://{0}/{1}/monitoring/baselining/data'.format(bucket_name, prefix)\n",
    "baseline_results_path = 's3://{0}/{1}/monitoring/baselining/results'.format(bucket_name, prefix)\n",
    "\n",
    "print(baseline_data_path)\n",
    "print(baseline_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that running the baselining job will require 8-10 minutes. In the meantime, you can take a look at the Deequ library, used to execute these analyses with the default Model Monitor container: https://github.com/awslabs/deequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.4xlarge',\n",
    "    volume_size_in_gb=5,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_path,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_path,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the statistics that were generated by the baselining job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can also visualize the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The baselining job has inspected the validation dataset and generated constraints and statistics, that will be used to monitor our endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor\n",
    "\n",
    "Once we have built the baseline for our data, we can enable endpoint monitoring by creating a monitoring schedule.\n",
    "When the schedule fires, a monitoring job will be kicked-off and will inspect the data captured at the endpoint with respect to the baseline; then it will generate some report files that can be used to analyze monitoring results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Monitoring Schedule - DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "endpoint_name = predictor.endpoint\n",
    "\n",
    "mon_schedule_name = 'turbofan-RUL-AE-monitor' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=endpoint_name,\n",
    "    output_s3_uri=reports_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Monitoring Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "desc_schedule_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Monitoring Schedule\n",
    "\n",
    "Once the schedule is created, it will kick of jobs at specified intervals. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. \n",
    "You might have to wait till you cross the hour boundary (in UTC) to see executions kick off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is just for the purpose of running this example.\n",
    "my_default_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is produced by the Monitor Job?  \n",
    "Since we won't have time today to feed the endpoint with predictions and see the monitor execute.  We've included some example files to show what the monitor outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://aws-machine-learning-immersion-day/resources/statistics.json ./data/statistics.json\n",
    "!aws s3 cp s3://aws-machine-learning-immersion-day/resources/constraints.json ./data/constraints.json\n",
    "!aws s3 cp s3://aws-machine-learning-immersion-day/resources/constraint_violations.json ./data/constraint_violations.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "file = open('./data/constraint_violations.json', 'r')\n",
    "data = file.read()\n",
    "\n",
    "violations_df = pd.json_normalize(json.loads(data)['violations'])\n",
    "violations_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be asking yourself what are the type of violations that are monitored and how drift from the baseline is computed.\n",
    "\n",
    "The types of violations monitored are listed here: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-violations.html. Most of them use configurable thresholds, that are specified in the monitoring configuration section of the baseline constraints JSON. Let's take a look at this configuration from the baseline constraints file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open (\"./data/constraints.json\", \"r\") as myfile:\n",
    "    data=myfile.read()\n",
    "\n",
    "print(json.dumps(json.loads(data)['monitoring_config'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration is intepreted when the monitoring job is executed and used to compare captured data to the baseline. If you want to customize this section, you will have to update the **constraints.json** file and upload it back to Amazon S3 before launching the monitoring job.\n",
    "\n",
    "When data distributions are compared to detect potential drift, you can choose to use either a _Simple_ or _Robust_ comparison method, where the latter has to be preferred when dealing with small datasets. Additional info: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up\n",
    "\n",
    "If you are done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
