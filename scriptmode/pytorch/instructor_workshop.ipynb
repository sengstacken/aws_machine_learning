{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Models with PyTorch - Advanced Amazon SageMaker Workshop\n",
    "\n",
    "Although the built-in algorithms provided by Amazon SageMaker are useful, sometimes it is necessary to write custom algorithms. In this workshop, you will learn how to write custom algorithms while taking advantage of all the features offered by Amazon SageMaker, including training jobs, hyperparameter tuning jobs, and debugger. This workshop is just a primer meant to get you started; there are too many features to cover in a couple of hours. \n",
    "\n",
    "Amazon SageMaker supports many major deep learning frameworks, including TensorFlow, PyTorch, and Apache MXNet. In this workshop, you will build a custom model in PyTorch, but there are plenty of resources available for the other frameworks.\n",
    "\n",
    "**Contents**\n",
    "1. [Create a PyTorch script](#script)\n",
    "1. [Transform script for Amazon SageMaker](#transform)\n",
    " 1. [Add environment variables](#vars)\n",
    " 1. [Enable hyperparameter tracking](#hyperparams)\n",
    " 1. [Add logging](#logging)\n",
    "1. [Running the script with Amazon SageMaker](#run)\n",
    " 1. [Setup](#setup)\n",
    " 1. [Training](#train)\n",
    " 1. [Hyperparameter tuning](#tune)\n",
    " 1. [Deploy](#deploy)\n",
    "1. [Resources](#iwantmore)\n",
    "\n",
    "><font color=red>Please note:</font> This workshop assumes that you have a basic knowledge of using deep learning frameworks, like PyTorch, as well as a basic knowledge of Amazon SageMaker. Please consider completing a [basic Amazon SageMaker workshop](https://github.com/moose-in-australia/amazon-sagemaker-direct-marketing-workshop) before commencing with this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a PyTorch script <a class=\"anchor\" id=\"script\"></a>\n",
    "\n",
    "Let's get started by grabbing one of the example scripts from the official PyTorch repository. We will use the [MNIST example](https://github.com/pytorch/examples/tree/master/mnist) to keep things simple. If you are not familiar with the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), it is a collection of images of handwritten digits (0 - 9) popular for deep learning tutorials. It could be seen as the \"Hello, World\" of deep learning.\n",
    "\n",
    "<img src=\"img/mnist.png\" alt=\"MNIST data\" width=\"300\"/>\n",
    "\n",
    "We will start with the original PyTorch script and modify it to work with Amazon SageMaker. First, take a look at the original script. A copy of it has already been placed in the `scripts` folder contained in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlr_scheduler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StepLR\n",
      "\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33mThis is the original MNIST example code provided by PyTorch, copied in its\u001b[39;49;00m\n",
      "\u001b[33mentirety for the purpose of an Amazon SageMaker workshop. The original code\u001b[39;49;00m\n",
      "\u001b[33mcan be found here: https://github.com/pytorch/examples/blob/master/mnist/main.py\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
      "        \u001b[36mself\u001b[39;49;00m.conv1 = nn.Conv2d(\u001b[34m1\u001b[39;49;00m, \u001b[34m32\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2 = nn.Conv2d(\u001b[34m32\u001b[39;49;00m, \u001b[34m64\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.dropout1 = nn.Dropout2d(\u001b[34m0.25\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.dropout2 = nn.Dropout2d(\u001b[34m0.5\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.fc1 = nn.Linear(\u001b[34m9216\u001b[39;49;00m, \u001b[34m128\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.fc2 = nn.Linear(\u001b[34m128\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\n",
      "        x = \u001b[36mself\u001b[39;49;00m.conv1(x)\n",
      "        x = F.relu(x)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.conv2(x)\n",
      "        x = F.max_pool2d(x, \u001b[34m2\u001b[39;49;00m)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.dropout1(x)\n",
      "        x = torch.flatten(x, \u001b[34m1\u001b[39;49;00m)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.fc1(x)\n",
      "        x = F.relu(x)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.dropout2(x)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.fc2(x)\n",
      "        output = F.log_softmax(x, dim=\u001b[34m1\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m output\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args, model, device, train_loader, optimizer, epoch):\n",
      "    model.train()\n",
      "    \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\n",
      "        data, target = data.to(device), target.to(device)\n",
      "        optimizer.zero_grad()\n",
      "        output = model(data)\n",
      "        loss = F.nll_loss(output, target)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)]\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mLoss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "                epoch, batch_idx * \u001b[36mlen\u001b[39;49;00m(data), \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
      "                \u001b[34m100.\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader), loss.item()))\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(args, model, device, test_loader):\n",
      "    model.eval()\n",
      "    test_loss = \u001b[34m0\u001b[39;49;00m\n",
      "    correct = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m test_loader:\n",
      "            data, target = data.to(device), target.to(device)\n",
      "            output = model(data)\n",
      "            test_loss += F.nll_loss(output, target, reduction=\u001b[33m'\u001b[39;49;00m\u001b[33msum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).item()  \u001b[37m# sum up batch loss\u001b[39;49;00m\n",
      "            pred = output.argmax(dim=\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[34mTrue\u001b[39;49;00m)  \u001b[37m# get the index of the max log-probability\u001b[39;49;00m\n",
      "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
      "\n",
      "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mTest set: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m, Accuracy: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "        test_loss, correct, \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "        \u001b[34m100.\u001b[39;49;00m * correct / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)))\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\n",
      "    \u001b[37m# Training settings\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mPyTorch MNIST Example\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1000\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m14\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 14)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m1.0\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 1.0)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--gamma\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.7\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mLearning rate step gamma (default: 0.7)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--no-cuda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mdisables CUDA training\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--save-model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mFor Saving the current Model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    args = parser.parse_args()\n",
      "    use_cuda = \u001b[35mnot\u001b[39;49;00m args.no_cuda \u001b[35mand\u001b[39;49;00m torch.cuda.is_available()\n",
      "\n",
      "    torch.manual_seed(args.seed)\n",
      "\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    kwargs = {\u001b[33m'\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m {}\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        datasets.MNIST(\u001b[33m'\u001b[39;49;00m\u001b[33m../data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, train=\u001b[34mTrue\u001b[39;49;00m, download=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                       transform=transforms.Compose([\n",
      "                           transforms.ToTensor(),\n",
      "                           transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))\n",
      "                       ])),\n",
      "        batch_size=args.batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m, **kwargs)\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        datasets.MNIST(\u001b[33m'\u001b[39;49;00m\u001b[33m../data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, train=\u001b[34mFalse\u001b[39;49;00m, transform=transforms.Compose([\n",
      "                           transforms.ToTensor(),\n",
      "                           transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))\n",
      "                       ])),\n",
      "        batch_size=args.test_batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m, **kwargs)\n",
      "\n",
      "    model = Net().to(device)\n",
      "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
      "\n",
      "    scheduler = StepLR(optimizer, step_size=\u001b[34m1\u001b[39;49;00m, gamma=args.gamma)\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        train(args, model, device, train_loader, optimizer, epoch)\n",
      "        test(args, model, device, test_loader)\n",
      "        scheduler.step()\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.save_model:\n",
      "        torch.save(model.state_dict(), \u001b[33m\"\u001b[39;49;00m\u001b[33mmnist_cnn.pt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "!pygmentize scripts/original_pytorch_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script constructs a simple convolutional neural network, provides training and testing code, defines the hyperparameters as arguments, and loads the MNIST dataset.\n",
    "\n",
    "Let's run the script as-is to see if it works. The code below will run the PyTorch script on the instance running this notebook, which should be an `ml.t2.medium` (best practice is to run notebooks on small instances and only use powerful GPU instances for training). Since this instance does not have a GPU, the script will take a long time to run. We only want to see if it works, so set the epoch to 1 to reduce the time it takes.\n",
    "\n",
    "*While you wait for the script to complete training, please read on and complete the next steps.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "9920512it [00:00, 35007271.72it/s]                                              \n",
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "32768it [00:00, 618531.58it/s]\n",
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "1654784it [00:00, 11362440.35it/s]                                              \n",
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "8192it [00:00, 228475.46it/s]\n",
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.296684\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.748891\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.648853\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.438432\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.491942\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.354500\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.413751\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.226165\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.730089\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.368887\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.301291\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.388217\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.315740\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.248978\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.150645\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.079609\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.091435\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.159394\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.221665\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.145178\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.245527\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.170353\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.281704\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.094932\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.167184\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.084097\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.056939\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.028223\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.238478\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.106528\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.228536\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.117438\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.146099\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.220447\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.111017\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.089025\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.104800\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.082932\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.110739\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.057913\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.099913\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.142150\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.197050\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.055090\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.216063\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.132349\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.069517\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.076694\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.284537\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.068178\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.254370\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.018864\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.134591\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.042003\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.146057\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.073709\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.083271\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.238364\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.166492\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.033673\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.266795\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.150988\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.087859\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.251209\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.097889\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.227575\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.146024\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.213757\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.176779\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.119777\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.042379\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.070881\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.037454\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.156237\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.013530\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.060918\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.074345\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.142018\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.172386\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.050020\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.169532\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.073768\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.062845\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.138867\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.148316\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.235372\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.248503\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.103566\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.048908\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.122974\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.152643\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.112435\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.068692\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.154748\n",
      "\n",
      "Test set: Average loss: 0.0662, Accuracy: 9795/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts/original_pytorch_mnist.py --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform script for Amazon SageMaker <a class=\"anchor\" id=\"transform\"></a>\n",
    "\n",
    "Now, you are going to make changes to this script to make it compatible with Amazon SageMaker. Navigate to the `scripts` folder and duplicate `original_pytorch_mnist.py`, then rename the duplicate to `sagemaker_pytorch_mnist.py`. Open this file and make the changes described in the following sections.\n",
    "\n",
    "### Add environment variables <a class=\"anchor\" id=\"vars\"></a>\n",
    "\n",
    "First, you need to add some additional arguments to the script. By doing so, your script has access to important environment variables from the Amazon SageMaker container. In this case, the only variables you need to add are:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to. These artifacts are uploaded to S3 for model hosting\n",
    "* `SM_CHANNEL_TRAINING`: A string that represents the path to the directory that contains the input data for the training channel.\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "However, there are many variables not covered in this workshop, which can be useful when building custom models. For more information, see the [SageMaker Containers GitHub](https://github.com/aws/sagemaker-containers#important-environment-variables) and the [Amazon SageMaker Python SDK Documentation](https://sagemaker.readthedocs.io/en/stable/index.html).\n",
    "\n",
    "Start by adding the code below in the `main()` method of `sagemaker_pytorch_mnist.py`. Place it below the original argument additions, but before `args = parser.parse_args()`.\n",
    "\n",
    "```\n",
    "# SageMaker Container environment\n",
    "parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "parser.add_argument('--num-gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "```\n",
    "\n",
    "Now that your script has access to the environment variables, you need to edit the script to use them. Starting with the number of GPUs available to the host. Remove the following argument from the script, it won't be needed anymore:\n",
    "\n",
    "```\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "```\n",
    "\n",
    "Now replace\n",
    "\n",
    "```\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "```\n",
    "\n",
    "with\n",
    "\n",
    "```\n",
    "use_cuda = args.num_gpus > 0\n",
    "```\n",
    "\n",
    "to make use of the new environment variable.\n",
    "\n",
    "Next, you need to make sure the script fetches the data from the data directory specified in our SageMaker environment, instead of downloading it to the instance running this notebook. \n",
    "\n",
    "In this section of the code\n",
    "\n",
    "```\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "```\n",
    "\n",
    "replace the two occurrences of `'../data'` with `args.data_dir`.\n",
    "\n",
    "Similarly, you want to change the script so the model is saved in the model directory specified in our SageMaker environment, instead of saving it to the instance running this notebook.\n",
    "\n",
    "Replace\n",
    "\n",
    "```\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "```\n",
    "\n",
    "with\n",
    "\n",
    "```\n",
    "    if args.save_model:\n",
    "        model_path = os.path.join(args.model_dir, \"mnist_cnn.pt\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "```\n",
    "\n",
    "and don't forget to add `import os` to the top of the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable hyperparameter tracking <a class=\"anchor\" id=\"hyperparams\"></a>\n",
    "\n",
    "This part is super easy, because you don't have to make any changes! You want SageMaker to know which hyperparameters are used by the script, so you can define these in the training jobs and hyperparameter tuning jobs. To do this, you must add the hyperparameters as arguments to `ArgumentParser` in the `main()` method. Fortunately, this was already done in the original PyTorch script, hence there is nothing additional to be done for SageMaker to track these hyperparameters.\n",
    "\n",
    "### Add logging <a class=\"anchor\" id=\"logging\"></a>\n",
    "\n",
    "SageMaker writes the logs of its training and hyperparameter tuning jobs to Amazon CloudWatch. Fortunately, this also does not require any changes to our original script. All of the `print()` statements in the original script, along with some additional startup information, will be automatically read by SageMaker and printed in the logs. Similarly, you can use the Python `logging` module instead of `print()` statements - both will work in the same way.\n",
    "\n",
    "That's it! This is all you need to run your custom PyTorch algorithms using Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the script with Amazon SageMaker <a class=\"anchor\" id=\"run\"></a>\n",
    "\n",
    "Assuming you made all the changes correctly, we should now be able to use `sagemaker_pytorch_mnist.py` to run the full training on a GPU instance using a SageMaker training job. In this workshop, we store the script on the instance running this notebook, but SageMaker can also fetch these scripts from code repositories, where they would normally be stored in production.\n",
    "\n",
    "### Setup <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "First, we need to set up our execution role, session, and S3 bucket. \n",
    "\n",
    "*If you used the CloudFormation template to create the resources for this workshop in your account, or if you are running this notebook as part of an AWS-hosted workshop, an S3 bucket has already been created in your account. It will likely have a name similar to `mod-6fb15f3e965040ba-databucket-rrz2gpjzg623`. Copy and paste this name in the code below.*\n",
    "\n",
    "*If you do not have an existing S3 bucket in your account for this workshop, use `bucket = sagemaker_session.default_bucket()` to have SageMaker create a bucket for you.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role name: arn:aws:iam::431615879134:role/sagemaker-test-role\n",
      "Bucket name: sagemaker-us-east-1-431615879134\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# Get the role associated with this SageMaker notebook.\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"Role name: {}\".format(role))\n",
    "\n",
    "# Start a session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Specify an S3 bucket for storing the training data.\n",
    "# !ACTION REQUIRED! Replace <TODO> with the name of the S3 bucket created by the CloudFormation template.\n",
    "# If no S3 bucket has been created, use bucket = sagemaker_session.default_bucket()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(\"Bucket name: {}\".format(bucket))\n",
    "\n",
    "# Set a prefix for storing your data - this will look like a folder in the S3 bucket.\n",
    "prefix = 'sagemaker-workshop-pytorch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, the next step would be to download the dataset. However, you already downloaded the dataset when you ran the original PyTorch MNIST script at the start of this workshop. If you check the root folder of this workshop, you should see a `data` folder with the MNIST data. So all we need to do is upload that to the S3 bucket by running the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to: s3://sagemaker-us-east-1-431615879134/sagemaker-workshop-pytorch\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='../data', bucket=bucket, key_prefix=prefix)\n",
    "print('Data uploaded to: {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training <a class=\"anchor\" id=\"train\"></a>\n",
    "\n",
    "Now we can call the SageMaker PyTorch estimator to start a training job. This should look familiar to you if you have used the SageMaker built-in algorithms before. It has similar parameters to specifying the instance type, instance count, role, and job name. \n",
    "\n",
    "However, it also has new parameters which are unfamiliar. Use `entry_point` to tell SageMaker where to find your custom PyTorch script, use `framework_version` to specify the version of the PyTorch container to use and use `py_version` to specify if your code is Python 2 or Python 3. \n",
    "\n",
    "The hyperparameter names should match exactly the names used in `ArgumentParser` in your script. Notice that we use a string instead of a boolean value for `save-model` - this is because the Estimator does not support boolean values.\n",
    "\n",
    "The final part of the process is to define the metrics which you want SageMaker to track. Take a look at the output produced by the original script when you ran it at the start of this workshop. It should look similar to:\n",
    "\n",
    "```\n",
    "...\n",
    "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.161597\n",
    "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.040758\n",
    "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.080115\n",
    "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.104489\n",
    "\n",
    "Test set: Average loss: 0.0602, Accuracy: 9799/10000 (98%)\n",
    "```\n",
    "\n",
    "We can ask SageMaker to track specific values from the output by providing regular expressions which extract the values of interest. In this case, you'll want to track the training loss, training accuracy, validation loss, and validation accuracy. In the code below, we have provided the regular expression for extracting the training loss, but the other three have been left blank for you to define. We recommend using a [regex tool](https://pythex.org/) to test your regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# !ACTION REQUIRED! In the code below, you need to replace the <TODO>'s!\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='scripts/instructor_pytorch_mnist.py',\n",
    "    base_job_name='training-pytorch-mnist',\n",
    "    role=role,\n",
    "    framework_version='1.2.0',\n",
    "    py_version='py3',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p2.xlarge',\n",
    "    hyperparameters={\n",
    "        'batch-size': 64,\n",
    "        'test-batch-size': 1000,\n",
    "        'epochs': 10,\n",
    "        'lr': 1.0,\n",
    "        'gamma': 0.7,\n",
    "        'seed': 1,\n",
    "        'save-model': 'True'\n",
    "    },\n",
    "    metric_definitions=[\n",
    "        {'Name': 'train:loss', 'Regex': 'Train Epoch: .+Loss: (.+)'},\n",
    "        {'Name': 'train:accuracy', 'Regex': 'Train Epoch: .+\\((.+)%\\)'},\n",
    "        {'Name': 'val:loss', 'Regex': 'Test set: Average loss: (.+),'},\n",
    "        {'Name': 'val:accuracy', 'Regex': 'Test set: Average loss: (.+), Accuracy: .+ \\((.+)%\\)'}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're happy with the hyperparameter values and the metric definitions, go ahead and run the training job!\n",
    "\n",
    "><font color=red>Encounter an error?</font>\n",
    "If you encounter an error related to instance types, this likely means that you do not have access to p2 instances on your AWS account. New AWS accounts have limits on the resource types to prevent abuse. If you are using your own account, request a limit increase through the [AWS Support Center](https://console.aws.amazon.com/support/home#/). If this is not your own account, simply run the training on an `ml.c4.xlarge` instance (and reduce the number of epochs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'training': inputs}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training job is running, navigate to SageMaker in the AWS console to see your training job. It will likely show the status 'InProgress'. Feel free to continue with this workshop and check back later to view the results.\n",
    "\n",
    "<img src=\"img/training_job.PNG\" alt=\"Training job in the console\" width=\"800\"/>\n",
    "\n",
    "If you look at the training job more closely, you'll see that the console also lists the values of the hyperparameters, which is great if you need to look up this information at a later time.\n",
    "\n",
    "<img src=\"img/training_job_parameters.PNG\" alt=\"Training job hyperparameters\" width=\"800\"/>\n",
    "\n",
    "Once the training job has finished, SageMaker will also display graphs of the metrics we asked it to track.\n",
    "\n",
    "<img src=\"img/training_job_metrics.PNG\" alt=\"Training job metrics\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning <a class=\"anchor\" id=\"tune\"></a>\n",
    "\n",
    "Similar to running training jobs with custom PyTorch algorithms, you can run hyperparameter tuning jobs. The code below shows you how. Most of this code should look familiar to you if you have used built-in algorithms before. First, you define an estimator, same as for a training job. Then you specify which hyperparameters to tune and their search range. Finally, you define a tuner, with a strategy, an objective, and job settings. \n",
    "\n",
    "Don't forget to add the `metric_definitions` you defined before for the training job. In this example, we choose to find the best values for the learning rate (`lr`) and `gamma`. We also tell SageMaker to tune based on the `val:accuracy` metric which you have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "\n",
    "# !ACTION REQUIRED! In the code below, you need to replace the <TODO>'s!\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='scripts/sagemaker_pytorch_mnist.py',\n",
    "    base_job_name='training-pytorch-mnist',\n",
    "    role=role,\n",
    "    framework_version='1.2.0',\n",
    "    py_version='py3',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p2.xlarge',\n",
    "    hyperparameters={\n",
    "        'batch-size': 64,\n",
    "        'test-batch-size': 1000,\n",
    "        'epochs': 5,\n",
    "        'seed': 1,\n",
    "        'save-model': 'True'\n",
    "    },\n",
    "    metric_definitions=[\n",
    "        {'Name': 'train:loss', 'Regex': 'Train Epoch: .+Loss: (.+)'},\n",
    "        {'Name': 'train:accuracy', 'Regex': 'Train Epoch: .+\\((.+)%\\)'},\n",
    "        {'Name': 'val:loss', 'Regex': 'Test set: Average loss: (.+),'},\n",
    "        {'Name': 'val:accuracy', 'Regex': 'Test set: Average loss: (.+), Accuracy: .+ \\((.+)%\\)'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'lr': ContinuousParameter(0.001, 1.0, scaling_type='Logarithmic'),\n",
    "    'gamma': ContinuousParameter(0.01, 0.9, scaling_type='Auto')\n",
    "}\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name='val:accuracy',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=[\n",
    "        {'Name': 'train:loss', 'Regex': 'Train Epoch: .+Loss: (.+)'},\n",
    "        {'Name': 'train:accuracy', 'Regex': 'Train Epoch: .+\\((.+)%\\)'},\n",
    "        {'Name': 'val:loss', 'Regex': 'Test set: Average loss: (.+),'},\n",
    "        {'Name': 'val:accuracy', 'Regex': 'Test set: Average loss: (.+), Accuracy: .+ \\((.+)%\\)'}\n",
    "    ],\n",
    "    strategy='Bayesian',\n",
    "    objective_type='Maximize',\n",
    "    max_jobs=5,\n",
    "    max_parallel_jobs=3,\n",
    "    base_tuning_job_name='tuning-pytorch-mnist'\n",
    ")\n",
    "\n",
    "tuner.fit(inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the hyperparameter tuning job is running, navigate to SageMaker in the AWS console to see it. It will likely show the status 'InProgress'. Feel free to continue with this workshop and check back later to view the results.\n",
    "\n",
    "><font color=red>Encounter an error?</font>\n",
    "If you encounter an error related to instance types, this likely means that you do not have access to p2 instances on your AWS account. New AWS accounts have limits on the resource types to prevent abuse. If you are using your own account, request a limit increase through the [AWS Support Center](https://console.aws.amazon.com/support/home#/). If this is not your own account, simply run the training on an `ml.c4.xlarge` instance (and reduce the number of epochs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy <a class=\"anchor\" id=\"deploy\"></a>\n",
    "\n",
    "Finally, let's take a look at how to deploy a custom PyTorch model to an Amazon SageMaker endpoint. The code required to deploy the model could be included in `sagemaker_pytorch_mnist.py`, but in this case we will store it in a separate file called `serve_pytorch_mnist.py` which has already been placed in the `scripts` folder of this workshop. Take a look at the script first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33mThis code is used to run an Amazon SageMaker PyTorch Model\u001b[39;49;00m\n",
      "\u001b[33mServer for a model trained using the MNIST training script.\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
      "        \u001b[36mself\u001b[39;49;00m.conv1 = nn.Conv2d(\u001b[34m1\u001b[39;49;00m, \u001b[34m32\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2 = nn.Conv2d(\u001b[34m32\u001b[39;49;00m, \u001b[34m64\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.dropout1 = nn.Dropout2d(\u001b[34m0.25\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.dropout2 = nn.Dropout2d(\u001b[34m0.5\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.fc1 = nn.Linear(\u001b[34m9216\u001b[39;49;00m, \u001b[34m128\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.fc2 = nn.Linear(\u001b[34m128\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\n",
      "        x = \u001b[36mself\u001b[39;49;00m.conv1(x)\n",
      "        x = F.relu(x)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.conv2(x)\n",
      "        x = F.max_pool2d(x, \u001b[34m2\u001b[39;49;00m)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.dropout1(x)\n",
      "        x = torch.flatten(x, \u001b[34m1\u001b[39;49;00m)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.fc1(x)\n",
      "        x = F.relu(x)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.dropout2(x)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.fc2(x)\n",
      "        output = F.log_softmax(x, dim=\u001b[34m1\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m output\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = torch.nn.DataParallel(Net())\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize scripts/serve_pytorch_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the script includes the same convolutional neural network build as the training script. However, the key method to understand is the `model_fn()` method. Amazon SageMaker model serving defines four methods which you can use to manipulate the behavior of the deployed model:\n",
    "\n",
    "* `model_fn`: Takes a model directory and loads the model artifacts into an estimator object.\n",
    "* `input_fn` (*optional*): Takes request data and deserializes the data into an object for prediction.\n",
    "* `predict_fn` (*optional*): Takes the deserialized request object and performs inference against the loaded model.\n",
    "* `output_fn` (*optional*): Takes the result of prediction and serializes this according to the response content type.\n",
    "\n",
    "These methods can be completely customized based on your needs. The `input_fn`, `predict_fn`, and `output_fn` methods have default implementations in the SageMaker PyTorch model server. You only need to include these methods if you want to modify the default implementation. However, the `model_fn` method must always be defined.\n",
    "\n",
    "When you're happy with the serving script, you can use the code below to set up a SageMaker endpoint. Hopefully, at this point, at least one of your training jobs has finished. You'll need to find the S3 URI of the model artifact for the model you want to deploy. It should look like: `s3://<bucket-name>/.../model.tar.gz`. This URI can be found through the console or through code in this notebook - it is up to you to pick a method, figure out how to find the URI, and insert it in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# !ACTION REQUIRED! In the code below, you need to replace the <TODO>'s!\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data='s3://sagemaker-us-east-1-431615879134/training-pytorch-mnist-2020-11-30-17-21-10-114/output/model.tar.gz',\n",
    "    role=role,\n",
    "    framework_version='1.2.0',\n",
    "    py_version='py3',\n",
    "    entry_point='scripts/serve_pytorch_mnist.py',\n",
    ")\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop will not go through the process of running test data through the endpoint, because this is covered in all basic workshops. However, feel free to write this code yourself as a challenge.\n",
    "\n",
    "Don't forget to delete the endpoint before you finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources <a class=\"anchor\" id=\"iwantmore\"></a>\n",
    "\n",
    "If you are interested in learning more about the advanced features of Amazon SageMaker, below are some recommended resources for development and further learning.\n",
    "\n",
    "**Further Learning**\n",
    "* [Distributed MNIST with PyTorch on Amazon SageMaker](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/pytorch_mnist/mnist.py) - an improved version of the code in this workshop, making use of distributed computing.\n",
    "* [Using the Amazon SageMaker Debugger with PyTorch](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-debugger/pytorch_custom_container) - a tutorial demonstrating how to use the Debugger feature with a custom PyTorch container.\n",
    "* [Amazon SageMaker Debugger and Model Monitor](https://github.com/aws-samples/reinvent2019-aim362-sagemaker-debugger-model-monitor) - a workshop demonstrating how to use the Debugger and Model Monitor features of Amazon SageMaker.\n",
    "\n",
    "**Useful Resources**\n",
    "* [Amazon SageMaker Official Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html)\n",
    "* [AWS SageMaker Blogs](https://aws.amazon.com/blogs/?filtered-posts.q=sagemaker&filtered-posts.q_operator=AND)\n",
    "* [Amazon SageMaker Python SDK Documentation](https://sagemaker.readthedocs.io/en/stable/index.html)\n",
    "* [Using PyTorch with the Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/using_pytorch.html)\n",
    "* [AWS Samples GitHub](https://github.com/aws-samples)\n",
    "* [Amazon SageMaker Examples GitHub](https://github.com/awslabs/amazon-sagemaker-examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
