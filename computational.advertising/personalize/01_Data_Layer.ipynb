{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating and Importing User-Item-Interaction Data <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "In this notebook, you will choose a dataset and prepare it for use with Amazon Personalize.\n",
    "\n",
    "1. [How to Use the Notebook](#usenotebook)\n",
    "1. [Introduction](#intro)\n",
    "1. [Define your Use Case](#usecase)\n",
    "1. [Choose a Dataset or Data Source](#source)\n",
    "1. [Prepare the Interactions Data](#prepare_interactions)\n",
    "1. [Prepare the Item Metadata](#prepare_items)\n",
    "1. [Prepare the User Metadata](#prepare_users)\n",
    "1. [Configure an S3 bucket and an IAM  role](#bucket_role)\n",
    "1. [Create Dataset Group](#group_dataset)\n",
    "1. [Create the Interactions Schema](#interact_schema)\n",
    "1. [Create the Items (Movies) Schema](#items_schema)\n",
    "1. [Create the Users Schema](#users_schema)\n",
    "1. [Import the Interactions Data](#import_interactions)\n",
    "1. [Import the Item Metadata](#import_items)\n",
    "1. [Import the User Metadata](#import_users)\n",
    "\n",
    "## How to Use the Notebook <a class=\"anchor\" id=\"usenotebook\"></a>\n",
    "\n",
    "The code is broken up into cells like the one below. There's a triangular Run button at the top of this page that you can click to execute each cell and move onto the next, or you can press `Shift` + `Enter` while in the cell to execute it and move onto the next one.\n",
    "\n",
    "As a cell is executing you'll notice a line to the side showcase an `*` while the cell is running or it will update to a number to indicate the last cell that completed executing after it has finished exectuting all the code within a cell.\n",
    "\n",
    "Simply follow the instructions below and execute the cells to get started with Amazon Personalize using case optimized recommenders.\n",
    "\n",
    "\n",
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "In Amazon Personalize, you start by creating a dataset group, which is a container for Amazon Personalize components. Your dataset group can be one of the following:\n",
    "\n",
    "A Domain dataset group, where you create preconfigured resources for different business domains and use cases, such as getting recommendations for similar videos (VIDEO_ON_DEMAND domain) or best selling items (ECOMMERCE domain). You choose your business domain, import your data, and create recommenders. You use recommenders in your application to get recommendations.\n",
    "\n",
    "Use a [Domain dataset group](https://docs.aws.amazon.com/personalize/latest/dg/domain-dataset-groups.html) if you have a video on demand or e-commerce application and want Amazon Personalize to find the best configurations for your use cases. If you start with a Domain dataset group, you can also add custom resources such as solutions with solution versions trained with recipes for custom use cases.\n",
    "\n",
    "A [Custom dataset group](https://docs.aws.amazon.com/personalize/latest/dg/custom-dataset-groups.html), where you create configurable resources for custom use cases and batch recommendation workflows. You choose a recipe, train a solution version (model), and deploy the solution version with a campaign. You use a campaign in your application to get recommendations.\n",
    "\n",
    "Use a Custom dataset group if you don't have a video on demand or e-commerce application or want to configure and manage only custom resources, or want to get recommendations in a batch workflow. If you start with a Custom dataset group, you can't associate it with a domain later. Instead, create a new Domain dataset group.\n",
    "\n",
    "You can create and manage Domain dataset groups and Custom dataset groups with the AWS console, the AWS Command Line Interface (AWS CLI), or programmatically with the AWS SDKs.\n",
    "\n",
    "\n",
    "## Define your Use Case <a class=\"anchor\" id=\"usecase\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "There are a few guidelines for scoping a problem suitable for Personalize. We recommend the values below as a starting point, although the [official limits](https://docs.aws.amazon.com/personalize/latest/dg/limits.html) lie a little lower.\n",
    "\n",
    "* Authenticated users\n",
    "* At least 50 unique users\n",
    "* At least 100 unique items\n",
    "* At least 2 dozen interactions for each user \n",
    "\n",
    "Most of the time this is easily attainable, and if you are low in one category, you can often make up for it by having a larger number in another category.\n",
    "\n",
    "The user-item-iteraction data is key for getting started with the service. This means we need to look for use cases that generate that kind of data, a few common examples are:\n",
    "\n",
    "1. Video-on-demand applications\n",
    "1. E-commerce platforms\n",
    "\n",
    "Defining your use-case will inform what data and what type of data you need.\n",
    "\n",
    "In this example we are going to be creating:\n",
    "\n",
    "1. Amazon Personalize VIDEO_ON_DEMAND Domain recommender for the [\"More Like X\"](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO_ON_DEMAND-use-cases.html#more-like-y-use-case) use case.\n",
    "1. Amazon Personalize VIDEO_ON_DEMAND Domain recommender for the [\"Top pics for you\"](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO_ON_DEMAND-use-cases.html#top-picks-use-case) use case.\n",
    "1. Amazon Personalize Custom Campaign for a personalized ranked list of movies, for instance shelf/rail/carousel based on some information (director, location, superhero franchise, etc...) \n",
    "\n",
    "All of these will be created within the same dataset group and with the same input data.\n",
    "\n",
    "The diagram bellow shows an overview of what we will be building in this wokshop.\n",
    "\n",
    "![Workflow](images/image1.png)\n",
    "\n",
    "In this notebook we will be working on the Data Layer, Creatig a Dataset Group and importing the Datasets. \n",
    "\n",
    "\n",
    "## Choose a Dataset or Data Source <a class=\"anchor\" id=\"source\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Regardless of the use case, the algorithms all share a base of learning on user-item-interaction data which is defined by 3 core attributes:\n",
    "\n",
    "1. **UserID** - The user who interacted\n",
    "1. **ItemID** - The item the user interacted with\n",
    "1. **Timestamp** - The time at which the interaction occurred\n",
    "\n",
    "To begin, we are going to use the latest MovieLens dataset, this dataset has over 25 million interactions and a rich collection of metadata for items. There is also a smaller version of this dataset, which can be used to shorten training times, while still incorporating the same capabilities as the full dataset.\n",
    "\n",
    "Generally speaking your data will not arrive in a perfect form for Personalize, and will take some modification to be structured correctly. This notebook guides you through all of that. \n",
    "\n",
    "Set USE_FULL_MOVIELENS to True to use the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FULL_MOVIELENS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you will download the dataset from the Movielens website and unzip it in a new folder using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘poc_data’: File exists\n",
      "--2022-08-16 14:10:29--  http://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 261978986 (250M) [application/zip]\n",
      "Saving to: ‘ml-25m.zip.3’\n",
      "\n",
      "ml-25m.zip.3        100%[===================>] 249.84M  76.4MB/s    in 3.5s    \n",
      "\n",
      "2022-08-16 14:10:35 (71.3 MB/s) - ‘ml-25m.zip.3’ saved [261978986/261978986]\n",
      "\n",
      "Archive:  ml-25m.zip\n",
      "  inflating: ml-25m/tags.csv         \n",
      "  inflating: ml-25m/links.csv        \n",
      "  inflating: ml-25m/README.txt       \n",
      "  inflating: ml-25m/ratings.csv      \n",
      "  inflating: ml-25m/genome-tags.csv  \n",
      "  inflating: ml-25m/genome-scores.csv  \n",
      "  inflating: ml-25m/movies.csv       \n"
     ]
    }
   ],
   "source": [
    "data_dir = \"poc_data\"\n",
    "!mkdir $data_dir\n",
    "\n",
    "if not USE_FULL_MOVIELENS:\n",
    "    !cd $data_dir && wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "    !cd $data_dir && unzip ml-latest-small.zip\n",
    "    dataset_dir = data_dir + \"/ml-latest-small/\"\n",
    "else:\n",
    "    !cd $data_dir && wget http://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
    "    !cd $data_dir && unzip ml-25m.zip\n",
    "    dataset_dir = data_dir + \"/ml-25m/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data files you have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.txt\t   genome-tags.csv  movies.csv\t tags.csv\n",
      "genome-scores.csv  links.csv\t    ratings.csv\n"
     ]
    }
   ],
   "source": [
    "!ls $dataset_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At present not much is known except that we have a few CSVs and a readme. Next we will output the readme to learn more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "=======\n",
      "\n",
      "This dataset (ml-25m) describes 5-star rating and free-text tagging activity from [MovieLens](http://movielens.org), a movie recommendation service. It contains 25000095 ratings and 1093360 tag applications across 62423 movies. These data were created by 162541 users between January 09, 1995 and November 21, 2019. This dataset was generated on November 21, 2019.\n",
      "\n",
      "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
      "\n",
      "The data are contained in the files `genome-scores.csv`, `genome-tags.csv`, `links.csv`, `movies.csv`, `ratings.csv` and `tags.csv`. More details about the contents and use of all these files follows.\n",
      "\n",
      "This and other GroupLens data sets are publicly available for download at <http://grouplens.org/datasets/>.\n",
      "\n",
      "\n",
      "Usage License\n",
      "=============\n",
      "\n",
      "Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. The data set may be used for any research purposes under the following conditions:\n",
      "\n",
      "* The user may not state or imply any endorsement from the University of Minnesota or the GroupLens Research Group.\n",
      "* The user must acknowledge the use of the data set in publications resulting from the use of the data set (see below for citation information).\n",
      "* The user may not redistribute the data without separate permission.\n",
      "* The user may not use this information for any commercial or revenue-bearing purposes without first obtaining permission from a faculty member of the GroupLens Research Project at the University of Minnesota.\n",
      "* The executable software scripts are provided \"as is\" without warranty of any kind, either expressed or implied, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose. The entire risk as to the quality and performance of them is with you. Should the program prove defective, you assume the cost of all necessary servicing, repair or correction.\n",
      "\n",
      "In no event shall the University of Minnesota, its affiliates or employees be liable to you for any damages arising out of the use or inability to use these programs (including but not limited to loss of data or data being rendered inaccurate).\n",
      "\n",
      "If you have any further questions or comments, please email <grouplens-info@umn.edu>\n",
      "\n",
      "\n",
      "Citation\n",
      "========\n",
      "\n",
      "To acknowledge use of the dataset in publications, please cite the following paper:\n",
      "\n",
      "> F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. <https://doi.org/10.1145/2827872>\n",
      "\n",
      "\n",
      "Further Information About GroupLens\n",
      "===================================\n",
      "\n",
      "GroupLens is a research group in the Department of Computer Science and Engineering at the University of Minnesota. Since its inception in 1992, GroupLens's research projects have explored a variety of fields including:\n",
      "\n",
      "* recommender systems\n",
      "* online communities\n",
      "* mobile and ubiquitious technologies\n",
      "* digital libraries\n",
      "* local geographic information systems\n",
      "\n",
      "GroupLens Research operates a movie recommender based on collaborative filtering, MovieLens, which is the source of these data. We encourage you to visit <http://movielens.org> to try it out! If you have exciting ideas for experimental work to conduct on MovieLens, send us an email at <grouplens-info@cs.umn.edu> - we are always interested in working with external collaborators.\n",
      "\n",
      "\n",
      "Content and Use of Files\n",
      "========================\n",
      "\n",
      "Verifying the Dataset Contents\n",
      "------------------------------\n",
      "\n",
      "We encourage you to verify that the dataset you have on your computer is identical to the ones hosted at [grouplens.org](http://grouplens.org).  This is an important step if you downloaded the dataset from a location other than [grouplens.org](http://grouplens.org), or if you wish to publish research results based on analysis of the MovieLens dataset.\n",
      "\n",
      "We provide a [MD5 checksum](http://en.wikipedia.org/wiki/Md5sum) with the same name as the downloadable `.zip` file, but with a `.md5` file extension. To verify the dataset:\n",
      "\n",
      "    # on linux\n",
      "    md5sum ml-25m.zip; cat ml-25m.zip.md5\n",
      "\n",
      "    # on OSX\n",
      "    md5 ml-25m.zip; cat ml-25m.zip.md5\n",
      "\n",
      "    # windows users can download a tool from Microsoft (or elsewhere) that verifies MD5 checksums\n",
      "\n",
      "Check that the two lines of output contain the same hash value.\n",
      "\n",
      "\n",
      "Formatting and Encoding\n",
      "-----------------------\n",
      "\n",
      "The dataset files are written as [comma-separated values](http://en.wikipedia.org/wiki/Comma-separated_values) files with a single header row. Columns that contain commas (`,`) are escaped using double-quotes (`\"`). These files are encoded as UTF-8. If accented characters in movie titles or tag values (e.g. Misérables, Les (1995)) display incorrectly, make sure that any program reading the data, such as a text editor, terminal, or script, is configured for UTF-8.\n",
      "\n",
      "\n",
      "User Ids\n",
      "--------\n",
      "\n",
      "MovieLens users were selected at random for inclusion. Their ids have been anonymized. User ids are consistent between `ratings.csv` and `tags.csv` (i.e., the same id refers to the same user across the two files).\n",
      "\n",
      "\n",
      "Movie Ids\n",
      "---------\n",
      "\n",
      "Only movies with at least one rating or tag are included in the dataset. These movie ids are consistent with those used on the MovieLens web site (e.g., id `1` corresponds to the URL <https://movielens.org/movies/1>). Movie ids are consistent between `ratings.csv`, `tags.csv`, `movies.csv`, and `links.csv` (i.e., the same id refers to the same movie across these four data files).\n",
      "\n",
      "\n",
      "Ratings Data File Structure (ratings.csv)\n",
      "-----------------------------------------\n",
      "\n",
      "All ratings are contained in the file `ratings.csv`. Each line of this file after the header row represents one rating of one movie by one user, and has the following format:\n",
      "\n",
      "    userId,movieId,rating,timestamp\n",
      "\n",
      "The lines within this file are ordered first by userId, then, within user, by movieId.\n",
      "\n",
      "Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
      "\n",
      "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
      "\n",
      "\n",
      "Tags Data File Structure (tags.csv)\n",
      "-----------------------------------\n",
      "\n",
      "All tags are contained in the file `tags.csv`. Each line of this file after the header row represents one tag applied to one movie by one user, and has the following format:\n",
      "\n",
      "    userId,movieId,tag,timestamp\n",
      "\n",
      "The lines within this file are ordered first by userId, then, within user, by movieId.\n",
      "\n",
      "Tags are user-generated metadata about movies. Each tag is typically a single word or short phrase. The meaning, value, and purpose of a particular tag is determined by each user.\n",
      "\n",
      "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
      "\n",
      "\n",
      "Movies Data File Structure (movies.csv)\n",
      "---------------------------------------\n",
      "\n",
      "Movie information is contained in the file `movies.csv`. Each line of this file after the header row represents one movie, and has the following format:\n",
      "\n",
      "    movieId,title,genres\n",
      "\n",
      "Movie titles are entered manually or imported from <https://www.themoviedb.org/>, and include the year of release in parentheses. Errors and inconsistencies may exist in these titles.\n",
      "\n",
      "Genres are a pipe-separated list, and are selected from the following:\n",
      "\n",
      "* Action\n",
      "* Adventure\n",
      "* Animation\n",
      "* Children's\n",
      "* Comedy\n",
      "* Crime\n",
      "* Documentary\n",
      "* Drama\n",
      "* Fantasy\n",
      "* Film-Noir\n",
      "* Horror\n",
      "* Musical\n",
      "* Mystery\n",
      "* Romance\n",
      "* Sci-Fi\n",
      "* Thriller\n",
      "* War\n",
      "* Western\n",
      "* (no genres listed)\n",
      "\n",
      "\n",
      "Links Data File Structure (links.csv)\n",
      "---------------------------------------\n",
      "\n",
      "Identifiers that can be used to link to other sources of movie data are contained in the file `links.csv`. Each line of this file after the header row represents one movie, and has the following format:\n",
      "\n",
      "    movieId,imdbId,tmdbId\n",
      "\n",
      "movieId is an identifier for movies used by <https://movielens.org>. E.g., the movie Toy Story has the link <https://movielens.org/movies/1>.\n",
      "\n",
      "imdbId is an identifier for movies used by <http://www.imdb.com>. E.g., the movie Toy Story has the link <http://www.imdb.com/title/tt0114709/>.\n",
      "\n",
      "tmdbId is an identifier for movies used by <https://www.themoviedb.org>. E.g., the movie Toy Story has the link <https://www.themoviedb.org/movie/862>.\n",
      "\n",
      "Use of the resources listed above is subject to the terms of each provider.\n",
      "\n",
      "\n",
      "Tag Genome (genome-scores.csv and genome-tags.csv)\n",
      "-------------------------------------------------\n",
      "\n",
      "This data set includes a current copy of the Tag Genome.\n",
      "\n",
      "[genome-paper]: http://files.grouplens.org/papers/tag_genome.pdf\n",
      "\n",
      "The tag genome is a data structure that contains tag relevance scores for movies.  The structure is a dense matrix: each movie in the genome has a value for *every* tag in the genome.\n",
      "\n",
      "As described in [this article][genome-paper], the tag genome encodes how strongly movies exhibit particular properties represented by tags (atmospheric, thought-provoking, realistic, etc.). The tag genome was computed using a machine learning algorithm on user-contributed content including tags, ratings, and textual reviews.\n",
      "\n",
      "The genome is split into two files.  The file `genome-scores.csv` contains movie-tag relevance data in the following format:\n",
      "\n",
      "    movieId,tagId,relevance\n",
      "\n",
      "The second file, `genome-tags.csv`, provides the tag descriptions for the tag IDs in the genome file, in the following format:\n",
      "\n",
      "    tagId,tag\n",
      "\n",
      "The `tagId` values are generated when the data set is exported, so they may vary from version to version of the MovieLens data sets.\n",
      "\n",
      "Please include the following citation if referencing tag genome data:\n",
      "\n",
      "> Jesse Vig, Shilad Sen, and John Riedl. 2012. The Tag Genome: Encoding Community Knowledge to Support Novel Interaction. ACM Trans. Interact. Intell. Syst. 2, 3: 13:1–13:44. <https://doi.org/10.1145/2362394.2362395>\n",
      "\n",
      "\n",
      "Cross-Validation\n",
      "----------------\n",
      "\n",
      "Prior versions of the MovieLens dataset included either pre-computed cross-folds or scripts to perform this computation. We no longer bundle either of these features with the dataset, since most modern toolkits provide this as a built-in feature. If you wish to learn about standard approaches to cross-fold computation in the context of recommender systems evaluation, see [LensKit](http://lenskit.org) for tools, documentation, and open-source code examples.\n"
     ]
    }
   ],
   "source": [
    "!pygmentize $dataset_dir/README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the README, we see there is a file `ratings.csv` that should work as a proxy for our interactions data, after all rating a film definitely is a form of interacting with it. The dataset also has some genre information as some movie genome data. In this POC we will focus on the interactions and the genre data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Interactions data <a class=\"anchor\" id=\"prepare_interactions\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "The next thing to be done is to load the data and confirm the data is in a good state.\n",
    "\n",
    "Python ships with a broad collection of libraries and we need to import those as well as the ones installed to help us like [boto3](https://aws.amazon.com/sdk-for-python/) (AWS SDK for python) and [Pandas](https://pandas.pydata.org/)/[Numpy](https://numpy.org/)  which are core data science tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, open the `ratings.csv` file and take a look at the first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147880044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147868828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147878820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1      296     5.0  1147880044\n",
       "1       1      306     3.5  1147868817\n",
       "2       1      307     5.0  1147868828\n",
       "3       1      665     5.0  1147878820\n",
       "4       1      899     3.5  1147868510"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data = pd.read_csv(dataset_dir + '/ratings.csv')\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000095, 4)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.500010e+07</td>\n",
       "      <td>2.500010e+07</td>\n",
       "      <td>2.500010e+07</td>\n",
       "      <td>2.500010e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.118928e+04</td>\n",
       "      <td>2.138798e+04</td>\n",
       "      <td>3.533854e+00</td>\n",
       "      <td>1.215601e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.679172e+04</td>\n",
       "      <td>3.919886e+04</td>\n",
       "      <td>1.060744e+00</td>\n",
       "      <td>2.268758e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>7.896520e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.051000e+04</td>\n",
       "      <td>1.196000e+03</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.011747e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.091400e+04</td>\n",
       "      <td>2.947000e+03</td>\n",
       "      <td>3.500000e+00</td>\n",
       "      <td>1.198868e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.215570e+05</td>\n",
       "      <td>8.623000e+03</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.447205e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.625410e+05</td>\n",
       "      <td>2.091710e+05</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.574328e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userId       movieId        rating     timestamp\n",
       "count  2.500010e+07  2.500010e+07  2.500010e+07  2.500010e+07\n",
       "mean   8.118928e+04  2.138798e+04  3.533854e+00  1.215601e+09\n",
       "std    4.679172e+04  3.919886e+04  1.060744e+00  2.268758e+08\n",
       "min    1.000000e+00  1.000000e+00  5.000000e-01  7.896520e+08\n",
       "25%    4.051000e+04  1.196000e+03  3.000000e+00  1.011747e+09\n",
       "50%    8.091400e+04  2.947000e+03  3.500000e+00  1.198868e+09\n",
       "75%    1.215570e+05  8.623000e+03  4.000000e+00  1.447205e+09\n",
       "max    1.625410e+05  2.091710e+05  5.000000e+00  1.574328e+09"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we have a good range of values for `userId` and `movieId`. Next, it is always a good idea to confirm the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000095 entries, 0 to 25000094\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   userId     int64  \n",
      " 1   movieId    int64  \n",
      " 2   rating     float64\n",
      " 3   timestamp  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 762.9 MB\n"
     ]
    }
   ],
   "source": [
    "original_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId       False\n",
       "movieId      False\n",
       "rating       False\n",
       "timestamp    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1147878050.0\n",
      "2006-05-17 15:00:50\n"
     ]
    }
   ],
   "source": [
    "arb_time_stamp = original_data.iloc[50]['timestamp']\n",
    "print(arb_time_stamp)\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you can see that there are a total of (25,000,095 for full 100836 for small) entries in the dataset, with 4 columns, and each cell stored as int64 format, with the exception of the rating whihch is a float64.\n",
    "\n",
    "The int64 format is clearly suitable for `userId` and `movieId`. However, we need to dive deeper to understand the timestamps in the data. To use Amazon Personalize, you need to save timestamps in [Unix Epoch](https://en.wikipedia.org/wiki/Unix_time) format.\n",
    "\n",
    "Currently, the timestamp values are not human-readable. So let's grab an arbitrary timestamp value and figure out how to interpret it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a quick sanity check on the transformed dataset by picking an arbitrary timestamp and transforming it to a human-readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This date makes sense as a timestamp, so we can continue formatting the rest of the data. Remember, the data we need is user-item-interaction data, which is `userId`, `movieId`, and `timestamp` in this case. Our dataset has an additional column, `rating`, which can be dropped from the dataset after we have leveraged it to focus on positive interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Interactions Data\n",
    "\n",
    "Since this is a dataset of an explicit feedback movie ratings, it includes movies rated from 1 to 5. We want to include only moves that were \"liked\" by the users, and simulate a dataset of data that would be gathered by a VOD platform. In order to do that, we will filter out all interactions under 2 out of 5, and create two event types: \"Click\" and and \"Watch\". We will then assign all movies rated 2 and above as \"Click\" and movies rated 4 and above as both \"Click\" and \"Watch\". \n",
    "\n",
    "Note that for a real data set you would actually model based on implicit feedback such as clicks, watches and/or explicit feedback such as ratings, likes etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>1147880044</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>1147868817</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>1147868828</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>1147878820</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>1147868510</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId   timestamp EVENT_TYPE\n",
       "0       1      296  1147880044      Watch\n",
       "1       1      306  1147868817      Watch\n",
       "2       1      307  1147868828      Watch\n",
       "3       1      665  1147878820      Watch\n",
       "4       1      899  1147868510      Watch"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watched_df = original_data.copy()\n",
    "watched_df = watched_df[watched_df['rating'] > 3]\n",
    "watched_df = watched_df[['userId', 'movieId', 'timestamp']]\n",
    "watched_df['EVENT_TYPE']='Watch'\n",
    "watched_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>1147880044</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>1147868817</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>1147868828</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>1147878820</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>1147868510</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId   timestamp EVENT_TYPE\n",
       "0       1      296  1147880044      Click\n",
       "1       1      306  1147868817      Click\n",
       "2       1      307  1147868828      Click\n",
       "3       1      665  1147878820      Click\n",
       "4       1      899  1147868510      Click"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicked_df = original_data.copy()\n",
    "clicked_df = clicked_df[clicked_df['rating'] > 1]\n",
    "clicked_df = clicked_df[['userId', 'movieId', 'timestamp']]\n",
    "clicked_df['EVENT_TYPE']='Click'\n",
    "clicked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = clicked_df.copy()\n",
    "interactions_df = interactions_df.append(watched_df)\n",
    "interactions_df.sort_values(\"timestamp\", axis = 0, ascending = True, \n",
    "                 inplace = True, na_position ='last') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39460341 entries, 326810 to 13207877\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Dtype \n",
      "---  ------      ----- \n",
      " 0   userId      int64 \n",
      " 1   movieId     int64 \n",
      " 2   timestamp   int64 \n",
      " 3   EVENT_TYPE  object\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 1.5+ GB\n"
     ]
    }
   ],
   "source": [
    "interactions_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at what the new dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.946034e+07</td>\n",
       "      <td>3.946034e+07</td>\n",
       "      <td>3.946034e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.120744e+04</td>\n",
       "      <td>2.176237e+04</td>\n",
       "      <td>1.222687e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.680316e+04</td>\n",
       "      <td>3.939153e+04</td>\n",
       "      <td>2.252168e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.896520e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.051900e+04</td>\n",
       "      <td>1.198000e+03</td>\n",
       "      <td>1.025804e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.094600e+04</td>\n",
       "      <td>2.959000e+03</td>\n",
       "      <td>1.211223e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.215620e+05</td>\n",
       "      <td>8.798000e+03</td>\n",
       "      <td>1.448994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.625410e+05</td>\n",
       "      <td>2.091710e+05</td>\n",
       "      <td>1.574328e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userId       movieId     timestamp\n",
       "count  3.946034e+07  3.946034e+07  3.946034e+07\n",
       "mean   8.120744e+04  2.176237e+04  1.222687e+09\n",
       "std    4.680316e+04  3.939153e+04  2.252168e+08\n",
       "min    1.000000e+00  1.000000e+00  7.896520e+08\n",
       "25%    4.051900e+04  1.198000e+03  1.025804e+09\n",
       "50%    8.094600e+04  2.959000e+03  1.211223e+09\n",
       "75%    1.215620e+05  8.798000e+03  1.448994e+09\n",
       "max    1.625410e+05  2.091710e+05  1.574328e+09"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manipulating the data, always confirm the data format has not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId         int64\n",
       "movieId        int64\n",
       "timestamp      int64\n",
       "EVENT_TYPE    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Amazon Personalize has default column names for users, items, and timestamp. These default column names are `USER_ID`, `ITEM_ID`, `TIMESTAMP` and `EVENT_VALUE` for the [VIDEO_ON_DEMAND domain dataset](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO-ON-DEMAND-datasets-and-schemas.html). The final modification to the dataset is to replace the existing column headers with the default headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.rename(columns = {'userId':'USER_ID', 'movieId':'ITEM_ID', \n",
    "                              'timestamp':'TIMESTAMP'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! At this point the data is ready to go, and we just need to save it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_filename = \"interactions.csv\"\n",
    "interactions_df.to_csv((data_dir+\"/\"+interactions_filename), index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Item Metadata <a class=\"anchor\" id=\"prepare_items\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "This will allow you to work with filters as well as supporting the [Top Pics for you Domain Recommender](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO_ON_DEMAND-use-cases.html#top-picks-use-case), and complying with the [VIDEO_ON_DEMAND domain dataset and schema requirements](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO-ON-DEMAND-datasets-and-schemas.html#VIDEO-ON-DEMAND-dataset-requirements)..\n",
    "\n",
    "Next we load the data and confirm the data is in a good state.\n",
    "\n",
    "Next, open the `movies.csv` file and take a look at the first rows. This file has information about the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data = pd.read_csv(dataset_dir + '/movies.csv')\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>62423.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>122220.387646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>63264.744844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>82146.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>138022.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>173222.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>209171.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             movieId\n",
       "count   62423.000000\n",
       "mean   122220.387646\n",
       "std     63264.744844\n",
       "min         1.000000\n",
       "25%     82146.500000\n",
       "50%    138022.000000\n",
       "75%    173222.000000\n",
       "max    209171.000000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not really tell us much about the dataset, so we will explore a bit more and look at the raw information. We can see that genres often appear in groups. That is fine for us as Personalize supports this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62423 entries, 0 to 62422\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   movieId  62423 non-null  int64 \n",
      " 1   title    62423 non-null  object\n",
      " 2   genres   62423 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "original_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you can see that there are a total of (62,000+ for full 9742 for small) entries in the dataset, with 3 columns.\n",
    "\n",
    "Lets look for potential data issues. First we will check for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId    0\n",
       "title      0\n",
       "genres     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, we currently have no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty small dataset of just the movieId, title and the list of genres that are applicable to each entry. However there is additional data available in the Movielens dataset. For instance the title includes the year of the movies release. Let's make that another column of metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  year  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  1995  \n",
       "1                   Adventure|Children|Fantasy  1995  \n",
       "2                               Comedy|Romance  1995  \n",
       "3                         Comedy|Drama|Romance  1995  \n",
       "4                                       Comedy  1995  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data['year'] = original_data['title'].str.extract('.*\\((.*)\\).*',expand = False)\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check again for null values, now that we have added a new field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId      0\n",
       "title        0\n",
       "genres       0\n",
       "year       407\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have introduced some null values, this is likely due to something in the orginal data. If we had time, we could investigate the titles that resulted in the null values. However, for this workshop we will drop the null value titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = original_data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets validate that we resololved the data issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId    0\n",
       "title      0\n",
       "genres     0\n",
       "year       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From an item metadata perspective, we only want to include information that is relevant to training a model and/or filtering results, so we will drop the title column, and keep the genre information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>genres</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                                       genres  year\n",
       "0        1  Adventure|Animation|Children|Comedy|Fantasy  1995\n",
       "1        2                   Adventure|Children|Fantasy  1995\n",
       "2        3                               Comedy|Romance  1995\n",
       "3        4                         Comedy|Drama|Romance  1995\n",
       "4        5                                       Comedy  1995"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemmetadata_df = original_data.copy()\n",
    "itemmetadata_df = itemmetadata_df[['movieId', 'genres', 'year']]\n",
    "itemmetadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a new dataframe to help us generate a creation timestamp. If you don’t provide the CREATION_TIMESTAMP for an item, the model infers this information from the interaction dataset and uses the timestamp of the item’s earliest interaction as its corresponding release date. If an item doesn’t have an interaction, its release date is set as the timestamp of the latest interaction in the training set and it is considered a new item. \n",
    "\n",
    "For the current example we are selecting a today's date as the creation timestamp because the actual creation timestamp is unknown. In your use-case, please provide the appropriate creation timestamp for the item. This can be when the item was added to your platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1640995200\n"
     ]
    }
   ],
   "source": [
    "ts = datetime(2022, 1, 1, 0, 0).strftime('%s')\n",
    "print(ts)\n",
    "\n",
    "itemmetadata_df['CREATION_TIMESTAMP'] = ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manipulating the data, always confirm that the data format has not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId                int64\n",
       "genres                object\n",
       "year                  object\n",
       "CREATION_TIMESTAMP    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemmetadata_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Personalize has a default column for `ITEM_ID` that will map to our `movieId`. We will flesh out more information by specifying `GENRE` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemmetadata_df.rename(columns = {'genres':'GENRES', 'movieId':'ITEM_ID', 'year':'YEAR'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>GENRES</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>CREATION_TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>1995</td>\n",
       "      <td>1640995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "      <td>1995</td>\n",
       "      <td>1640995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>1995</td>\n",
       "      <td>1640995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>1995</td>\n",
       "      <td>1640995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>1995</td>\n",
       "      <td>1640995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62418</th>\n",
       "      <td>209157</td>\n",
       "      <td>Drama</td>\n",
       "      <td>2018</td>\n",
       "      <td>1640995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62419</th>\n",
       "      <td>209159</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>2001</td>\n",
       "      <td>1640995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62420</th>\n",
       "      <td>209163</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>2018</td>\n",
       "      <td>1640995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62421</th>\n",
       "      <td>209169</td>\n",
       "      <td>(no genres listed)</td>\n",
       "      <td>2001</td>\n",
       "      <td>1640995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62422</th>\n",
       "      <td>209171</td>\n",
       "      <td>Action|Adventure|Drama</td>\n",
       "      <td>1962</td>\n",
       "      <td>1640995200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62016 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ITEM_ID                                       GENRES  YEAR  \\\n",
       "0            1  Adventure|Animation|Children|Comedy|Fantasy  1995   \n",
       "1            2                   Adventure|Children|Fantasy  1995   \n",
       "2            3                               Comedy|Romance  1995   \n",
       "3            4                         Comedy|Drama|Romance  1995   \n",
       "4            5                                       Comedy  1995   \n",
       "...        ...                                          ...   ...   \n",
       "62418   209157                                        Drama  2018   \n",
       "62419   209159                                  Documentary  2001   \n",
       "62420   209163                                 Comedy|Drama  2018   \n",
       "62421   209169                           (no genres listed)  2001   \n",
       "62422   209171                       Action|Adventure|Drama  1962   \n",
       "\n",
       "      CREATION_TIMESTAMP  \n",
       "0             1640995200  \n",
       "1             1640995200  \n",
       "2             1640995200  \n",
       "3             1640995200  \n",
       "4             1640995200  \n",
       "...                  ...  \n",
       "62418         1640995200  \n",
       "62419         1640995200  \n",
       "62420         1640995200  \n",
       "62421         1640995200  \n",
       "62422         1640995200  \n",
       "\n",
       "[62016 rows x 4 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemmetadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! At this point the item data is ready to go, and we just need to save it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_filename = \"item-meta.csv\"\n",
    "itemmetadata_df.to_csv((data_dir+\"/\"+items_filename), index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the User Metadata <a class=\"anchor\" id=\"prepare_users\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "This will supporting the [Top Pics for you Domain Recommender](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO_ON_DEMAND-use-cases.html#top-picks-use-case), and complying with the [VIDEO_ON_DEMAND domain dataset and schema requirements](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO-ON-DEMAND-datasets-and-schemas.html#VIDEO-ON-DEMAND-dataset-requirements).\n",
    "\n",
    "The dataset does not have any user metadata so we will create a fake metadata field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162518</th>\n",
       "      <td>33489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162519</th>\n",
       "      <td>69896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162520</th>\n",
       "      <td>46776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162521</th>\n",
       "      <td>87707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162522</th>\n",
       "      <td>85523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162523 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        USER_ID\n",
       "0          2262\n",
       "1        102689\n",
       "2        109832\n",
       "3         13424\n",
       "4         42937\n",
       "...         ...\n",
       "162518    33489\n",
       "162519    69896\n",
       "162520    46776\n",
       "162521    87707\n",
       "162522    85523\n",
       "\n",
       "[162523 rows x 1 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique user ids from the interaction dataset\n",
    "\n",
    "user_ids = interactions_df['USER_ID'].unique()\n",
    "user_data = pd.DataFrame()\n",
    "user_data[\"USER_ID\"]= user_ids\n",
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Metadata\n",
    "The current dataset does not contain additiona user information. For this example, we'll randomly assign a gender to the users with equal probablity of male and female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>GENDER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2262</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102689</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109832</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13424</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42937</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162518</th>\n",
       "      <td>33489</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162519</th>\n",
       "      <td>69896</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162520</th>\n",
       "      <td>46776</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162521</th>\n",
       "      <td>87707</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162522</th>\n",
       "      <td>85523</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162523 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        USER_ID  GENDER\n",
       "0          2262    male\n",
       "1        102689    male\n",
       "2        109832    male\n",
       "3         13424    male\n",
       "4         42937  female\n",
       "...         ...     ...\n",
       "162518    33489  female\n",
       "162519    69896  female\n",
       "162520    46776  female\n",
       "162521    87707  female\n",
       "162522    85523  female\n",
       "\n",
       "[162523 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_genders = ['female', 'male']\n",
    "random = np.random.choice(possible_genders, len(user_data.index), p=[0.5, 0.5])\n",
    "user_data[\"GENDER\"] = random\n",
    "user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data as a CSV file\n",
    "users_filename = \"users.csv\"\n",
    "user_data.to_csv((data_dir+\"/\"+users_filename), index=False, float_format='%.0f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure an S3 bucket and an IAM  role <a class=\"anchor\" id=\"bucket_role\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "So far, we have downloaded, manipulated, and saved the data onto the Amazon EBS instance attached to instance running this Jupyter notebook. \n",
    "\n",
    "By default, the Personalize service does not have permission to acccess the data we uploaded into the S3 bucket in our account. In order to grant access to the  Personalize service to read our CSVs, we need to set a Bucket Policy and create an IAM role that the Amazon Personalize service will assume. Let's set all of that up.\n",
    "\n",
    "Use the metadata stored on the instance underlying this Amazon SageMaker notebook, to determine the region it is operating in. If you are using a Jupyter notebook outside of Amazon SageMaker, simply define the region as a string below. The Amazon S3 bucket needs to be in the same region as the Amazon Personalize resources we have been creating so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print('region:', region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3 bucket names are globally unique. To create a unique bucket name, the code below will append the string `personalizepocvod` to your AWS account number. Then it creates a bucket with this name in the region discovered in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket_name: 061703360474-us-east-1-personalizepocvod\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket_name = account_id + \"-\" + region + \"-\" + \"personalizepocvod\"\n",
    "print('bucket_name:', bucket_name)\n",
    "try: \n",
    "    if region == \"us-east-1\":\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "    print(\"Bucket already exists. Using bucket\", bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the S3 bucket policy\n",
    "Amazon Personalize needs to be able to read the contents of your S3 bucket. So add a bucket policy which allows that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'GTHZ3C23RDH28687',\n",
       "  'HostId': 'X4wvjwsqjTSWSj3JYPo3QCGpr9dhGUnEQlLcyEzie2UwUS3FwvnK1w7c0nqI9RHmPhqB6+s+y5s=',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'X4wvjwsqjTSWSj3JYPo3QCGpr9dhGUnEQlLcyEzie2UwUS3FwvnK1w7c0nqI9RHmPhqB6+s+y5s=',\n",
       "   'x-amz-request-id': 'GTHZ3C23RDH28687',\n",
       "   'date': 'Tue, 16 Aug 2022 14:15:03 GMT',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:*Object\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an IAM role\n",
    "\n",
    "Amazon Personalize needs the ability to assume roles in AWS in order to have the permissions to execute certain tasks. Let's create an IAM role and attach the required policies to it. The code below attaches very permissive policies; please use more restrictive policies for any production application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: role already exists: An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name 061703360474-PersonalizeS3-Immersion-Day already exists.\n",
      "IAM Role: arn:aws:iam::061703360474:role/061703360474-PersonalizeS3-Immersion-Day\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = account_id+\"-PersonalizeS3-Immersion-Day\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"personalize.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_role_response = iam.create_role(\n",
    "        RoleName = role_name,\n",
    "        AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    "    );\n",
    "    \n",
    "except iam.exceptions.EntityAlreadyExistsException as e:\n",
    "    print('Warning: role already exists:', e)\n",
    "    create_role_response = iam.get_role(\n",
    "        RoleName = role_name\n",
    "    );\n",
    "\n",
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "    \n",
    "print('IAM Role: {}'.format(role_arn))\n",
    "    \n",
    "attach_response = iam.attach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n",
    ");\n",
    "\n",
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "\n",
    "# Pause to allow role to be fully consistent\n",
    "time.sleep(30)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3\n",
    "\n",
    "Now that your Amazon S3 bucket has been created, upload the CSV file of our user-item-interaction data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_file_path = data_dir + \"/\" + interactions_filename\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(interactions_filename).upload_file(interactions_file_path)\n",
    "\n",
    "items_file_path = data_dir + \"/\" + items_filename\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(items_filename).upload_file(items_file_path)\n",
    "\n",
    "users_file_path = data_dir + \"/\" + users_filename\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(users_filename).upload_file(users_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset Group <a class=\"anchor\" id=\"group_dataset\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "The highest level of isolation and abstraction with Amazon Personalize is a *dataset group*. Information stored within one of these dataset groups has no impact on any other dataset group or models created from one - they are completely isolated. This allows you to run many experiments and is part of how we keep your models private and fully trained only on your data. \n",
    "\n",
    "Before importing the data prepared earlier, there needs to be a dataset group and a dataset added to it that handles the interactions.\n",
    "\n",
    "Dataset groups can house the following types of information:\n",
    "\n",
    "* User-item-interactions\n",
    "* Event streams (real-time interactions)\n",
    "* User metadata\n",
    "* Item metadata\n",
    "\n",
    "We need to create the dataset group that will contain our three datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset Group\n",
    "\n",
    "The following cell will create a new dataset group with the name `personalize-poc-movielens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can communicate with Personalize!\n"
     ]
    }
   ],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')\n",
    "print(\"We can communicate with Personalize!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetGroupArn\": \"arn:aws:personalize:us-east-1:061703360474:dataset-group/personalize-poc-movielens\",\n",
      "  \"domain\": \"VIDEO_ON_DEMAND\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"cc93046f-f555-490d-9bae-5f944c0767e2\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 16 Aug 2022 14:15:54 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"131\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"cc93046f-f555-490d-9bae-5f944c0767e2\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "DatasetGroupArn = arn:aws:personalize:us-east-1:061703360474:dataset-group/personalize-poc-movielens\n"
     ]
    }
   ],
   "source": [
    "create_dataset_group_response = personalize.create_dataset_group(\n",
    "    name = \"personalize-poc-movielens\",\n",
    "    domain='VIDEO_ON_DEMAND'\n",
    ")\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "print(json.dumps(create_dataset_group_response, indent=2))\n",
    "\n",
    "print(f'DatasetGroupArn = {dataset_group_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to Have ACTIVE Status \n",
    "\n",
    "Before we can use the Dataset Group in any items below it must be active. This can take a minute or two. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the dataset group every 60 seconds, up to a maximum of 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGroup: CREATE PENDING\n",
      "DatasetGroup: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a dataset group, you can create a dataset for the interaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Interactions Schema <a class=\"anchor\" id=\"interact_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Now that we've loaded and prepared our three datasets we'll need to configure the Amazon Personalize service to understand our data so that it can be used to train models for generating recommendations.Amazon Personalize requires a schema for each dataset so it can map the columns in our CSVs to fields for model training. Each schema is declared in JSON using the [Apache Avro](https://avro.apache.org/) format. \n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Here, you will create a schema for interactions data, which requires the `USER_ID`, `ITEM_ID`, and `TIMESTAMP` fields. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interactions dataset has three required columns: `ITEM_ID`, `USER_ID`, and `TIMESTAMP`. The `TIMESTAMP` represents when the user interated with an item and must be expressed in Unix timestamp format (seconds). For this dataset we also have an `EVENT_TYPE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You aready created this schema.\n",
      "Using existing schema: arn:aws:personalize:us-east-1:061703360474:schema/personalize-poc-movielens-interactions-schema\n"
     ]
    }
   ],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EVENT_TYPE\", # \"Watch\", \"Click\", etc.\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = \"personalize-poc-movielens-interactions-schema\",\n",
    "        schema = json.dumps(interactions_schema),\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    interactions_schema_arn = create_schema_response['schemaArn']\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You aready created this schema.')\n",
    "    schemas = personalize.list_schemas(maxResults=100)['schemas']\n",
    "    for schema_response in schemas:\n",
    "        if schema_response['name'] == \"personalize-poc-movielens-interactions-schema\":\n",
    "            interactions_schema_arn = schema_response['schemaArn']\n",
    "            print(f\"Using existing schema: {interactions_schema_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Interactions Dataset\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:061703360474:dataset/personalize-poc-movielens/INTERACTIONS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"fff23e67-0085-4594-a332-b986b21f6086\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 16 Aug 2022 14:17:39 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"106\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"fff23e67-0085-4594-a332-b986b21f6086\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"personalize-poc-movielens-interactions\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = interactions_schema_arn\n",
    ")\n",
    "\n",
    "interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Items (Movies) Schema<a class=\"anchor\" id=\"items_schema\"></a>\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Here, you will create a schema for item metadata data, and we define the `ITEM_ID`, `GENRES`, `YEAR`, and `CREATION_TIMESTAMP` fields. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You aready created this schema.\n",
      "Using existing schema: arn:aws:personalize:us-east-1:061703360474:schema/personalize-poc-movielens-items-schema\n"
     ]
    }
   ],
   "source": [
    "items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENRES\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        },{\n",
    "            \"name\": \"YEAR\",\n",
    "            \"type\": \"int\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CREATION_TIMESTAMP\",\n",
    "            \"type\": \"long\",\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "    \n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = \"personalize-poc-movielens-items-schema\",\n",
    "        schema = json.dumps(items_schema),\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "    items_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You aready created this schema.')\n",
    "    schemas = personalize.list_schemas(maxResults=100)['schemas']\n",
    "    for schema_response in schemas:\n",
    "        if schema_response['name'] == \"personalize-poc-movielens-items-schema\":\n",
    "            items_schema_arn = schema_response['schemaArn']\n",
    "            print(f\"Using existing schema: {items_schema_arn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Items Dataset\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:061703360474:dataset/personalize-poc-movielens/ITEMS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"ecf05e5d-fd66-4230-a828-70d9077b87e5\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 16 Aug 2022 14:17:48 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"99\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"ecf05e5d-fd66-4230-a828-70d9077b87e5\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"ITEMS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"personalize-poc-movielens-items\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = items_schema_arn\n",
    ")\n",
    "\n",
    "items_dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Users Schema<a class=\"anchor\" id=\"users_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Here, you will create a schema for user data, which requires the `USER_ID`, and an additonal metadata field, in this case `GENDER`. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You aready created this schema.\n",
      "Using existing schema: arn:aws:personalize:us-east-1:061703360474:schema/personalize-poc-movielens-users-schema\n"
     ]
    }
   ],
   "source": [
    "users_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Users\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENDER\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "    \n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = \"personalize-poc-movielens-users-schema\",\n",
    "        schema = json.dumps(users_schema),\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    users_schema_arn = create_schema_response['schemaArn']\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You aready created this schema.')\n",
    "    schemas = personalize.list_schemas(maxResults=100)['schemas']\n",
    "    for schema_response in schemas:\n",
    "        if schema_response['name'] == \"personalize-poc-movielens-users-schema\":\n",
    "            users_schema_arn = schema_response['schemaArn']\n",
    "            print(f\"Using existing schema: {users_schema_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Users dataset\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:061703360474:dataset/personalize-poc-movielens/USERS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"1c388341-179f-4ffd-9866-fceace1b0059\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 16 Aug 2022 14:17:54 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"99\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"1c388341-179f-4ffd-9866-fceace1b0059\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"USERS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"personalize-poc-movielens-users\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = users_schema_arn\n",
    ")\n",
    "\n",
    "users_dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait untill all the datasets have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions Dataset: ACTIVE\n",
      "Items Dataset: ACTIVE\n",
      "Users Dataset: ACTIVE\n",
      "CPU times: user 10.7 ms, sys: 2.82 ms, total: 13.5 ms\n",
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 6*60*60 # 6 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = interactions_dataset_arn\n",
    "    )\n",
    "    status =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Interactions Dataset: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = items_dataset_arn\n",
    "    )\n",
    "    status =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Items Dataset: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = users_dataset_arn\n",
    "    )\n",
    "    status =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Users Dataset: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the interactions data <a class=\"anchor\" id=\"import_interactions\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, so now you will execute an import job that will load the interactions data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:061703360474:dataset-import-job/personalize-poc-interactions-import\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"e3d9563b-83d1-4656-986d-82bdbae01f53\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 16 Aug 2022 14:18:10 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"123\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"e3d9563b-83d1-4656-986d-82bdbae01f53\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"personalize-poc-interactions-import\",\n",
    "    datasetArn = interactions_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "interactions_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Item Metadata <a class=\"anchor\" id=\"import_items\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, now you will execute an import job that will load the item data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:061703360474:dataset-import-job/personalize-poc-items-import\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"e732261d-a8df-44dc-ae98-6ee65ff0cad6\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 16 Aug 2022 14:18:16 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"116\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"e732261d-a8df-44dc-ae98-6ee65ff0cad6\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"personalize-poc-items-import\",\n",
    "    datasetArn = items_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, items_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "items_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the User Metadata <a class=\"anchor\" id=\"import_users\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, now you will execute an import job that will load the user data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:061703360474:dataset-import-job/personalize-poc-users-import\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"ed5c10f8-4f5e-4bdb-b549-4ac2db425f4a\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 16 Aug 2022 14:18:19 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"116\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"ed5c10f8-4f5e-4bdb-b549-4ac2db425f4a\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"personalize-poc-users-import\",\n",
    "    datasetArn = users_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, users_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "users_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the dataset, the import job must be active. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the import job every minute, up to a maximum of 6 hours.\n",
    "\n",
    "Importing the data can take some time, depending on the size of the dataset. In this workshop, the data import job should take around 15 minutes. While you're waiting you can learn more about Datasets and Schemas in [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html). We need to wait for the data imports to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions DatasetImportJob: CREATE PENDING\n",
      "Interactions DatasetImportJob: CREATE IN_PROGRESS\n",
      "Interactions DatasetImportJob: CREATE IN_PROGRESS\n",
      "Interactions DatasetImportJob: CREATE IN_PROGRESS\n",
      "Interactions DatasetImportJob: ACTIVE\n",
      "Items DatasetImportJob: CREATE IN_PROGRESS\n",
      "Items DatasetImportJob: CREATE IN_PROGRESS\n",
      "Items DatasetImportJob: ACTIVE\n",
      "Users DatasetImportJob: ACTIVE\n",
      "CPU times: user 83.8 ms, sys: 15.2 ms, total: 99 ms\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 6*60*60 # 6 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = interactions_dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"Interactions DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = items_dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"Items DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = users_dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"Users DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all imports now complete you can  start training recommenders and solutions. Run the cell below before moving on to store a few values for usage in the next notebooks. After completing that cell open notebook `02_Training_Layer.ipynb` to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'USE_FULL_MOVIELENS' (bool)\n",
      "Stored 'dataset_dir' (str)\n",
      "Stored 'dataset_group_arn' (str)\n",
      "Stored 'bucket_name' (str)\n",
      "Stored 'role_arn' (str)\n",
      "Stored 'role_name' (str)\n",
      "Stored 'data_dir' (str)\n",
      "Stored 'region' (str)\n",
      "Stored 'interactions_dataset_arn' (str)\n",
      "Stored 'items_dataset_arn' (str)\n",
      "Stored 'users_dataset_arn' (str)\n",
      "Stored 'interactions_schema_arn' (str)\n",
      "Stored 'items_schema_arn' (str)\n",
      "Stored 'users_schema_arn' (str)\n"
     ]
    }
   ],
   "source": [
    "%store USE_FULL_MOVIELENS\n",
    "%store dataset_dir\n",
    "%store dataset_group_arn\n",
    "%store bucket_name\n",
    "%store role_arn\n",
    "%store role_name\n",
    "%store data_dir\n",
    "%store region\n",
    "%store interactions_dataset_arn\n",
    "%store items_dataset_arn\n",
    "%store users_dataset_arn\n",
    "%store interactions_schema_arn\n",
    "%store items_schema_arn\n",
    "%store users_schema_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
