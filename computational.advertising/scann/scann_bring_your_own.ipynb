{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your own TensorFlow - SCANN container\n",
    "\n",
    "With Amazon SageMaker, you can package your own algorithms that can then be trained and deployed in the SageMaker environment. This notebook guides you through an example using TensorFlow that shows you how to build a Docker container for SageMaker and use it for real time inference.\n",
    "\n",
    "## Permissions\n",
    "\n",
    "Running this notebook requires permissions in addition to the normal `SageMakerFullAccess` permissions. This is because it creates new repositories on Amazon ECR. The easiest way to add these permissions is simply to add the managed policy `AmazonEC2ContainerRegistryFullAccess` to the role that you used to start your notebook instance. There's no need to restart your notebook instance when you do this, the new permissions will be available immediately.\n",
    "\n",
    "## The example\n",
    "\n",
    "In this example we show how to package a custom TensorFlow container with a trained SCANN index on the movie lens dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parts of the sample container\n",
    "\n",
    "The `container` directory has all the components you need to package the sample algorithm for `Amazon SageMager`:\n",
    "```\n",
    "    .\n",
    "    |-- Dockerfile\n",
    "    |-- build_and_push.sh\n",
    "    |-- savedscann\n",
    "    |   |-- 1\n",
    "    |       |-- assets\n",
    "    |       |-- variables\n",
    "    |       |-- saved_model.pb\n",
    "    |-- code\n",
    "        |-- nginx.conf\n",
    "        |-- serve\n",
    "\n",
    "```\n",
    "\n",
    "Let's discuss each of these in turn:\n",
    "\n",
    "* __`Dockerfile`__ describes how to build your Docker container image. More details are provided below.\n",
    "* __`build_and_push.sh`__ is a script that uses the `Dockerfile` to build your container images and then pushes it to ECR. We invoke the commands directly later in this notebook, but you can just copy and run the script for your own algorithms.\n",
    "* __`code`__ is the directory which contains the files that are installed in the container.\n",
    "* __`scavedscann`__ is the directory which contains the files that are the uncompressed model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Dockerfile`\n",
    "\n",
    "The `Dockerfile` describes the image that we want to build. You can think of it as describing the complete operating system installation of the system that you want to run. A Docker container running is quite a bit lighter than a full operating system, however, because it takes advantage of Linux on the host machine for the basic operations.\n",
    "\n",
    "For the Python science stack, we start from an official TensorFlow serving scann docker image. Then we add the code that implements our specific algorithm to the container and set up the right environment for it to run under.\n",
    "\n",
    "Let's look at the `Dockerfile` for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# SageMaker TF Inference image\n",
      "FROM google/tf-serving-scann\n",
      "\n",
      "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
      "         wget \\\n",
      "         python3-pip \\\n",
      "         python3-setuptools \\\n",
      "         nginx \\\n",
      "         ca-certificates\n",
      "\n",
      "RUN ln -s /usr/bin/python3 /usr/bin/python\n",
      "RUN ln -s /usr/bin/pip3 /usr/bin/pip\n",
      "ENV PATH=\"/opt/program:${PATH}\"\n",
      "\n",
      "# Set up the program in the image\n",
      "COPY code /opt/ml/code\n",
      "COPY savedscann /opt/ml/model/index\n",
      "WORKDIR /opt/ml/code\n",
      "ENTRYPOINT [\"python\",\"serve\"]\n"
     ]
    }
   ],
   "source": [
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and registering the container\n",
    "\n",
    "The following shell code shows how to build the container image using `docker build` and push the container image to ECR using `docker push`. This code is also available as the shell script `build-and-push.sh`, which you can run as `build-and-push.sh` to build the image `sagemaker-tf-scann-example`. \n",
    "\n",
    "This code looks for an ECR repository in the account you're using and the current default region (if you're using a SageMaker notebook instance, this is the region where the notebook instance was created). If the repository doesn't exist, the script will create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%sh\n",
      "\n",
      "# The name of our algorithm\n",
      "algorithm_name=sagemaker-tf-scann-example\n",
      "\n",
      "chmod +x code/train\n",
      "chmod +x code/serve\n",
      "\n",
      "account=$(aws sts get-caller-identity --query Account --output text)\n",
      "\n",
      "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
      "region=$(aws configure get region)\n",
      "region=${region:-us-west-2}\n",
      "\n",
      "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
      "\n",
      "# If the repository doesn't exist in ECR, create it.\n",
      "\n",
      "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
      "\n",
      "if [ $? -ne 0 ]\n",
      "then\n",
      "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
      "fi\n",
      "\n",
      "# Get the login command from ECR and execute it directly\n",
      "$(aws ecr get-login --region ${region} --no-include-email)\n",
      "\n",
      "# Build the docker image locally with the image name and then push it to ECR\n",
      "# with the full name.\n",
      "\n",
      "docker build  -t ${algorithm_name} .\n",
      "docker tag ${algorithm_name} ${fullname}\n",
      "\n",
      "docker push ${fullname}"
     ]
    }
   ],
   "source": [
    "!cat build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting your Algorithm in Amazon SageMaker\n",
    "Once you have your container packaged, you can use it to serve models. Let's do that with the algorithm we made above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "ecr_image = '431615879134.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tf-scann-example:latest'\n",
    "instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "scann_model = Model(\n",
    "    role=role,\n",
    "    image_uri=ecr_image,\n",
    ")\n",
    "\n",
    "\n",
    "predictor = scann_model.deploy(1, instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-tf-scann-example-2022-10-31-20-01-30-897'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scann_model.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ContentType\": \"application/json\",\n",
      "    \"InvokedProductionVariant\": \"AllTraffic\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws sagemaker-runtime invoke-endpoint \\\n",
    "--endpoint-name sagemaker-tf-scann-example-2022-10-31-20-01-30-897 \\\n",
    "--body '{\"instances\": [\"42\",\"43\"]}' prediction_response.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predictions\": [\n",
      "        {\n",
      "            \"output_1\": [2.19959259, 2.11500525, 1.87805462, 1.87397075, 1.82147372, 1.81811309, 1.77092397, 1.76034117, 1.75835824, 1.7510314],\n",
      "            \"output_2\": [\"Kid in King Arthur's Court, A (1995)\", \"Homeward Bound: The Incredible Journey (1993)\", \"Cool Runnings (1993)\", \"Angels in the Outfield (1994)\", \"Bridges of Madison County, The (1995)\", \"Aristocats, The (1970)\", \"Winnie the Pooh and the Blustery Day (1968)\", \"Lion King, The (1994)\", \"Jungle Book, The (1994)\", \"D3: The Mighty Ducks (1996)\"]\n",
      "        },\n",
      "        {\n",
      "            \"output_1\": [1.78076386, 1.69247246, 1.68209136, 1.65398324, 1.64403212, 1.59631383, 1.49829984, 1.49201, 1.47684073, 1.45613444],\n",
      "            \"output_2\": [\"Now and Then (1995)\", \"Cool Runnings (1993)\", \"Only You (1994)\", \"Little Women (1994)\", \"Corrina, Corrina (1994)\", \"Fox and the Hound, The (1981)\", \"Affair to Remember, An (1957)\", \"Walk in the Clouds, A (1995)\", \"While You Were Sleeping (1995)\", \"Homeward Bound: The Incredible Journey (1993)\"]\n",
      "        }\n",
      "    ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat prediction_response.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional cleanup\n",
    "When you're done with the endpoint, you should clean it up.\n",
    "\n",
    "All the training jobs, models and endpoints we created can be viewed through the SageMaker console of your AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [How Amazon SageMaker interacts with your Docker container for training](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)\n",
    "- [How Amazon SageMaker interacts with your Docker container for inference](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html)\n",
    "- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk)\n",
    "- [`Dockerfile`](https://docs.docker.com/engine/reference/builder/)\n",
    "- [scikit-bring-your-own](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
