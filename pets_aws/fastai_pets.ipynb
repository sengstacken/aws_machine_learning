{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastai: lesson 1 - Pets example with Amazon SageMaker\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "This notebook shows how to use the SageMaker Python SDK to run your fastai library based model in a local container before deploying to SageMaker's managed training or hosting environments.  This can speed up iterative testing and debugging while using the same familiar Python SDK interface.  Just change your estimator's `train_instance_type` to `local`. \n",
    "\n",
    "In order to use this feature you'll need to install docker-compose (and nvidia-docker if training with a GPU).\n",
    "\n",
    "**Note, you can only run a single local notebook at one time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import subprocess\n",
    "\n",
    "import PIL\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.predictor import RealTimePredictor, json_deserializer\n",
    "\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **SageMaker Python SDK** helps you deploy your models for training and hosting in optimized, productions ready containers in SageMaker. The SageMaker Python SDK is easy to use, modular, extensible and compatible with TensorFlow, MXNet, PyTorch and Chainer. This tutorial focuses on how to create a convolutional neural network model to train the [Oxford IIIT Pet dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) as per [Lesson 1 of the fast.ai MOOC course](https://course.fast.ai/videos/?lesson=1) using **PyTorch in local mode**.\n",
    "\n",
    "### Set up the environment\n",
    "\n",
    "To setup a new SageMaker notebook instance with fastai library installed then follow steps outlined [here](https://course.fast.ai/start_sagemaker.html).\n",
    "\n",
    "This notebook was created and tested on a single ml.p3.2xlarge notebook instance. \n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test your training or hosting of your fastai model then run the following cell to update the Docker daemon default shared memory to 2gb. Only run this command if you are using the `ml.p3.2xlarge` instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo cp daemon.json /etc/docker/daemon.json && sudo pkill -SIGHUP dockerd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-fastai-pets'\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Oxford Pets dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download the dataset and save locally on our notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/.fastai/data/oxford-iiit-pet')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.PETS); path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ec2-user/.fastai/data/oxford-iiit-pet/images/english_setter_100.jpg'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/oxford-iiit-pet/images/British_Shorthair_88.jpg'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/oxford-iiit-pet/images/Abyssinian_43.jpg'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/oxford-iiit-pet/images/beagle_198.jpg'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/oxford-iiit-pet/images/great_pyrenees_133.jpg')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_anno = path/'annotations'\n",
    "path_img = path/'images'\n",
    "\n",
    "fnames = get_image_files(path_img)\n",
    "fnames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-east-1-431615879134/sagemaker/DEMO-fastai-pets\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=path, bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for training and inference\n",
    "Here is the full code that both trains the model and does model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrequests\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mfastai.vision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\n",
      "\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "\n",
      "\u001b[37m# set the constants for the content types\u001b[39;49;00m\n",
      "JSON_CONTENT_TYPE = \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "JPEG_CONTENT_TYPE = \u001b[33m'\u001b[39;49;00m\u001b[33mimage/jpeg\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_train\u001b[39;49;00m(args):\n",
      "    device = \u001b[33m'\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mDevice Type: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading Pets dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m'\u001b[39;49;00m\u001b[33mBatch size: {args.batch_size}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    path = Path(args.data_dir)\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m'\u001b[39;49;00m\u001b[33mData path is: {path}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    path_anno = path/\u001b[33m'\u001b[39;49;00m\u001b[33mannotations\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    path_img = path/\u001b[33m'\u001b[39;49;00m\u001b[33mimages\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    fnames = get_image_files(path_img)\n",
      "    \n",
      "    \u001b[37m# get the pattern to select the training/validation data\u001b[39;49;00m\n",
      "    np.random.seed(\u001b[34m2\u001b[39;49;00m)\n",
      "    pat = re.compile(\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m/([^/]+)_\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\u001b[33md+.jpg$\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCreating DataBunch object\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    data = ImageDataBunch.from_name_re(path_img, fnames, pat, \n",
      "                                       ds_tfms=get_transforms(), \n",
      "                                       size=args.image_size, \n",
      "                                       bs=args.batch_size).normalize(imagenet_stats)\n",
      "\n",
      "    \u001b[37m# create the CNN model\u001b[39;49;00m\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCreate CNN model from model zoo\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m'\u001b[39;49;00m\u001b[33mModel architecture is {args.model_arch}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    arch = \u001b[36mgetattr\u001b[39;49;00m(models, args.model_arch)\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCreating pretrained conv net\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \n",
      "    learn = create_cnn(data, arch, metrics=error_rate)\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFit for 4 cycles\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    learn.fit_one_cycle(\u001b[34m4\u001b[39;49;00m)    \n",
      "    learn.unfreeze()\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mUnfreeze and fit for another 2 cycles\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    learn.fit_one_cycle(\u001b[34m2\u001b[39;49;00m, max_lr=\u001b[36mslice\u001b[39;49;00m(\u001b[34m1e-6\u001b[39;49;00m,\u001b[34m1e-4\u001b[39;49;00m))\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFinished Training\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model_path = Path(args.model_dir)\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m'\u001b[39;49;00m\u001b[33mExport data object\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    data.export(model_path/\u001b[33m'\u001b[39;49;00m\u001b[33mexport.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[37m# create empty models dir\u001b[39;49;00m\n",
      "    os.mkdir(model_path/\u001b[33m'\u001b[39;49;00m\u001b[33mmodels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m'\u001b[39;49;00m\u001b[33mSaving model weights\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m learn.save(model_path/f\u001b[33m'\u001b[39;49;00m\u001b[33m{args.model_arch}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mmodel_fn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    path = Path(model_dir)\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCreating DataBunch object\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    empty_data = ImageDataBunch.load_empty(path)\n",
      "    arch_name = os.path.splitext(os.path.split(glob.glob(f\u001b[33m'\u001b[39;49;00m\u001b[33m{model_dir}/resnet*.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m0\u001b[39;49;00m])[\u001b[34m1\u001b[39;49;00m])[\u001b[34m0\u001b[39;49;00m]\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m'\u001b[39;49;00m\u001b[33mModel architecture is: {arch_name}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    arch = \u001b[36mgetattr\u001b[39;49;00m(models, arch_name)    \n",
      "    learn = create_cnn(empty_data, arch, pretrained=\u001b[36mFalse\u001b[39;49;00m).load(path/f\u001b[33m'\u001b[39;49;00m\u001b[33m{arch_name}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m learn\n",
      "\n",
      "\u001b[37m# Deserialize the Invoke request body into an object we can perform prediction on\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(request_body, content_type=JPEG_CONTENT_TYPE):\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[37m# process an image uploaded to the endpoint\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == JPEG_CONTENT_TYPE:\n",
      "        img = open_image(io.BytesIO(request_body))\n",
      "        \u001b[34mreturn\u001b[39;49;00m img\n",
      "    \u001b[37m# process a URL submitted to the endpoint\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == JSON_CONTENT_TYPE:\n",
      "        img_request = requests.get(request_body[\u001b[33m'\u001b[39;49;00m\u001b[33murl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], stream=\u001b[36mTrue\u001b[39;49;00m)\n",
      "        img = open_image(io.BytesIO(img_request.content))\n",
      "        \u001b[34mreturn\u001b[39;49;00m img        \n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(content_type))\n",
      "\n",
      "\u001b[37m# Perform prediction on the deserialized object, with the loaded model\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_object, model):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mCalling model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    start_time = time.time()\n",
      "    predict_class,predict_idx,predict_values = model.predict(input_object)\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m--- Inference time: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m seconds ---\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % (time.time() - start_time))\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m'\u001b[39;49;00m\u001b[33mPredicted class is {str(predict_class)}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m'\u001b[39;49;00m\u001b[33mPredict confidence score is {predict_values[predict_idx.item()].item()}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    response = {}\n",
      "    response[\u001b[33m'\u001b[39;49;00m\u001b[33mclass\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(predict_class)\n",
      "    response[\u001b[33m'\u001b[39;49;00m\u001b[33mconfidence\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = predict_values[predict_idx.item()].item()\n",
      "    \u001b[34mreturn\u001b[39;49;00m response\n",
      "\n",
      "\u001b[37m# Serialize the prediction result into the desired response content type\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction, accept=JSON_CONTENT_TYPE):        \n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m accept == JSON_CONTENT_TYPE:\n",
      "        output = json.dumps(prediction)\n",
      "        \u001b[34mreturn\u001b[39;49;00m output, accept\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in Accept: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(accept))    \n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mW\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of data loading workers (default: 2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of total epochs to run (default: 2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mBS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mbatch size (default: 4)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minitial learning rate (default: 0.001)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mmomentum (default: 0.9)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dist_backend\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mgloo\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mdistributed backend (default: gloo)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "     \u001b[37m# fast.ai specific parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--image-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m224\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mIS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mimage size (default: 224)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-arch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mresnet34\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mMA\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mmodel arch (default: resnet34)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)   \n",
      "    \n",
      "    env = sagemaker_containers.training_env()\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=env.hosts)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=env.current_host)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=env.model_dir)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=env.channel_input_dirs.get(\u001b[33m'\u001b[39;49;00m\u001b[33mtraining\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=env.num_gpus)\n",
      "\n",
      "    _train(parser.parse_args())\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source/pets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Functions\n",
    "\n",
    "SageMaker invokes the main function defined within your training script for training. When deploying your trained model to an endpoint, the model_fn() is called to determine how to load your trained model. The model_fn() along with a few other functions list below are called to enable predictions on SageMaker.\n",
    "\n",
    "### [Predicting Functions](https://github.com/aws/sagemaker-pytorch-containers/blob/master/src/sagemaker_pytorch_container/serving.py)\n",
    "* model_fn(model_dir) - loads your model.\n",
    "* input_fn(serialized_input_data, content_type) - deserializes predictions to predict_fn.\n",
    "* output_fn(prediction_output, accept) - serializes predictions from predict_fn.\n",
    "* predict_fn(input_data, model) - calls a model on data deserialized in input_fn.\n",
    "\n",
    "The model_fn() is the only function that doesn't have a default implementation and is required by the user for using PyTorch on SageMaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.PyTorch estimator\n",
    "\n",
    "The `PyTorch` class allows us to run our training function on SageMaker. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. \n",
    "\n",
    "For local training with GPU, we could set this to ```local_gpu```.  In this case, `instance_type` was set below based on whether you're running a GPU instance. If `instance_type` is set to a SageMaker instance type (e.g. ml.p2.xlarge) then the training will happen on SageMaker.\n",
    "\n",
    "The parameter `data_location` determines where the training data is. If training locally then it can be set to the local file system to avoid having to download from S3. If training on SageMaker then it needs to reference the training data on S3.\n",
    "\n",
    "After we've constructed our `PyTorch` object, we fit it using the data we uploaded to S3. Even though we're in local mode, using S3 as our data source makes sense because it maintains consistency with how SageMaker's distributed, managed training ingests data.\n",
    "\n",
    "If you want to train locally then uncomment out all of the lines in the code block below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out all lines below if not training locally\n",
    "data_location='file://'+str(path)\n",
    "instance_type = 'local'\n",
    "if subprocess.call('nvidia-smi') == 0:\n",
    "    ## Set type to GPU if one is present\n",
    "    instance_type = 'local_gpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train your model on SageMaker then comment out the cell above and uncomment the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out all lines below if not training on SageMaker\n",
    "#data_location=inputs\n",
    "#instance_type = 'ml.p3.2xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_estimator = PyTorch(entry_point='source/pets.py',\n",
    "                         base_job_name='fastai-pets',\n",
    "                         role=role,\n",
    "                         framework_version='1.0.0',\n",
    "                         train_instance_count=1,\n",
    "                         train_instance_type=instance_type)\n",
    "\n",
    "pets_estimator.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model to prepare for predictions\n",
    "\n",
    "First we need to create a `PyTorchModel` object from the estimator. The `deploy()` method on the model object creates an endpoint (in this case locally) which serves prediction requests in real-time. If the `instance_type` is set to a SageMaker instance type (e.g. ml.m5.large) then the model will be deployed on SageMaker. If the `instance_type` parameter is set to `local` then it will be deployed locally as a Docker container and ready for testing locally.\n",
    "\n",
    "First we need to create a `RealTimePredictor` class to accept `jpeg` images as input and output JSON. The default behaviour is to accept a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(ImagePredictor, self).__init__(endpoint_name, sagemaker_session=sagemaker_session, serializer=None, \n",
    "                                            deserializer=json_deserializer, content_type='image/jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to deploy your model locally then comment out the ```instance_type``` declaration below. \n",
    "\n",
    "If you want to deploy your model on SageMaker then uncomment the the ```instance_type``` declaration below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment out for SageMaker Deployment\n",
    "#instance_type = 'ml.c5.large'\n",
    "\n",
    "pets_model=PyTorchModel(model_data=pets_estimator.model_data,\n",
    "                        name=pets_estimator._current_job_name,\n",
    "                        role=role,\n",
    "                        framework_version=pets_estimator.framework_version,\n",
    "                        entry_point=pets_estimator.entry_point,\n",
    "                        predictor_cls=ImagePredictor)\n",
    "\n",
    "pets_predictor = pets_model.deploy(initial_instance_count=1,\n",
    "                                       instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "# English Cocker Spaniel\n",
    "urls.append('https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2017/11/16105011/English-Cocker-Spaniel-Slide03.jpg')\n",
    "# Shiba Inu\n",
    "urls.append('https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/Taka_Shiba.jpg/1200px-Taka_Shiba.jpg')\n",
    "# German Short haired\n",
    "urls.append('https://vetstreet.brightspotcdn.com/dims4/default/232fcc6/2147483647/crop/0x0%2B0%2B0/resize/645x380/quality/90/?url=https%3A%2F%2Fvetstreet-brightspot.s3.amazonaws.com%2Fda%2Fa44590a0d211e0a2380050568d634f%2Ffile%2FGerman-Shorthair-Pointer-2-645mk062111.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random selection\n",
    "img_bytes = requests.get(random.choice(urls)).content\n",
    "img = PIL.Image.open(io.BytesIO(img_bytes))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pets_predictor.predict(img_bytes)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up\n",
    "\n",
    "Deleting the local endpoint when you're finished is important since you can only run one local endpoint at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_estimator.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
