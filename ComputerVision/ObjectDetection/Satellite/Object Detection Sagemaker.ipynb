{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Object Detection - Satellite Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Object detection is the process of identifying and localizing objects in an image. A typical object detection solution takes in an image as input and the output is a bounding box and classification of an object.  But before we have this solution, we need to acquire and process a traning dataset, create and setup a training job for the alorithm so that the aglorithm can learn about the dataset and then host the algorithm as an endpoint, to which we can supply the query image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To train the Object Detection algorithm on Amazon SageMaker, we need to setup and authenticate the use of AWS services. To begin with we need an AWS account role with SageMaker access. This role is used to give SageMaker access to your data in S3 will automatically be obtained from the role used to start the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::431615879134:role/sagemaker-test-role\n",
      "CPU times: user 118 ms, sys: 0 ns, total: 118 ms\n",
      "Wall time: 185 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the S3 bucket that you want to use for training and to store the tranied model artifacts. In this notebook, we require a custom bucket that exists so as to keep the naming clean. You can end up using a default bucket that SageMaker comes with as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data bucket\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'XView-ObjectDetection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811284229777.dkr.ecr.us-east-1.amazonaws.com/object-detection:latest\n"
     ]
    }
   ],
   "source": [
    "# training image location for object detection algorithm \n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "training_image = get_image_uri(sess.boto_region_name, 'object-detection', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this notebook comes from the XView competition[].  The data is described here https://arxiv.org/pdf/1802.07856.pdf.  Other interesting satellite image datasets could also be used.  For example:\n",
    "\n",
    "* spacenet\n",
    "* COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://random.datasets.sengstacken/xview_satellite_images/val.tgz to data/val.tgz\n"
     ]
    }
   ],
   "source": [
    "# val data\n",
    "!aws s3 cp s3://random.datasets.sengstacken/xview_satellite_images/val.tgz ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./val_images/\n",
      "./val_images/1038.tif\n",
      "./val_images/1040.tif\n",
      "./val_images/1043.tif\n",
      "./val_images/1054.tif\n",
      "./val_images/1060.tif\n",
      "./val_images/1062.tif\n",
      "./val_images/1064.tif\n",
      "./val_images/1066.tif\n",
      "./val_images/1069.tif\n",
      "./val_images/1071.tif\n",
      "./val_images/1073.tif\n",
      "./val_images/1075.tif\n",
      "./val_images/108.tif\n",
      "./val_images/1082.tif\n",
      "./val_images/1097.tif\n",
      "./val_images/1098.tif\n",
      "./val_images/11.tif\n",
      "./val_images/1102.tif\n",
      "./val_images/1115.tif\n",
      "./val_images/1116.tif\n",
      "./val_images/1117.tif\n",
      "./val_images/1122.tif\n",
      "./val_images/1134.tif\n",
      "./val_images/1138.tif\n",
      "./val_images/1148.tif\n",
      "./val_images/1159.tif\n",
      "./val_images/1161.tif\n",
      "./val_images/1177.tif\n",
      "./val_images/1194.tif\n",
      "./val_images/12.tif\n",
      "./val_images/1207.tif\n",
      "./val_images/121.tif\n",
      "./val_images/1213.tif\n",
      "./val_images/122.tif\n",
      "./val_images/1234.tif\n",
      "./val_images/1235.tif\n",
      "./val_images/1254.tif\n",
      "./val_images/1258.tif\n",
      "./val_images/1263.tif\n",
      "./val_images/1267.tif\n",
      "./val_images/1282.tif\n",
      "./val_images/1308.tif\n",
      "./val_images/1326.tif\n",
      "./val_images/1333.tif\n",
      "./val_images/1358.tif\n",
      "./val_images/1359.tif\n",
      "./val_images/1360.tif\n",
      "./val_images/1401.tif\n",
      "./val_images/1402.tif\n",
      "./val_images/1414.tif\n",
      "./val_images/1426.tif\n",
      "./val_images/143.tif\n",
      "./val_images/1434.tif\n",
      "./val_images/1464.tif\n",
      "./val_images/147.tif\n",
      "./val_images/1470.tif\n",
      "./val_images/1471.tif\n",
      "./val_images/1474.tif\n",
      "./val_images/1475.tif\n",
      "./val_images/1506.tif\n",
      "./val_images/1510.tif\n",
      "./val_images/1520.tif\n",
      "./val_images/1521.tif\n",
      "./val_images/1522.tif\n",
      "./val_images/1554.tif\n",
      "./val_images/1566.tif\n",
      "./val_images/1578.tif\n",
      "./val_images/1595.tif\n",
      "./val_images/1599.tif\n",
      "./val_images/1605.tif\n",
      "./val_images/1627.tif\n",
      "./val_images/1650.tif\n",
      "./val_images/1652.tif\n",
      "./val_images/1670.tif\n",
      "./val_images/1681.tif\n",
      "./val_images/1693.tif\n",
      "./val_images/1728.tif\n",
      "./val_images/1742.tif\n",
      "./val_images/1746.tif\n",
      "./val_images/1750.tif\n",
      "./val_images/1765.tif\n",
      "./val_images/178.tif\n",
      "./val_images/1794.tif\n",
      "./val_images/1800.tif\n",
      "./val_images/1804.tif\n",
      "./val_images/1811.tif\n",
      "./val_images/1813.tif\n",
      "./val_images/1814.tif\n",
      "./val_images/1816.tif\n",
      "./val_images/1819.tif\n",
      "./val_images/1833.tif\n",
      "./val_images/1835.tif\n",
      "./val_images/1836.tif\n",
      "./val_images/1843.tif\n",
      "./val_images/1846.tif\n",
      "./val_images/1866.tif\n",
      "./val_images/1872.tif\n",
      "./val_images/1882.tif\n",
      "./val_images/1895.tif\n",
      "./val_images/1903.tif\n",
      "./val_images/1905.tif\n",
      "./val_images/1909.tif\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf ./data/val.tgz -C ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^Cmpleted 426.5 MiB/14.4 GiB (157.9 MiB/s) with 1 file(s) remaining\n",
      "Completed 429.0 MiB/14.4 GiB (149.6 MiB/s) with 1 file(s) remaining\r"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "!aws s3 cp s3://random.datasets.sengstacken/xview_satellite_images/train.tgz ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train_images/\n",
      "./train_images/10.tif\n",
      "./train_images/._100.tif\n",
      "./train_images/100.tif\n",
      "./train_images/._102.tif\n",
      "./train_images/102.tif\n",
      "./train_images/1036.tif\n",
      "./train_images/1037.tif\n",
      "./train_images/104.tif\n",
      "./train_images/1042.tif\n",
      "./train_images/1044.tif\n",
      "./train_images/1046.tif\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf ./data/train.tgz -C ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://random.datasets.sengstacken/xview_satellite_images/train_labels.tgz to data/train_labels.tgz\n"
     ]
    }
   ],
   "source": [
    "# train labels\n",
    "!aws s3 cp s3://random.datasets.sengstacken/xview_satellite_images/train_labels.tgz ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./xView_train.geojson\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf ./data/train_labels.tgz -C ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using this dataset, we need to perform some data cleaning. The algorithm expects the dataset in a particular JSON format. The xView dataset, while containing annotations in JSON, does not follow our specifications. We will use this as an opportunity to introduce our JSON format by performing this convertion. To begin with we create appropriate directories for training images, validation images, as well as the annotation files for both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset \n",
    "\n",
    "The Amazon SageMaker Object Detection algorithm expects lables to be indexed from `0`. It also expects lables to be unique, successive and not skip any integers. For instance, if there are ten classes, the algorithm expects and the labels only be in the set `[0,1,2,3,4,5,6,7,8,9]`. \n",
    "\n",
    "In the xView validation set unfortunately, the labels do not satistify this requirement. Some indices are skipped and the labels start from `1`. We therefore need a mapper that will convert this index system to our requirement. Let us create a generic mapper therefore that could also be used to other datasets that might have nonunique or even string labels. All we need in a dictionary that would create a key-value mapping where an original label is hashed to a label that we require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir jpeg_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tiff to jpeg images\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "p = \"./data/train_images\"\n",
    "img_files = []\n",
    "for infile in os.listdir(\"./data/train_images\"):\n",
    "    if infile[-3:] == \"tif\" or infile[-3:] == \"bmp\" :\n",
    "        \n",
    "        try:\n",
    "            outfile = './data/jpeg_images/' + infile[:-3] + \"jpeg\"\n",
    "            im = Image.open(p + '/' + infile)\n",
    "            out = im.convert(\"RGB\")\n",
    "            out.save(outfile, \"JPEG\", quality=80)\n",
    "            img_files.append(outfile)\n",
    "            print('saved - ',outfile)\n",
    "        except:\n",
    "            print('failed - ',infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "img_files = os.listdir('./data/jpeg_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = {11:\"Fixed-wing Aircraft\",\n",
    "         12:\"Small Aircraft\", \n",
    "         13:\"Passenger/Cargo Plane\",\n",
    "         15:\"Helicopter\",\n",
    "         17:\"Passenger Vehicle\",\n",
    "         18:\"Small Car\",\n",
    "         19:\"Bus\",\n",
    "         20:\"Pickup Truck\",\n",
    "         21:\"Utility Truck\",\n",
    "         23:\"Truck\",\n",
    "         24:\"Cargo Truck\",\n",
    "         25:\"Truck Tractor w/ Box Trailer\",\n",
    "         26:\"Truck Tractor\",\n",
    "         27:\"Trailer\",\n",
    "         28:\"Truck Tractor w/ Flatbed Trailer\",\n",
    "         29:\"Truck Tractor w/ Liquid Tank\",\n",
    "         32:\"Crane Truck\",\n",
    "         33:\"Railway Vehicle\",\n",
    "         34:\"Passenger Car\",\n",
    "         35:\"Cargo/Container Car\",\n",
    "         36:\"Flat Car\",\n",
    "         37:\"Tank car\",\n",
    "         38:\"Locomotive\",\n",
    "         40:\"Maritime Vessel\",\n",
    "         41:\"Motorboat\",\n",
    "         42:\"Sailboat\",\n",
    "         44:\"Tugboat\",\n",
    "         45:\"Barge\",\n",
    "         47:\"Fishing Vessel\",\n",
    "         49:\"Ferry\",\n",
    "         50:\"Yacht\",\n",
    "         51:\"Container Ship\",\n",
    "         52:\"Oil Tanker\",\n",
    "         53:\"Engineering Vehicle\",\n",
    "         54:\"Tower crane\",\n",
    "         55:\"Container Crane\",\n",
    "         56:\"Reach Stacker\",\n",
    "         57:\"Straddle Carrier\",\n",
    "         59:\"Mobile Crane\",\n",
    "         60:\"Dump Truck\",\n",
    "         61:\"Haul Truck\",\n",
    "         62:\"Scraper/Tractor\",\n",
    "         63:\"Front loader/Bulldozer\",\n",
    "         64:\"Excavator\",\n",
    "         65:\"Cement Mixer\",\n",
    "         66:\"Ground Grader\",\n",
    "         71:\"Hut/Tent\",\n",
    "         72:\"Shed\",\n",
    "         73:\"Building\",\n",
    "         74:\"Aircraft Hangar\",\n",
    "         75:\"Unknown\",\n",
    "         76:\"Damaged Building\",\n",
    "         77:\"Facility\",\n",
    "         79:\"Construction Site\",\n",
    "         82:\"Unknown\",\n",
    "         83:\"Vehicle Lot\",\n",
    "         84:\"Helipad\",\n",
    "         86:\"Storage Tank\",\n",
    "         89:\"Shipping container lot\",\n",
    "         91:\"Shipping Container\",\n",
    "         93:\"Pylon\",\n",
    "         94:\"Tower\"}\n",
    "\n",
    "def get_mapper():\n",
    "    original_list = [11, 12, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 33,\n",
    "                    34, 35, 36, 37, 38, 40, 41, 42, 44, 45, 47, 49, 50, 51, 52, 53, 54,\n",
    "                    55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 71, 72, 73, 74, 75, 76, 77, 79, 82, 83, 84,\n",
    "                    86, 89, 91, 93, 94]\n",
    "    iter_counter = 0\n",
    "    xView = {}\n",
    "    for orig in original_list:\n",
    "        xView[orig] = iter_counter\n",
    "        iter_counter += 1\n",
    "    return xView"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use this dictionary, to create a look up method. Let us do so in a way that any dictionary could be used to create this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapper_fn(map):  \n",
    "    def mapper(in_category):\n",
    "        return map[in_category]\n",
    "    return mapper\n",
    "\n",
    "fix_index_mapping = get_mapper_fn(get_mapper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `fix_index_mapping` is essentially a look-up method, which we can use to convert lables. Let us now iterate over every annotation in the dataset and prepare our data. Note how the keywords are created and a structure is established. For more information on the JSON format details, refer the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (4.45.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from scikit-image) (5.4.1)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from scikit-image) (2.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from scikit-image) (1.18.5)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 49.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: imageio>=2.3.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from scikit-image) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from scikit-image) (2.2.3)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.7.4-py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 66.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from networkx>=2.0->scikit-image) (4.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2018.4)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.7.3)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (45.2.0)\n",
      "Installing collected packages: PyWavelets, tifffile, scikit-image\n",
      "  Attempting uninstall: PyWavelets\n",
      "    Found existing installation: PyWavelets 0.5.2\n",
      "    Uninstalling PyWavelets-0.5.2:\n",
      "      Successfully uninstalled PyWavelets-0.5.2\n",
      "  Attempting uninstall: scikit-image\n",
      "    Found existing installation: scikit-image 0.16.2\n",
      "    Uninstalling scikit-image-0.16.2:\n",
      "      Successfully uninstalled scikit-image-0.16.2\n",
      "Successfully installed PyWavelets-1.1.1 scikit-image-0.17.2 tifffile-2020.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601937/601937 [00:02<00:00, 220708.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import data_utilities.wv_util as wv\n",
    "import data_utilities.aug_util as aug\n",
    "\n",
    "coords, chips, classes = wv.get_labels('./data/xView_train.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./data/jpeg_chips\n",
    "!mkdir ./data/generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import trange\n",
    "\n",
    "for i in trange(len(img_files)):\n",
    "    \n",
    "    if '.jpeg' in img_files[i]:\n",
    "        arr = wv.get_image('./data/jpeg_images/'+img_files[i])\n",
    "        match_name = img_files[i].strip('.jpeg')+'.tif'\n",
    "        c_coords = coords[chips==match_name]\n",
    "        c_classes = classes[chips==match_name].astype(np.int64)\n",
    "        c_img, c_box, c_cls = wv.chip_image(img = arr, coords= c_coords, classes=c_classes, shape=(500,500))\n",
    "\n",
    "        for j in range(len(c_img)):\n",
    "            if(len(c_cls) >= 1 and c_cls[j][0] !=0):\n",
    "                save_name = img_files[i].strip('.jpeg') + \"_\" + str(j) + '.jpeg'\n",
    "                chip = Image.fromarray(c_img[j])\n",
    "                chip.save('./data/jpeg_chips/' + save_name)\n",
    "\n",
    "                jsonFile = save_name.strip('.jpeg')+'.json'\n",
    "\n",
    "                line = {}\n",
    "                line['file'] = save_name\n",
    "                line['image_size'] = [{\n",
    "                    'width':500,\n",
    "                    'height':500,\n",
    "                    'depth':3\n",
    "                }]\n",
    "\n",
    "                line['annotations'] = []\n",
    "                line['categories'] = []\n",
    "\n",
    "    #             for k in range(len(c_box[j])):\n",
    "    #                 line['annotations'].append({\n",
    "    #                     'class_id':int(fix_index_mapping(c_cls[j][k])),\n",
    "    #                     'top':int(c_box[j][k][0]),\n",
    "    #                     'left':int(c_box[j][k][1]),\n",
    "    #                     'width':int(c_box[j][k][2]),\n",
    "    #                     'height':int(c_box[j][k][3])\n",
    "    #                 })\n",
    "    #                 line['categories'].append({\n",
    "    #                     'class_id':int(fix_index_mapping(c_cls[j][k])),\n",
    "    #                     'name':rlist.get(c_cls[j][k])\n",
    "    #                 })\n",
    "\n",
    "                for k in range(len(c_box[j])):\n",
    "                    line['annotations'].append({\n",
    "                        'class_id':int(fix_index_mapping(c_cls[j][k])),\n",
    "                        'top':int(c_box[j][k][1]),\n",
    "                        'left':int(c_box[j][k][0]),\n",
    "                        'width':int(c_box[j][k][2]-c_box[j][k][0]),\n",
    "                        'height':int(c_box[j][k][3]-c_box[j][k][1])\n",
    "                    })\n",
    "                    line['categories'].append({\n",
    "                        'class_id':int(fix_index_mapping(c_cls[j][k])),\n",
    "                        'name':rlist.get(c_cls[j][k])\n",
    "                    })    \n",
    "                with open(os.path.join('./data/generated', jsonFile),'w') as p:\n",
    "                    json.dump(line,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14673"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('./data/generated/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "path = './data/jpeg_chips/'\n",
    "\n",
    "folder = os.fsencode(path)\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    filename = os.fsdecode(file)\n",
    "    file_names.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the images without annotations, we have 6432 annotated images. Let us split this dataset and create our training and validation datasets, with which our algorithm will train. To do so, we will simply split the dataset into training and validation data and move them to their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14673"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = 0.8\n",
    "train_list,val_list= train_test_split(file_names,train_size=train,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir './data/train/'\n",
    "!mkdir './data/train_annotation'\n",
    "!mkdir './data/validation/'\n",
    "!mkdir './data/validation_annotation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1193_15.jpeg',\n",
       " '42_0.jpeg',\n",
       " '203_28.jpeg',\n",
       " '325_15.jpeg',\n",
       " '47_4.jpeg',\n",
       " '1465_36.jpeg',\n",
       " '756_4.jpeg',\n",
       " '1910_5.jpeg',\n",
       " '2542_14.jpeg',\n",
       " '1690_4.jpeg',\n",
       " '1095_8.jpeg',\n",
       " '5_13.jpeg',\n",
       " '2436_26.jpeg',\n",
       " '1124_11.jpeg',\n",
       " '2011_21.jpeg',\n",
       " '2305_10.jpeg',\n",
       " '2032_22.jpeg',\n",
       " '1831_24.jpeg',\n",
       " '145_22.jpeg',\n",
       " '871_40.jpeg',\n",
       " '1701_11.jpeg',\n",
       " '1985_19.jpeg',\n",
       " '92_10.jpeg',\n",
       " '629_15.jpeg',\n",
       " '2398_8.jpeg',\n",
       " '2370_14.jpeg',\n",
       " '805_4.jpeg',\n",
       " '73_17.jpeg',\n",
       " '2497_14.jpeg',\n",
       " '2012_45.jpeg',\n",
       " '1124_6.jpeg',\n",
       " '1912_24.jpeg',\n",
       " '379_24.jpeg',\n",
       " '1795_25.jpeg',\n",
       " '1509_29.jpeg',\n",
       " '2214_9.jpeg',\n",
       " '1284_12.jpeg',\n",
       " '128_18.jpeg',\n",
       " '2371_14.jpeg',\n",
       " '1128_33.jpeg',\n",
       " '1932_48.jpeg',\n",
       " '2542_19.jpeg',\n",
       " '105_17.jpeg',\n",
       " '1181_12.jpeg',\n",
       " '5_12.jpeg',\n",
       " '2519_20.jpeg',\n",
       " '112_30.jpeg',\n",
       " '2515_12.jpeg',\n",
       " '805_8.jpeg',\n",
       " '1403_30.jpeg',\n",
       " '1431_13.jpeg',\n",
       " '1150_23.jpeg',\n",
       " '1807_5.jpeg',\n",
       " '1807_34.jpeg',\n",
       " '1245_20.jpeg',\n",
       " '523_7.jpeg',\n",
       " '42_13.jpeg',\n",
       " '1441_9.jpeg',\n",
       " '1061_27.jpeg',\n",
       " '1472_10.jpeg',\n",
       " '2565_39.jpeg',\n",
       " '18_15.jpeg',\n",
       " '2564_9.jpeg',\n",
       " '1913_3.jpeg',\n",
       " '1094_28.jpeg',\n",
       " '1169_14.jpeg',\n",
       " '880_20.jpeg',\n",
       " '2014_43.jpeg',\n",
       " '130_7.jpeg',\n",
       " '2584_4.jpeg',\n",
       " '2519_31.jpeg',\n",
       " '2207_16.jpeg',\n",
       " '2294_7.jpeg',\n",
       " '2072_18.jpeg',\n",
       " '2062_18.jpeg',\n",
       " '1121_19.jpeg',\n",
       " '2391_23.jpeg',\n",
       " '1894_8.jpeg',\n",
       " '907_0.jpeg',\n",
       " '1205_24.jpeg',\n",
       " '600_6.jpeg',\n",
       " '1976_7.jpeg',\n",
       " '598_2.jpeg',\n",
       " '2315_16.jpeg',\n",
       " '1165_16.jpeg',\n",
       " '2148_3.jpeg',\n",
       " '1233_7.jpeg',\n",
       " '1858_15.jpeg',\n",
       " '2543_8.jpeg',\n",
       " '1445_5.jpeg',\n",
       " '322_15.jpeg',\n",
       " '223_39.jpeg',\n",
       " '1906_22.jpeg',\n",
       " '295_31.jpeg',\n",
       " '2000_4.jpeg',\n",
       " '1462_10.jpeg',\n",
       " '1169_24.jpeg',\n",
       " '1894_18.jpeg',\n",
       " '1193_12.jpeg',\n",
       " '2261_3.jpeg',\n",
       " '618_22.jpeg',\n",
       " '1679_0.jpeg',\n",
       " '1892_3.jpeg',\n",
       " '1444_32.jpeg',\n",
       " '1630_22.jpeg',\n",
       " '1378_17.jpeg',\n",
       " '860_26.jpeg',\n",
       " '893_22.jpeg',\n",
       " '546_23.jpeg',\n",
       " '105_18.jpeg',\n",
       " '1848_1.jpeg',\n",
       " '1928_24.jpeg',\n",
       " '2162_16.jpeg',\n",
       " '2552_19.jpeg',\n",
       " '287_47.jpeg',\n",
       " '2485_18.jpeg',\n",
       " '886_15.jpeg',\n",
       " '1855_2.jpeg',\n",
       " '1694_16.jpeg',\n",
       " '1140_5.jpeg',\n",
       " '1056_28.jpeg',\n",
       " '1405_2.jpeg',\n",
       " '2375_8.jpeg',\n",
       " '1694_25.jpeg',\n",
       " '1701_21.jpeg',\n",
       " '118_18.jpeg',\n",
       " '322_32.jpeg',\n",
       " '523_18.jpeg',\n",
       " '1224_5.jpeg',\n",
       " '2251_7.jpeg',\n",
       " '492_15.jpeg',\n",
       " '102_28.jpeg',\n",
       " '546_17.jpeg',\n",
       " '1072_19.jpeg',\n",
       " '1092_8.jpeg',\n",
       " '2568_5.jpeg',\n",
       " '1361_21.jpeg',\n",
       " '1465_24.jpeg',\n",
       " '2010_47.jpeg',\n",
       " '1037_30.jpeg',\n",
       " '1450_39.jpeg',\n",
       " '1133_40.jpeg',\n",
       " '2509_12.jpeg',\n",
       " '1893_27.jpeg',\n",
       " '2459_26.jpeg',\n",
       " '1914_12.jpeg',\n",
       " '1863_21.jpeg',\n",
       " '2552_3.jpeg',\n",
       " '1127_27.jpeg',\n",
       " '136_24.jpeg',\n",
       " '2509_24.jpeg',\n",
       " '1178_11.jpeg',\n",
       " '1482_15.jpeg',\n",
       " '2489_10.jpeg',\n",
       " '94_26.jpeg',\n",
       " '2423_27.jpeg',\n",
       " '1068_9.jpeg',\n",
       " '180_22.jpeg',\n",
       " '2044_7.jpeg',\n",
       " '110_13.jpeg',\n",
       " '320_44.jpeg',\n",
       " '74_13.jpeg',\n",
       " '1848_21.jpeg',\n",
       " '781_6.jpeg',\n",
       " '1132_9.jpeg',\n",
       " '1378_9.jpeg',\n",
       " '1914_13.jpeg',\n",
       " '1817_32.jpeg',\n",
       " '2214_27.jpeg',\n",
       " '2468_16.jpeg',\n",
       " '1441_19.jpeg',\n",
       " '2591_8.jpeg',\n",
       " '1815_15.jpeg',\n",
       " '1922_25.jpeg',\n",
       " '20_25.jpeg',\n",
       " '2293_4.jpeg',\n",
       " '393_19.jpeg',\n",
       " '486_1.jpeg',\n",
       " '323_21.jpeg',\n",
       " '647_6.jpeg',\n",
       " '129_15.jpeg',\n",
       " '510_0.jpeg',\n",
       " '91_25.jpeg',\n",
       " '1465_1.jpeg',\n",
       " '562_24.jpeg',\n",
       " '1217_15.jpeg',\n",
       " '744_15.jpeg',\n",
       " '128_12.jpeg',\n",
       " '1397_12.jpeg',\n",
       " '1141_24.jpeg',\n",
       " '1692_8.jpeg',\n",
       " '73_27.jpeg',\n",
       " '1585_4.jpeg',\n",
       " '669_17.jpeg',\n",
       " '320_7.jpeg',\n",
       " '46_18.jpeg',\n",
       " '69_0.jpeg',\n",
       " '2519_32.jpeg',\n",
       " '2017_37.jpeg',\n",
       " '716_11.jpeg',\n",
       " '47_24.jpeg',\n",
       " '1940_2.jpeg',\n",
       " '1418_2.jpeg',\n",
       " '283_52.jpeg',\n",
       " '1914_19.jpeg',\n",
       " '1357_17.jpeg',\n",
       " '2293_18.jpeg',\n",
       " '885_9.jpeg',\n",
       " '1886_31.jpeg',\n",
       " '1095_29.jpeg',\n",
       " '1593_18.jpeg',\n",
       " '131_12.jpeg',\n",
       " '1607_0.jpeg',\n",
       " '1834_38.jpeg',\n",
       " '1980_49.jpeg',\n",
       " '772_23.jpeg',\n",
       " '2301_30.jpeg',\n",
       " '2293_2.jpeg',\n",
       " '158_27.jpeg',\n",
       " '2020_17.jpeg',\n",
       " '2560_18.jpeg',\n",
       " '2382_7.jpeg',\n",
       " '46_7.jpeg',\n",
       " '1692_18.jpeg',\n",
       " '1182_12.jpeg',\n",
       " '362_31.jpeg',\n",
       " '1692_6.jpeg',\n",
       " '2391_14.jpeg',\n",
       " '1465_40.jpeg',\n",
       " '1945_0.jpeg',\n",
       " '1379_18.jpeg',\n",
       " '2215_15.jpeg',\n",
       " '2308_2.jpeg',\n",
       " '2408_8.jpeg',\n",
       " '1361_31.jpeg',\n",
       " '819_12.jpeg',\n",
       " '2318_4.jpeg',\n",
       " '1124_35.jpeg',\n",
       " '2301_31.jpeg',\n",
       " '1982_47.jpeg',\n",
       " '1124_38.jpeg',\n",
       " '819_29.jpeg',\n",
       " '1158_14.jpeg',\n",
       " '907_34.jpeg',\n",
       " '525_3.jpeg',\n",
       " '1208_23.jpeg',\n",
       " '859_6.jpeg',\n",
       " '1795_20.jpeg',\n",
       " '614_5.jpeg',\n",
       " '1450_35.jpeg',\n",
       " '2425_5.jpeg',\n",
       " '1565_19.jpeg',\n",
       " '2511_31.jpeg',\n",
       " '1106_26.jpeg',\n",
       " '1891_21.jpeg',\n",
       " '1196_21.jpeg',\n",
       " '1931_29.jpeg',\n",
       " '2193_7.jpeg',\n",
       " '2225_24.jpeg',\n",
       " '355_25.jpeg',\n",
       " '1432_9.jpeg',\n",
       " '1472_3.jpeg',\n",
       " '1590_17.jpeg',\n",
       " '2555_24.jpeg',\n",
       " '2011_17.jpeg',\n",
       " '72_12.jpeg',\n",
       " '252_6.jpeg',\n",
       " '1690_10.jpeg',\n",
       " '1132_28.jpeg',\n",
       " '870_29.jpeg',\n",
       " '1887_13.jpeg',\n",
       " '525_34.jpeg',\n",
       " '2239_3.jpeg',\n",
       " '1141_17.jpeg',\n",
       " '295_27.jpeg',\n",
       " '2521_23.jpeg',\n",
       " '1496_12.jpeg',\n",
       " '2375_13.jpeg',\n",
       " '2325_6.jpeg',\n",
       " '1831_4.jpeg',\n",
       " '1459_27.jpeg',\n",
       " '1692_24.jpeg',\n",
       " '2000_6.jpeg',\n",
       " '2341_33.jpeg',\n",
       " '1565_10.jpeg',\n",
       " '1233_9.jpeg',\n",
       " '321_12.jpeg',\n",
       " '1806_32.jpeg',\n",
       " '2459_2.jpeg',\n",
       " '1838_11.jpeg',\n",
       " '1855_13.jpeg',\n",
       " '1523_16.jpeg',\n",
       " '18_28.jpeg',\n",
       " '2509_0.jpeg',\n",
       " '1051_19.jpeg',\n",
       " '1418_13.jpeg',\n",
       " '42_9.jpeg',\n",
       " '2064_12.jpeg',\n",
       " '43_5.jpeg',\n",
       " '1602_23.jpeg',\n",
       " '1702_21.jpeg',\n",
       " '2560_29.jpeg',\n",
       " '2305_30.jpeg',\n",
       " '1565_20.jpeg',\n",
       " '1910_3.jpeg',\n",
       " '360_5.jpeg',\n",
       " '2562_29.jpeg',\n",
       " '903_34.jpeg',\n",
       " '1817_31.jpeg',\n",
       " '1462_19.jpeg',\n",
       " '1042_12.jpeg',\n",
       " '2511_29.jpeg',\n",
       " '2599_27.jpeg',\n",
       " '2009_43.jpeg',\n",
       " '2375_10.jpeg',\n",
       " '1608_15.jpeg',\n",
       " '307_18.jpeg',\n",
       " '1910_30.jpeg',\n",
       " '2310_16.jpeg',\n",
       " '1246_24.jpeg',\n",
       " '1465_17.jpeg',\n",
       " '2470_26.jpeg',\n",
       " '2547_45.jpeg',\n",
       " '2264_21.jpeg',\n",
       " '1807_38.jpeg',\n",
       " '2078_16.jpeg',\n",
       " '1913_31.jpeg',\n",
       " '2009_17.jpeg',\n",
       " '287_29.jpeg',\n",
       " '2053_13.jpeg',\n",
       " '1441_1.jpeg',\n",
       " '2341_11.jpeg',\n",
       " '73_1.jpeg',\n",
       " '621_21.jpeg',\n",
       " '1896_13.jpeg',\n",
       " '1106_9.jpeg',\n",
       " '2197_13.jpeg',\n",
       " '871_39.jpeg',\n",
       " '2413_8.jpeg',\n",
       " '2568_35.jpeg',\n",
       " '1590_18.jpeg',\n",
       " '1216_1.jpeg',\n",
       " '2408_30.jpeg',\n",
       " '1476_28.jpeg',\n",
       " '136_16.jpeg',\n",
       " '1894_23.jpeg',\n",
       " '2310_25.jpeg',\n",
       " '637_17.jpeg',\n",
       " '1824_17.jpeg',\n",
       " '124_11.jpeg',\n",
       " '1065_10.jpeg',\n",
       " '296_28.jpeg',\n",
       " '1420_12.jpeg',\n",
       " '598_14.jpeg',\n",
       " '1192_18.jpeg',\n",
       " '1444_17.jpeg',\n",
       " '548_15.jpeg',\n",
       " '1197_6.jpeg',\n",
       " '1806_22.jpeg',\n",
       " '1985_9.jpeg',\n",
       " '1135_31.jpeg',\n",
       " '1436_9.jpeg',\n",
       " '1284_19.jpeg',\n",
       " '2009_39.jpeg',\n",
       " '2148_22.jpeg',\n",
       " '1910_9.jpeg',\n",
       " '79_19.jpeg',\n",
       " '1065_22.jpeg',\n",
       " '2421_33.jpeg',\n",
       " '1165_5.jpeg',\n",
       " '2398_13.jpeg',\n",
       " '1863_34.jpeg',\n",
       " '2557_40.jpeg',\n",
       " '1378_2.jpeg',\n",
       " '1070_23.jpeg',\n",
       " '193_13.jpeg',\n",
       " '289_6.jpeg',\n",
       " '1880_25.jpeg',\n",
       " '1150_20.jpeg',\n",
       " '5_5.jpeg',\n",
       " '88_12.jpeg',\n",
       " '1077_16.jpeg',\n",
       " '104_11.jpeg',\n",
       " '2214_0.jpeg',\n",
       " '1139_6.jpeg',\n",
       " '1205_11.jpeg',\n",
       " '548_24.jpeg',\n",
       " '819_11.jpeg',\n",
       " '1233_11.jpeg',\n",
       " '2571_22.jpeg',\n",
       " '159_24.jpeg',\n",
       " '1351_30.jpeg',\n",
       " '2122_24.jpeg',\n",
       " '1883_34.jpeg',\n",
       " '2294_6.jpeg',\n",
       " '1695_22.jpeg',\n",
       " '2567_8.jpeg',\n",
       " '321_9.jpeg',\n",
       " '1329_27.jpeg',\n",
       " '1046_9.jpeg',\n",
       " '814_12.jpeg',\n",
       " '2310_31.jpeg',\n",
       " '140_36.jpeg',\n",
       " '2145_22.jpeg',\n",
       " '129_24.jpeg',\n",
       " '1607_17.jpeg',\n",
       " '2421_7.jpeg',\n",
       " '2320_32.jpeg',\n",
       " '1268_6.jpeg',\n",
       " '2574_24.jpeg',\n",
       " '1212_23.jpeg',\n",
       " '1109_23.jpeg',\n",
       " '46_9.jpeg',\n",
       " '1829_12.jpeg',\n",
       " '1457_1.jpeg',\n",
       " '106_16.jpeg',\n",
       " '2042_6.jpeg',\n",
       " '847_2.jpeg',\n",
       " '1945_21.jpeg',\n",
       " '1788_24.jpeg',\n",
       " '2591_20.jpeg',\n",
       " '1070_25.jpeg',\n",
       " '1633_21.jpeg',\n",
       " '1932_31.jpeg',\n",
       " '372_3.jpeg',\n",
       " '735_25.jpeg',\n",
       " '1429_3.jpeg',\n",
       " '1832_2.jpeg',\n",
       " '75_21.jpeg',\n",
       " '2353_6.jpeg',\n",
       " '1593_20.jpeg',\n",
       " '2608_1.jpeg',\n",
       " '1695_9.jpeg',\n",
       " '1089_14.jpeg',\n",
       " '2510_25.jpeg',\n",
       " '1840_20.jpeg',\n",
       " '772_3.jpeg',\n",
       " '1422_9.jpeg',\n",
       " '1914_15.jpeg',\n",
       " '2520_1.jpeg',\n",
       " '322_46.jpeg',\n",
       " '2032_21.jpeg',\n",
       " '2386_16.jpeg',\n",
       " '309_45.jpeg',\n",
       " '73_29.jpeg',\n",
       " '1037_11.jpeg',\n",
       " '546_25.jpeg',\n",
       " '72_0.jpeg',\n",
       " '825_2.jpeg',\n",
       " '2599_2.jpeg',\n",
       " '1446_3.jpeg',\n",
       " '735_31.jpeg',\n",
       " '2293_13.jpeg',\n",
       " '1809_29.jpeg',\n",
       " '2482_24.jpeg',\n",
       " '1845_7.jpeg',\n",
       " '2017_26.jpeg',\n",
       " '1505_3.jpeg',\n",
       " '772_24.jpeg',\n",
       " '2564_19.jpeg',\n",
       " '24_2.jpeg',\n",
       " '620_14.jpeg',\n",
       " '102_2.jpeg',\n",
       " '579_7.jpeg',\n",
       " '2425_27.jpeg',\n",
       " '1856_10.jpeg',\n",
       " '2353_12.jpeg',\n",
       " '1886_8.jpeg',\n",
       " '360_13.jpeg',\n",
       " '2555_3.jpeg',\n",
       " '342_3.jpeg',\n",
       " '1486_0.jpeg',\n",
       " '2564_32.jpeg',\n",
       " '1529_28.jpeg',\n",
       " '1224_9.jpeg',\n",
       " '1246_27.jpeg',\n",
       " '1945_22.jpeg',\n",
       " '1530_10.jpeg',\n",
       " '1529_26.jpeg',\n",
       " '1694_21.jpeg',\n",
       " '1607_10.jpeg',\n",
       " '2421_9.jpeg',\n",
       " '531_17.jpeg',\n",
       " '907_24.jpeg',\n",
       " '74_12.jpeg',\n",
       " '1090_15.jpeg',\n",
       " '145_18.jpeg',\n",
       " '283_40.jpeg',\n",
       " '741_1.jpeg',\n",
       " '2315_19.jpeg',\n",
       " '2552_15.jpeg',\n",
       " '2384_4.jpeg',\n",
       " '2072_19.jpeg',\n",
       " '1523_11.jpeg',\n",
       " '1114_31.jpeg',\n",
       " '1050_28.jpeg',\n",
       " '2017_39.jpeg',\n",
       " '2301_32.jpeg',\n",
       " '1139_11.jpeg',\n",
       " '903_10.jpeg',\n",
       " '2010_7.jpeg',\n",
       " '1154_7.jpeg',\n",
       " '1053_19.jpeg',\n",
       " '1695_6.jpeg',\n",
       " '2011_33.jpeg',\n",
       " '74_21.jpeg',\n",
       " '2334_26.jpeg',\n",
       " '2128_6.jpeg',\n",
       " '464_21.jpeg',\n",
       " '1444_4.jpeg',\n",
       " '724_3.jpeg',\n",
       " '2247_29.jpeg',\n",
       " '1827_19.jpeg',\n",
       " '2278_4.jpeg',\n",
       " '112_4.jpeg',\n",
       " '1482_4.jpeg',\n",
       " '1050_23.jpeg',\n",
       " '2303_18.jpeg',\n",
       " '2264_29.jpeg',\n",
       " '1586_24.jpeg',\n",
       " '807_0.jpeg',\n",
       " '1459_41.jpeg',\n",
       " '1184_7.jpeg',\n",
       " '807_15.jpeg',\n",
       " '2459_17.jpeg',\n",
       " '1362_18.jpeg',\n",
       " '2150_15.jpeg',\n",
       " '1476_34.jpeg',\n",
       " '1139_1.jpeg',\n",
       " '2326_3.jpeg',\n",
       " '534_10.jpeg',\n",
       " '38_4.jpeg',\n",
       " '613_15.jpeg',\n",
       " '1945_29.jpeg',\n",
       " '1364_24.jpeg',\n",
       " '1899_8.jpeg',\n",
       " '2009_20.jpeg',\n",
       " '18_18.jpeg',\n",
       " '33_17.jpeg',\n",
       " '1692_11.jpeg',\n",
       " '2399_23.jpeg',\n",
       " '2292_34.jpeg',\n",
       " '1084_32.jpeg',\n",
       " '2436_17.jpeg',\n",
       " '847_8.jpeg',\n",
       " '42_2.jpeg',\n",
       " '2361_31.jpeg',\n",
       " '1454_7.jpeg',\n",
       " '1900_19.jpeg',\n",
       " '2355_14.jpeg',\n",
       " '1446_22.jpeg',\n",
       " '362_23.jpeg',\n",
       " '282_20.jpeg',\n",
       " '1929_32.jpeg',\n",
       " '1044_21.jpeg',\n",
       " '647_18.jpeg',\n",
       " '414_34.jpeg',\n",
       " '1154_0.jpeg',\n",
       " '1695_20.jpeg',\n",
       " '2516_36.jpeg',\n",
       " '1914_35.jpeg',\n",
       " '2303_1.jpeg',\n",
       " '378_9.jpeg',\n",
       " '2021_48.jpeg',\n",
       " '2353_9.jpeg',\n",
       " '2519_3.jpeg',\n",
       " '418_4.jpeg',\n",
       " '1379_19.jpeg',\n",
       " '75_5.jpeg',\n",
       " '1587_12.jpeg',\n",
       " '1920_28.jpeg',\n",
       " '1449_27.jpeg',\n",
       " '283_35.jpeg',\n",
       " '1224_17.jpeg',\n",
       " '47_18.jpeg',\n",
       " '323_22.jpeg',\n",
       " '2555_9.jpeg',\n",
       " '5_28.jpeg',\n",
       " '1428_4.jpeg',\n",
       " '217_31.jpeg',\n",
       " '2561_9.jpeg',\n",
       " '1208_0.jpeg',\n",
       " '38_10.jpeg',\n",
       " '2399_21.jpeg',\n",
       " '1357_11.jpeg',\n",
       " '1584_16.jpeg',\n",
       " '2014_39.jpeg',\n",
       " '1184_18.jpeg',\n",
       " '2423_14.jpeg',\n",
       " '1880_33.jpeg',\n",
       " '2490_11.jpeg',\n",
       " '2413_19.jpeg',\n",
       " '2250_9.jpeg',\n",
       " '1051_13.jpeg',\n",
       " '1089_31.jpeg',\n",
       " '360_20.jpeg',\n",
       " '118_15.jpeg',\n",
       " '414_20.jpeg',\n",
       " '2011_30.jpeg',\n",
       " '469_22.jpeg',\n",
       " '1565_9.jpeg',\n",
       " '131_25.jpeg',\n",
       " '5_22.jpeg',\n",
       " '1399_31.jpeg',\n",
       " '1511_31.jpeg',\n",
       " '1523_0.jpeg',\n",
       " '639_3.jpeg',\n",
       " '534_12.jpeg',\n",
       " '665_20.jpeg',\n",
       " '2455_18.jpeg',\n",
       " '568_7.jpeg',\n",
       " '2387_15.jpeg',\n",
       " '2606_1.jpeg',\n",
       " '1446_16.jpeg',\n",
       " '615_24.jpeg',\n",
       " '788_3.jpeg',\n",
       " '2020_6.jpeg',\n",
       " '1192_10.jpeg',\n",
       " '2053_19.jpeg',\n",
       " '414_28.jpeg',\n",
       " '1036_18.jpeg',\n",
       " '2472_29.jpeg',\n",
       " '1838_34.jpeg',\n",
       " '112_28.jpeg',\n",
       " '1677_9.jpeg',\n",
       " '546_10.jpeg',\n",
       " '1511_8.jpeg',\n",
       " '2516_2.jpeg',\n",
       " '238_12.jpeg',\n",
       " '724_4.jpeg',\n",
       " '1237_21.jpeg',\n",
       " '2557_31.jpeg',\n",
       " '79_11.jpeg',\n",
       " '1169_23.jpeg',\n",
       " '788_6.jpeg',\n",
       " '2408_23.jpeg',\n",
       " '1095_18.jpeg',\n",
       " '131_7.jpeg',\n",
       " '217_28.jpeg',\n",
       " '1973_28.jpeg',\n",
       " '1505_8.jpeg',\n",
       " '525_5.jpeg',\n",
       " '1086_9.jpeg',\n",
       " '84_15.jpeg',\n",
       " '814_13.jpeg',\n",
       " '2567_22.jpeg',\n",
       " '2391_17.jpeg',\n",
       " '2078_15.jpeg',\n",
       " '2524_6.jpeg',\n",
       " '1196_24.jpeg',\n",
       " '1379_15.jpeg',\n",
       " '756_5.jpeg',\n",
       " '1980_53.jpeg',\n",
       " '711_12.jpeg',\n",
       " '569_31.jpeg',\n",
       " '1124_37.jpeg',\n",
       " '1052_21.jpeg',\n",
       " '302_37.jpeg',\n",
       " '598_16.jpeg',\n",
       " '1150_24.jpeg',\n",
       " '362_1.jpeg',\n",
       " '1831_13.jpeg',\n",
       " '223_33.jpeg',\n",
       " '2008_38.jpeg',\n",
       " '637_22.jpeg',\n",
       " '140_41.jpeg',\n",
       " '90_8.jpeg',\n",
       " '1397_21.jpeg',\n",
       " '2320_10.jpeg',\n",
       " '1070_22.jpeg',\n",
       " '914_0.jpeg',\n",
       " '1051_7.jpeg',\n",
       " '1268_29.jpeg',\n",
       " '1422_26.jpeg',\n",
       " '2493_19.jpeg',\n",
       " '1246_19.jpeg',\n",
       " '1824_15.jpeg',\n",
       " '1139_2.jpeg',\n",
       " '1929_43.jpeg',\n",
       " '2162_0.jpeg',\n",
       " '2334_14.jpeg',\n",
       " '131_23.jpeg',\n",
       " '2313_24.jpeg',\n",
       " '1094_23.jpeg',\n",
       " '825_5.jpeg',\n",
       " '744_17.jpeg',\n",
       " '1080_12.jpeg',\n",
       " '2029_21.jpeg',\n",
       " '1193_17.jpeg',\n",
       " '1457_3.jpeg',\n",
       " '1473_8.jpeg',\n",
       " '2587_1.jpeg',\n",
       " '1085_3.jpeg',\n",
       " '2584_10.jpeg',\n",
       " '1509_10.jpeg',\n",
       " '1809_17.jpeg',\n",
       " '892_6.jpeg',\n",
       " '2042_5.jpeg',\n",
       " '2460_7.jpeg',\n",
       " '1197_0.jpeg',\n",
       " '1823_6.jpeg',\n",
       " '1459_4.jpeg',\n",
       " '2510_0.jpeg',\n",
       " '74_17.jpeg',\n",
       " '181_41.jpeg',\n",
       " '1456_15.jpeg',\n",
       " '1149_5.jpeg',\n",
       " '131_26.jpeg',\n",
       " '1126_25.jpeg',\n",
       " '1505_9.jpeg',\n",
       " '1945_40.jpeg',\n",
       " '1468_22.jpeg',\n",
       " '389_15.jpeg',\n",
       " '574_13.jpeg',\n",
       " '2029_6.jpeg',\n",
       " '1048_20.jpeg',\n",
       " '2029_24.jpeg',\n",
       " '118_2.jpeg',\n",
       " '360_8.jpeg',\n",
       " '1361_35.jpeg',\n",
       " '2619_25.jpeg',\n",
       " '2562_14.jpeg',\n",
       " '1142_29.jpeg',\n",
       " '2139_16.jpeg',\n",
       " '1175_19.jpeg',\n",
       " '568_32.jpeg',\n",
       " '1361_19.jpeg',\n",
       " '2301_14.jpeg',\n",
       " '1206_10.jpeg',\n",
       " '1184_19.jpeg',\n",
       " '394_13.jpeg',\n",
       " '1268_7.jpeg',\n",
       " '1088_1.jpeg',\n",
       " '905_10.jpeg',\n",
       " '1702_5.jpeg',\n",
       " '1912_5.jpeg',\n",
       " '1468_23.jpeg',\n",
       " '884_18.jpeg',\n",
       " '1216_19.jpeg',\n",
       " '2398_11.jpeg',\n",
       " '510_5.jpeg',\n",
       " '1987_5.jpeg',\n",
       " '1206_2.jpeg',\n",
       " '291_39.jpeg',\n",
       " '74_22.jpeg',\n",
       " '46_17.jpeg',\n",
       " '158_13.jpeg',\n",
       " '1080_22.jpeg',\n",
       " '1385_21.jpeg',\n",
       " '1849_11.jpeg',\n",
       " '1465_4.jpeg',\n",
       " '600_5.jpeg',\n",
       " '254_10.jpeg',\n",
       " '2416_29.jpeg',\n",
       " '914_4.jpeg',\n",
       " '2354_26.jpeg',\n",
       " '926_12.jpeg',\n",
       " '2314_12.jpeg',\n",
       " '767_2.jpeg',\n",
       " '2400_33.jpeg',\n",
       " '1430_22.jpeg',\n",
       " '1431_29.jpeg',\n",
       " '2353_1.jpeg',\n",
       " '2408_34.jpeg',\n",
       " '217_41.jpeg',\n",
       " '80_15.jpeg',\n",
       " '2062_22.jpeg',\n",
       " '1823_11.jpeg',\n",
       " '2207_1.jpeg',\n",
       " '1139_24.jpeg',\n",
       " '1224_6.jpeg',\n",
       " '1065_16.jpeg',\n",
       " '1378_15.jpeg',\n",
       " '772_18.jpeg',\n",
       " '2587_2.jpeg',\n",
       " '1897_25.jpeg',\n",
       " '2498_9.jpeg',\n",
       " '1741_19.jpeg',\n",
       " '418_22.jpeg',\n",
       " '2518_13.jpeg',\n",
       " '1449_5.jpeg',\n",
       " '724_36.jpeg',\n",
       " '2250_4.jpeg',\n",
       " '2207_13.jpeg',\n",
       " '1454_12.jpeg',\n",
       " '562_28.jpeg',\n",
       " '2591_25.jpeg',\n",
       " '106_21.jpeg',\n",
       " '1121_11.jpeg',\n",
       " '91_17.jpeg',\n",
       " '2562_15.jpeg',\n",
       " '1142_32.jpeg',\n",
       " '1077_18.jpeg',\n",
       " '1036_26.jpeg',\n",
       " '2482_6.jpeg',\n",
       " '2568_39.jpeg',\n",
       " '2524_0.jpeg',\n",
       " '2009_27.jpeg',\n",
       " '1042_21.jpeg',\n",
       " '2042_0.jpeg',\n",
       " '2010_31.jpeg',\n",
       " '1422_27.jpeg',\n",
       " '1460_7.jpeg',\n",
       " '1809_5.jpeg',\n",
       " '1985_2.jpeg',\n",
       " '525_11.jpeg',\n",
       " '1175_12.jpeg',\n",
       " '1224_0.jpeg',\n",
       " '2011_19.jpeg',\n",
       " '355_7.jpeg',\n",
       " '1891_34.jpeg',\n",
       " '1438_9.jpeg',\n",
       " '2425_29.jpeg',\n",
       " '1980_15.jpeg',\n",
       " '2618_17.jpeg',\n",
       " '1136_7.jpeg',\n",
       " '1856_8.jpeg',\n",
       " '2012_33.jpeg',\n",
       " '1929_52.jpeg',\n",
       " '1815_6.jpeg',\n",
       " '2078_25.jpeg',\n",
       " '2399_14.jpeg',\n",
       " '1855_16.jpeg',\n",
       " '84_28.jpeg',\n",
       " '1406_20.jpeg',\n",
       " '372_4.jpeg',\n",
       " '1703_3.jpeg',\n",
       " '90_24.jpeg',\n",
       " '1447_2.jpeg',\n",
       " '2521_0.jpeg',\n",
       " '942_0.jpeg',\n",
       " '2619_2.jpeg',\n",
       " '1703_18.jpeg',\n",
       " '914_5.jpeg',\n",
       " '20_20.jpeg',\n",
       " '2384_20.jpeg',\n",
       " '1088_0.jpeg',\n",
       " '2294_12.jpeg',\n",
       " '886_4.jpeg',\n",
       " '546_26.jpeg',\n",
       " '379_7.jpeg',\n",
       " '1690_14.jpeg',\n",
       " '97_20.jpeg',\n",
       " '1906_21.jpeg',\n",
       " '600_22.jpeg',\n",
       " '2044_15.jpeg',\n",
       " '1091_39.jpeg',\n",
       " '1740_12.jpeg',\n",
       " '40_16.jpeg',\n",
       " '781_16.jpeg',\n",
       " '1906_29.jpeg',\n",
       " '610_12.jpeg',\n",
       " '1114_3.jpeg',\n",
       " '538_3.jpeg',\n",
       " '2544_17.jpeg',\n",
       " '2055_11.jpeg',\n",
       " '2515_10.jpeg',\n",
       " '309_39.jpeg',\n",
       " '2310_13.jpeg',\n",
       " '1584_24.jpeg',\n",
       " '2382_1.jpeg',\n",
       " '1431_21.jpeg',\n",
       " '587_17.jpeg',\n",
       " '1919_23.jpeg',\n",
       " '2472_31.jpeg',\n",
       " '345_15.jpeg',\n",
       " '2197_22.jpeg',\n",
       " '1465_13.jpeg',\n",
       " '1351_9.jpeg',\n",
       " '2026_14.jpeg',\n",
       " '1284_18.jpeg',\n",
       " '1486_7.jpeg',\n",
       " '1701_12.jpeg',\n",
       " '1106_11.jpeg',\n",
       " '331_0.jpeg',\n",
       " '2472_33.jpeg',\n",
       " '1050_11.jpeg',\n",
       " '1817_25.jpeg',\n",
       " '79_17.jpeg',\n",
       " '792_9.jpeg',\n",
       " '1449_28.jpeg',\n",
       " '84_16.jpeg',\n",
       " '79_23.jpeg',\n",
       " '712_17.jpeg',\n",
       " '2029_7.jpeg',\n",
       " '1169_2.jpeg',\n",
       " '107_8.jpeg',\n",
       " '1795_13.jpeg',\n",
       " '1180_23.jpeg',\n",
       " '129_12.jpeg',\n",
       " '1459_5.jpeg',\n",
       " '2399_10.jpeg',\n",
       " '600_18.jpeg',\n",
       " '1982_3.jpeg',\n",
       " '86_7.jpeg',\n",
       " '1224_14.jpeg',\n",
       " '2562_21.jpeg',\n",
       " '1090_16.jpeg',\n",
       " '1983_25.jpeg',\n",
       " '1378_7.jpeg',\n",
       " '331_5.jpeg',\n",
       " '2389_29.jpeg',\n",
       " '744_32.jpeg',\n",
       " '1450_10.jpeg',\n",
       " '1077_10.jpeg',\n",
       " '1964_25.jpeg',\n",
       " '1891_12.jpeg',\n",
       " '2519_1.jpeg',\n",
       " '716_13.jpeg',\n",
       " '287_31.jpeg',\n",
       " '343_34.jpeg',\n",
       " '104_9.jpeg',\n",
       " '2354_34.jpeg',\n",
       " '611_14.jpeg',\n",
       " '1050_5.jpeg',\n",
       " '1444_7.jpeg',\n",
       " '885_22.jpeg',\n",
       " '1468_29.jpeg',\n",
       " '1449_6.jpeg',\n",
       " '111_10.jpeg',\n",
       " '2561_27.jpeg',\n",
       " '295_22.jpeg',\n",
       " '2313_10.jpeg',\n",
       " '1397_0.jpeg',\n",
       " '2179_16.jpeg',\n",
       " '2568_7.jpeg',\n",
       " '2523_12.jpeg',\n",
       " '1980_3.jpeg',\n",
       " '2325_34.jpeg',\n",
       " '1809_14.jpeg',\n",
       " '2416_10.jpeg',\n",
       " '2389_25.jpeg',\n",
       " '2361_28.jpeg',\n",
       " '1422_14.jpeg',\n",
       " '2375_19.jpeg',\n",
       " '1080_14.jpeg',\n",
       " '1178_22.jpeg',\n",
       " '1068_32.jpeg',\n",
       " '1052_31.jpeg',\n",
       " '2303_3.jpeg',\n",
       " '2384_2.jpeg',\n",
       " '24_20.jpeg',\n",
       " '2510_27.jpeg',\n",
       " '523_22.jpeg',\n",
       " '2484_25.jpeg',\n",
       " '2408_32.jpeg',\n",
       " '1085_10.jpeg',\n",
       " '2413_17.jpeg',\n",
       " '2215_9.jpeg',\n",
       " '1063_21.jpeg',\n",
       " '322_0.jpeg',\n",
       " '1870_26.jpeg',\n",
       " '1192_24.jpeg',\n",
       " '2391_6.jpeg',\n",
       " '252_3.jpeg',\n",
       " '2470_23.jpeg',\n",
       " '2438_25.jpeg',\n",
       " '2416_24.jpeg',\n",
       " '1406_23.jpeg',\n",
       " '342_8.jpeg',\n",
       " '2139_28.jpeg',\n",
       " '1255_8.jpeg',\n",
       " '2371_5.jpeg',\n",
       " '889_15.jpeg',\n",
       " '1799_6.jpeg',\n",
       " '331_16.jpeg',\n",
       " '53_11.jpeg',\n",
       " '1329_8.jpeg',\n",
       " '2618_21.jpeg',\n",
       " '1457_24.jpeg',\n",
       " '2472_1.jpeg',\n",
       " '254_8.jpeg',\n",
       " '1606_6.jpeg',\n",
       " '600_4.jpeg',\n",
       " '2499_15.jpeg',\n",
       " '1084_33.jpeg',\n",
       " '765_18.jpeg',\n",
       " '506_22.jpeg',\n",
       " '47_27.jpeg',\n",
       " '1425_36.jpeg',\n",
       " '145_27.jpeg',\n",
       " '1840_22.jpeg',\n",
       " '1109_13.jpeg',\n",
       " '2322_30.jpeg',\n",
       " '763_10.jpeg',\n",
       " '2472_30.jpeg',\n",
       " '1406_19.jpeg',\n",
       " '2301_16.jpeg',\n",
       " '2062_29.jpeg',\n",
       " '885_11.jpeg',\n",
       " '110_10.jpeg',\n",
       " '1799_22.jpeg',\n",
       " '414_25.jpeg',\n",
       " '2423_0.jpeg',\n",
       " '1508_20.jpeg',\n",
       " '99_27.jpeg',\n",
       " '1654_25.jpeg',\n",
       " '2564_20.jpeg',\n",
       " '.ipynb_checkpoints',\n",
       " '97_18.jpeg',\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "#Moving training files to the training folders\n",
    "for i in train_list:\n",
    "    if '.jpeg' in i:\n",
    "        image_file = './data/jpeg_chips/'+i\n",
    "        shutil.copy(image_file, './data/train/')\n",
    "        shutil.copy('./data/generated/'+i.strip('.jpeg')+'.json', './data/train_annotation/')\n",
    "\n",
    "#Moving validation files to the validation folders\n",
    "for i in val_list:\n",
    "    if '.jpeg' in i:\n",
    "        image_file = './data/jpeg_chips/'+i\n",
    "        shutil.copy(image_file, './data/validation/')\n",
    "        shutil.copy('./data/generated/'+i.strip('.jpeg')+'.json', './data/validation_annotation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11738"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11739"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('./data/train/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3\n",
    "Next step in this process is to upload the data to the S3 bucket, from which the algorithm can read and use the data. We do this using multiple channels. Channels are simply directories in the bucket that differentiate between training and validation data. Let us simply call these directories `train` and `validation`. We will therefore require four channels: two for the data and two for annotations, the annotations ones named with the suffixes `_annotation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "train_annotation_channel = prefix + '/train_annotation'\n",
    "validation_annotation_channel = prefix + '/validation_annotation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sess.upload_data(path='./data/train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='./data/validation', bucket=bucket, key_prefix=validation_channel)\n",
    "sess.upload_data(path='./data/train_annotation', bucket=bucket, key_prefix=train_annotation_channel)\n",
    "sess.upload_data(path='./data/validation_annotation', bucket=bucket, key_prefix=validation_annotation_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)\n",
    "s3_train_annotation = 's3://{}/{}'.format(bucket, train_annotation_channel)\n",
    "s3_validation_annotation = 's3://{}/{}'.format(bucket, validation_annotation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p3.16xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000, \n",
    "                                         input_mode = 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object detection algorithm at its core is the [Single-Shot Multi-Box detection algorithm (SSD)](https://arxiv.org/abs/1512.02325). This algorithm uses a `base_network`, which is typically a [VGG](https://arxiv.org/abs/1409.1556) or a [ResNet](https://arxiv.org/abs/1512.03385). The Amazon SageMaker object detection algorithm supports VGG-16 and ResNet-50 now. It also has a lot of options for hyperparameters that help configure the training job. The next step in our training, is to setup these hyperparameters and data channels for training the model. Consider the following example definition of hyperparameters. See the SageMaker Object Detection [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html) for more details on the hyperparameters.\n",
    "\n",
    "One of the hyperparameters here for instance is the `epochs`. This defines how many passes of the dataset we iterate over and determines that training time of the algorithm. For the sake of demonstration let us run only `30` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_model.set_hyperparameters(base_network='resnet-50',\n",
    "                             use_pretrained_model=1,\n",
    "                             num_classes=62,\n",
    "                             mini_batch_size=32,\n",
    "                             epochs=200,\n",
    "                             learning_rate=0.001,\n",
    "                             lr_scheduler_step='10',\n",
    "                             lr_scheduler_factor=0.1,\n",
    "                             optimizer='rmsprop',\n",
    "                             momentum=0.9,\n",
    "                             weight_decay=0.0005,\n",
    "                             overlap_threshold=0.5,\n",
    "                             nms_threshold=0.45,\n",
    "                             image_shape=500,\n",
    "                             label_width=6000,\n",
    "                             num_training_samples=11738)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyperparameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm consumes. Notice that here we use a `content_type` as `image/jpeg` for the image channels and the annoation channels. Notice how unlike the [RecordIO format](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb), we use four channels here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution='FullyReplicated', \n",
    "                             content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution='FullyReplicated', \n",
    "                             content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data, \n",
    "                 'train_annotation': train_annotation, 'validation_annotation':validation_annotation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `Estimator` object, we have set the hyperparameters for this object and we have our data channels linked with the algorithm. The only remaining thing to do is to train the algorithm. The following cell will train the algorithm. Training the algorithm involves a few steps. Firstly, the instances that we requested while creating the `Estimator` classes are provisioned and are setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take time, depending on the size of the data. Therefore it might be a few minutes before we start getting data logs for our training jobs. The data logs will also print out Mean Average Precision (mAP) on the validation data, among other losses, for every run of the dataset once or one epoch. This metric is a proxy for the quality of the algorithm. \n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-08 20:51:09 Starting - Starting the training job...\n",
      "2020-07-08 20:51:11 Starting - Launching requested ML instances.........\n",
      "2020-07-08 20:53:00 Starting - Preparing the instances for training....................................\n",
      "2020-07-08 20:58:49 Downloading - Downloading input data....................................\n",
      "2020-07-08 21:05:08 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:12 INFO 139773230958400] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'label_width': u'350', u'early_stopping_min_epochs': u'10', u'epochs': u'30', u'overlap_threshold': u'0.5', u'lr_scheduler_factor': u'0.1', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0005', u'mini_batch_size': u'32', u'use_pretrained_model': u'0', u'freeze_layer_pattern': u'', u'lr_scheduler_step': u'', u'early_stopping': u'False', u'early_stopping_patience': u'5', u'momentum': u'0.9', u'num_training_samples': u'', u'optimizer': u'sgd', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.001', u'kv_store': u'device', u'nms_threshold': u'0.45', u'num_classes': u'', u'base_network': u'vgg-16', u'nms_topk': u'400', u'_kvstore': u'device', u'image_shape': u'300'}\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:12 INFO 139773230958400] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.001', u'epochs': u'200', u'nms_threshold': u'0.45', u'optimizer': u'rmsprop', u'base_network': u'resnet-50', u'image_shape': u'500', u'label_width': u'6000', u'lr_scheduler_step': u'10', u'momentum': u'0.9', u'overlap_threshold': u'0.5', u'num_training_samples': u'11738', u'mini_batch_size': u'32', u'weight_decay': u'0.0005', u'use_pretrained_model': u'1', u'num_classes': u'62', u'lr_scheduler_factor': u'0.1'}\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:12 INFO 139773230958400] Final configuration: {u'label_width': u'6000', u'early_stopping_min_epochs': u'10', u'epochs': u'200', u'overlap_threshold': u'0.5', u'lr_scheduler_factor': u'0.1', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0005', u'mini_batch_size': u'32', u'use_pretrained_model': u'1', u'freeze_layer_pattern': u'', u'lr_scheduler_step': u'10', u'early_stopping': u'False', u'early_stopping_patience': u'5', u'momentum': u'0.9', u'num_training_samples': u'11738', u'optimizer': u'rmsprop', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.001', u'kv_store': u'device', u'nms_threshold': u'0.45', u'num_classes': u'62', u'base_network': u'resnet-50', u'nms_topk': u'400', u'_kvstore': u'device', u'image_shape': u'500'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:12 INFO 139773230958400] Using default worker.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:12 INFO 139773230958400] Loaded iterator creator application/x-image for content type ('application/x-image', '1.0')\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:12 INFO 139773230958400] Loaded iterator creator application/x-recordio for content type ('application/x-recordio', '1.0')\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:12 INFO 139773230958400] Loaded iterator creator image/png for content type ('image/png', '1.0')\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:12 INFO 139773230958400] Loaded iterator creator image/jpeg for content type ('image/jpeg', '1.0')\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:12 INFO 139773230958400] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:23 INFO 139773230958400] nvidia-smi took: 0.0756008625031 secs to identify 8 gpus\u001b[0m\n",
      "\u001b[34mCreating .rec file from /opt/ml/input/data/train/train.lst in /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mtime: 0.0194399356842  count: 0\u001b[0m\n",
      "\u001b[34mtime: 0.769713878632  count: 1000\u001b[0m\n",
      "\u001b[34mtime: 0.710005998611  count: 2000\u001b[0m\n",
      "\u001b[34mtime: 0.697429180145  count: 3000\u001b[0m\n",
      "\u001b[34mtime: 0.71376991272  count: 4000\u001b[0m\n",
      "\u001b[34mtime: 0.74152803421  count: 5000\u001b[0m\n",
      "\u001b[34mtime: 0.724376916885  count: 6000\u001b[0m\n",
      "\u001b[34mtime: 0.710453033447  count: 7000\u001b[0m\n",
      "\u001b[34mtime: 0.706862926483  count: 8000\u001b[0m\n",
      "\u001b[34mtime: 0.723992109299  count: 9000\u001b[0m\n",
      "\u001b[34mtime: 0.719393968582  count: 10000\u001b[0m\n",
      "\u001b[34mtime: 0.71245598793  count: 11000\u001b[0m\n",
      "\u001b[34mCreating .rec file from /opt/ml/input/data/validation/val.lst in /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mtime: 0.00942802429199  count: 0\u001b[0m\n",
      "\u001b[34mtime: 0.772835969925  count: 1000\u001b[0m\n",
      "\u001b[34mtime: 0.762013912201  count: 2000\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:40 INFO 139773230958400] Number of GPUs being used: 8\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:40 WARNING 139773230958400] Training images are resized to image shape (3, 500, 500)\u001b[0m\n",
      "\u001b[34m[21:05:40] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:283: ImageDetRecordIOParser: /opt/ml/input/data/train/train.rec, use 48 threads for decoding..\u001b[0m\n",
      "\u001b[34m[21:05:40] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:340: ImageDetRecordIOParser: /opt/ml/input/data/train/train.rec, label padding width: 6000\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:42 WARNING 139773230958400] Validation images are resized to image shape (3, 500, 500)\u001b[0m\n",
      "\u001b[34m[21:05:42] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:283: ImageDetRecordIOParser: /opt/ml/input/data/validation/val.rec, use 32 threads for decoding..\u001b[0m\n",
      "\u001b[34m[21:05:43] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:340: ImageDetRecordIOParser: /opt/ml/input/data/validation/val.rec, label padding width: 6000\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:48 INFO 139773230958400] Number of GPUs being used: 8\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:48 INFO 139773230958400] Using [gpu(0),gpu(1),gpu(2),gpu(3),gpu(4),gpu(5),gpu(6),gpu(7)] as training context.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:48 INFO 139773230958400] Number of GPUs being used: 8\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:48 INFO 139773230958400] Create Store: device\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:48 INFO 139773230958400] Using (gpu(0),gpu(1),gpu(2),gpu(3),gpu(4),gpu(5),gpu(6),gpu(7)) as training context.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:48 INFO 139773230958400] Start training from pretrained model 1.\u001b[0m\n",
      "\u001b[34m[21:05:48] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[21:05:48] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:05:48 INFO 139773230958400] Loaded pretrained model parameters.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:06:28 INFO 139773230958400] Creating a new state instance.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1594242388.191934, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\"}, \"StartTime\": 1594242388.191878}\n",
      "\u001b[0m\n",
      "\u001b[34m[21:06:28] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[21:06:29] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/kvstore/././comm.h:744: only 32 out of 56 GPU pairs are enabled direct access. It may affect the performance. You can set MXNET_ENABLE_GPU_P2P=0 to turn it off\u001b[0m\n",
      "\u001b[34m[21:06:29] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/kvstore/././comm.h:753: .vvvv...\u001b[0m\n",
      "\u001b[34m[21:06:29] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/kvstore/././comm.h:753: v.vv.v..\u001b[0m\n",
      "\u001b[34m[21:06:29] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/kvstore/././comm.h:753: vv.v..v.\u001b[0m\n",
      "\u001b[34m[21:06:29] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/kvstore/././comm.h:753: vvv....v\u001b[0m\n",
      "\u001b[34m[21:06:29] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/kvstore/././comm.h:753: v....vvv\u001b[0m\n",
      "\u001b[34m[21:06:29] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/kvstore/././comm.h:753: .v..v.vv\u001b[0m\n",
      "\u001b[34m[21:06:29] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/kvstore/././comm.h:753: ..v.vv.v\u001b[0m\n",
      "\u001b[34m[21:06:29] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/kvstore/././comm.h:753: ...vvvv.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:07:12 INFO 139773230958400] Epoch:    0, batches:    100, num_examples:   3200, 72.7 samples/sec, epoch time so far:  0:00:44.013503\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:07:49 INFO 139773230958400] Epoch:    0, batches:    200, num_examples:   6400, 78.4 samples/sec, epoch time so far:  0:01:21.613916\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:08:27 INFO 139773230958400] Epoch:    0, batches:    300, num_examples:   9600, 80.3 samples/sec, epoch time so far:  0:01:59.555126\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:08:51 WARNING 139773230958400] Expected number of batches: 366, did not match the number of batches processed: 367. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:08:51 INFO 139773230958400] #quality_metric: host=algo-1, epoch=0, batch=367 train cross_entropy <loss>=(0.9737431754745648)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:08:51 INFO 139773230958400] #quality_metric: host=algo-1, epoch=0, batch=367 train smooth_l1 <loss>=(1.0950280570372783)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:08:51 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:08:52 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:09:33 INFO 139773230958400] #quality_metric: host=algo-1, epoch=0, validation mAP <score>=(0.003928527408456549)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:09:33 INFO 139773230958400] Updating the best model with validation-mAP=0.003928527408456549\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:09:33 INFO 139773230958400] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:09:33 INFO 139773230958400] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1594242573.94793, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 0}, \"StartTime\": 1594242388.192274}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:10:11 INFO 139773230958400] Epoch:    1, batches:    100, num_examples:   3200, 86.3 samples/sec, epoch time so far:  0:00:37.081703\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:10:48 INFO 139773230958400] Epoch:    1, batches:    200, num_examples:   6400, 85.7 samples/sec, epoch time so far:  0:01:14.697589\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:11:25 INFO 139773230958400] Epoch:    1, batches:    300, num_examples:   9600, 85.9 samples/sec, epoch time so far:  0:01:51.711995\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:11:49 WARNING 139773230958400] Expected number of batches: 366, did not match the number of batches processed: 367. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:11:49 INFO 139773230958400] #quality_metric: host=algo-1, epoch=1, batch=367 train cross_entropy <loss>=(0.8246560508496302)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:11:49 INFO 139773230958400] #quality_metric: host=algo-1, epoch=1, batch=367 train smooth_l1 <loss>=(0.9392707642640145)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:11:49 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:11:50 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:12:33 INFO 139773230958400] #quality_metric: host=algo-1, epoch=1, validation mAP <score>=(0.009639593646175252)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:12:33 INFO 139773230958400] Updating the best model with validation-mAP=0.009639593646175252\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:12:33 INFO 139773230958400] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:12:33 INFO 139773230958400] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1594242753.958034, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 1}, \"StartTime\": 1594242573.948081}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:13:12 INFO 139773230958400] Epoch:    2, batches:    100, num_examples:   3200, 82.8 samples/sec, epoch time so far:  0:00:38.661183\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:13:50 INFO 139773230958400] Epoch:    2, batches:    200, num_examples:   6400, 83.5 samples/sec, epoch time so far:  0:01:16.667664\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:14:26 INFO 139773230958400] Epoch:    2, batches:    300, num_examples:   9600, 85.3 samples/sec, epoch time so far:  0:01:52.522852\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:14:49 WARNING 139773230958400] Expected number of batches: 366, did not match the number of batches processed: 367. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:14:49 INFO 139773230958400] #quality_metric: host=algo-1, epoch=2, batch=367 train cross_entropy <loss>=(0.7987175040706141)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:14:49 INFO 139773230958400] #quality_metric: host=algo-1, epoch=2, batch=367 train smooth_l1 <loss>=(0.885331188426964)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:14:49 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:14:50 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:15:37 INFO 139773230958400] #quality_metric: host=algo-1, epoch=2, validation mAP <score>=(0.009988474477677794)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:15:37 INFO 139773230958400] Updating the best model with validation-mAP=0.009988474477677794\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:15:37 INFO 139773230958400] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:15:37 INFO 139773230958400] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1594242937.949368, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 2}, \"StartTime\": 1594242753.958556}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:16:15 INFO 139773230958400] Epoch:    3, batches:    100, num_examples:   3200, 85.1 samples/sec, epoch time so far:  0:00:37.611291\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:16:52 INFO 139773230958400] Epoch:    3, batches:    200, num_examples:   6400, 85.3 samples/sec, epoch time so far:  0:01:15.017290\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:17:31 INFO 139773230958400] Epoch:    3, batches:    300, num_examples:   9600, 84.9 samples/sec, epoch time so far:  0:01:53.096928\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:17:54 WARNING 139773230958400] Expected number of batches: 366, did not match the number of batches processed: 367. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:17:54 INFO 139773230958400] #quality_metric: host=algo-1, epoch=3, batch=367 train cross_entropy <loss>=(0.7828080052830567)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:17:54 INFO 139773230958400] #quality_metric: host=algo-1, epoch=3, batch=367 train smooth_l1 <loss>=(0.8707355602550825)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:17:54 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:17:54 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:18:34 INFO 139773230958400] #quality_metric: host=algo-1, epoch=3, validation mAP <score>=(0.012382298940267432)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:18:34 INFO 139773230958400] Updating the best model with validation-mAP=0.012382298940267432\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:18:34 INFO 139773230958400] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:18:34 INFO 139773230958400] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1594243114.96564, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 3}, \"StartTime\": 1594242937.949657}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:19:12 INFO 139773230958400] Epoch:    4, batches:    100, num_examples:   3200, 84.2 samples/sec, epoch time so far:  0:00:38.013101\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:19:49 INFO 139773230958400] Epoch:    4, batches:    200, num_examples:   6400, 85.5 samples/sec, epoch time so far:  0:01:14.878359\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:20:27 INFO 139773230958400] Epoch:    4, batches:    300, num_examples:   9600, 85.1 samples/sec, epoch time so far:  0:01:52.783789\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:20:51 WARNING 139773230958400] Expected number of batches: 366, did not match the number of batches processed: 367. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:20:51 INFO 139773230958400] #quality_metric: host=algo-1, epoch=4, batch=367 train cross_entropy <loss>=(0.767315405440205)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:20:51 INFO 139773230958400] #quality_metric: host=algo-1, epoch=4, batch=367 train smooth_l1 <loss>=(0.8392843172453528)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:20:51 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:20:51 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:21:35 INFO 139773230958400] #quality_metric: host=algo-1, epoch=4, validation mAP <score>=(0.015927241915325564)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:21:35 INFO 139773230958400] Updating the best model with validation-mAP=0.015927241915325564\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:21:35 INFO 139773230958400] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:21:35 INFO 139773230958400] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1594243295.873803, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 4}, \"StartTime\": 1594243114.965813}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:22:13 INFO 139773230958400] Epoch:    5, batches:    100, num_examples:   3200, 85.8 samples/sec, epoch time so far:  0:00:37.310696\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:22:51 INFO 139773230958400] Epoch:    5, batches:    200, num_examples:   6400, 84.4 samples/sec, epoch time so far:  0:01:15.821720\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:23:29 INFO 139773230958400] Epoch:    5, batches:    300, num_examples:   9600, 84.6 samples/sec, epoch time so far:  0:01:53.536059\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:23:51 INFO 139773230958400] #quality_metric: host=algo-1, epoch=5, batch=366 train cross_entropy <loss>=(0.7606481106910234)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:23:51 INFO 139773230958400] #quality_metric: host=algo-1, epoch=5, batch=366 train smooth_l1 <loss>=(0.8325684757697319)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:23:51 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:23:51 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:24:34 INFO 139773230958400] #quality_metric: host=algo-1, epoch=5, validation mAP <score>=(0.015406420599274375)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:24:34 INFO 139773230958400] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1594243474.366094, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 5}, \"StartTime\": 1594243295.874014}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:25:12 INFO 139773230958400] Epoch:    6, batches:    100, num_examples:   3200, 83.5 samples/sec, epoch time so far:  0:00:38.340114\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:25:49 INFO 139773230958400] Epoch:    6, batches:    200, num_examples:   6400, 85.7 samples/sec, epoch time so far:  0:01:14.695087\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:26:26 INFO 139773230958400] Epoch:    6, batches:    300, num_examples:   9600, 85.3 samples/sec, epoch time so far:  0:01:52.500904\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:26:51 WARNING 139773230958400] Expected number of batches: 366, did not match the number of batches processed: 367. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:26:51 INFO 139773230958400] #quality_metric: host=algo-1, epoch=6, batch=367 train cross_entropy <loss>=(0.7530955629122739)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:26:51 INFO 139773230958400] #quality_metric: host=algo-1, epoch=6, batch=367 train smooth_l1 <loss>=(0.811802501140084)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:26:51 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:26:51 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:27:36 INFO 139773230958400] #quality_metric: host=algo-1, epoch=6, validation mAP <score>=(0.017230170449599957)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:27:36 INFO 139773230958400] Updating the best model with validation-mAP=0.017230170449599957\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:27:36 INFO 139773230958400] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:27:36 INFO 139773230958400] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1594243656.217327, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 6}, \"StartTime\": 1594243474.366281}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:28:13 INFO 139773230958400] Epoch:    7, batches:    100, num_examples:   3200, 85.3 samples/sec, epoch time so far:  0:00:37.533164\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:28:49 INFO 139773230958400] Epoch:    7, batches:    200, num_examples:   6400, 87.5 samples/sec, epoch time so far:  0:01:13.163560\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:29:26 INFO 139773230958400] Epoch:    7, batches:    300, num_examples:   9600, 86.8 samples/sec, epoch time so far:  0:01:50.598779\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:29:50 WARNING 139773230958400] Expected number of batches: 366, did not match the number of batches processed: 367. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:29:50 INFO 139773230958400] #quality_metric: host=algo-1, epoch=7, batch=367 train cross_entropy <loss>=(0.7517539736815336)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:29:50 INFO 139773230958400] #quality_metric: host=algo-1, epoch=7, batch=367 train smooth_l1 <loss>=(0.8154973870039292)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:29:50 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:29:50 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:30:32 INFO 139773230958400] #quality_metric: host=algo-1, epoch=7, validation mAP <score>=(0.015368714582248586)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:30:32 INFO 139773230958400] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1594243832.987628, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 7}, \"StartTime\": 1594243656.217517}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:31:11 INFO 139773230958400] Epoch:    8, batches:    100, num_examples:   3200, 82.1 samples/sec, epoch time so far:  0:00:38.991387\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:31:49 INFO 139773230958400] Epoch:    8, batches:    200, num_examples:   6400, 83.9 samples/sec, epoch time so far:  0:01:16.310286\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:32:28 INFO 139773230958400] Epoch:    8, batches:    300, num_examples:   9600, 83.0 samples/sec, epoch time so far:  0:01:55.613674\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:32:51 WARNING 139773230958400] Expected number of batches: 366, did not match the number of batches processed: 367. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:32:51 INFO 139773230958400] #quality_metric: host=algo-1, epoch=8, batch=367 train cross_entropy <loss>=(0.7470091241475739)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:32:51 INFO 139773230958400] #quality_metric: host=algo-1, epoch=8, batch=367 train smooth_l1 <loss>=(0.7840897943425542)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:32:51 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:32:52 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:33:34 INFO 139773230958400] #quality_metric: host=algo-1, epoch=8, validation mAP <score>=(0.02053583922783297)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:33:34 INFO 139773230958400] Updating the best model with validation-mAP=0.02053583922783297\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:33:34 INFO 139773230958400] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:33:34 INFO 139773230958400] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1594244014.959493, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 8}, \"StartTime\": 1594243832.987772}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:34:13 INFO 139773230958400] Epoch:    9, batches:    100, num_examples:   3200, 84.0 samples/sec, epoch time so far:  0:00:38.085486\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:34:50 INFO 139773230958400] Epoch:    9, batches:    200, num_examples:   6400, 85.3 samples/sec, epoch time so far:  0:01:15.056937\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:35:29 INFO 139773230958400] Epoch:    9, batches:    300, num_examples:   9600, 84.1 samples/sec, epoch time so far:  0:01:54.214597\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:35:52 WARNING 139773230958400] Expected number of batches: 366, did not match the number of batches processed: 367. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:35:52 INFO 139773230958400] #quality_metric: host=algo-1, epoch=9, batch=367 train cross_entropy <loss>=(0.7437249437661635)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:35:52 INFO 139773230958400] #quality_metric: host=algo-1, epoch=9, batch=367 train smooth_l1 <loss>=(0.7932806210219564)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:35:52 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:35:53 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:36:35 INFO 139773230958400] #quality_metric: host=algo-1, epoch=9, validation mAP <score>=(0.019977468037245445)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:36:35 INFO 139773230958400] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1594244195.89438, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 9}, \"StartTime\": 1594244014.959673}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:37:14 INFO 139773230958400] Epoch:    10, batches:    100, num_examples:   3200, 83.8 samples/sec, epoch time so far:  0:00:38.204671\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:37:50 INFO 139773230958400] Epoch:    10, batches:    200, num_examples:   6400, 85.8 samples/sec, epoch time so far:  0:01:14.566588\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:38:28 INFO 139773230958400] Epoch:    10, batches:    300, num_examples:   9600, 85.4 samples/sec, epoch time so far:  0:01:52.453461\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:38:51 INFO 139773230958400] #quality_metric: host=algo-1, epoch=10, batch=366 train cross_entropy <loss>=(0.7378205487138056)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:38:51 INFO 139773230958400] #quality_metric: host=algo-1, epoch=10, batch=366 train smooth_l1 <loss>=(0.7751127988363787)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:38:51 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:38:51 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:39:34 INFO 139773230958400] #quality_metric: host=algo-1, epoch=10, validation mAP <score>=(0.018251054071214665)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:39:34 INFO 139773230958400] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}}, \"EndTime\": 1594244374.773652, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 10}, \"StartTime\": 1594244195.894603}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:40:12 INFO 139773230958400] Epoch:    11, batches:    100, num_examples:   3200, 85.0 samples/sec, epoch time so far:  0:00:37.652253\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:40:51 INFO 139773230958400] Epoch:    11, batches:    200, num_examples:   6400, 83.7 samples/sec, epoch time so far:  0:01:16.423948\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:41:27 INFO 139773230958400] Epoch:    11, batches:    300, num_examples:   9600, 85.3 samples/sec, epoch time so far:  0:01:52.536169\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:41:50 WARNING 139773230958400] Expected number of batches: 366, did not match the number of batches processed: 367. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:41:50 INFO 139773230958400] #quality_metric: host=algo-1, epoch=11, batch=367 train cross_entropy <loss>=(0.737178332770235)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:41:50 INFO 139773230958400] #quality_metric: host=algo-1, epoch=11, batch=367 train smooth_l1 <loss>=(0.7857210022516989)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:41:50 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:41:51 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:42:32 INFO 139773230958400] #quality_metric: host=algo-1, epoch=11, validation mAP <score>=(0.018353310136939752)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:42:32 INFO 139773230958400] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1594244552.772027, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 11}, \"StartTime\": 1594244374.773803}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:43:11 INFO 139773230958400] Epoch:    12, batches:    100, num_examples:   3200, 83.3 samples/sec, epoch time so far:  0:00:38.420131\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:43:48 INFO 139773230958400] Epoch:    12, batches:    200, num_examples:   6400, 84.9 samples/sec, epoch time so far:  0:01:15.352483\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:44:25 INFO 139773230958400] Epoch:    12, batches:    300, num_examples:   9600, 84.8 samples/sec, epoch time so far:  0:01:53.204573\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:44:49 WARNING 139773230958400] Expected number of batches: 366, did not match the number of batches processed: 367. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:44:49 INFO 139773230958400] #quality_metric: host=algo-1, epoch=12, batch=367 train cross_entropy <loss>=(0.7334084822409179)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:44:49 INFO 139773230958400] #quality_metric: host=algo-1, epoch=12, batch=367 train smooth_l1 <loss>=(0.77250245075112)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:44:49 INFO 139773230958400] Round of batches complete\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:44:50 INFO 139773230958400] Updated the metrics\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:45:32 INFO 139773230958400] #quality_metric: host=algo-1, epoch=12, validation mAP <score>=(0.020963001341094956)\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:45:32 INFO 139773230958400] Updating the best model with validation-mAP=0.020963001341094956\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:45:32 INFO 139773230958400] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:45:32 INFO 139773230958400] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}}, \"EndTime\": 1594244732.383237, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 12}, \"StartTime\": 1594244552.772201}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/08/2020 21:46:10 INFO 139773230958400] Epoch:    13, batches:    100, num_examples:   3200, 82.9 samples/sec, epoch time so far:  0:00:38.590445\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "od_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same insantance (or type of instance) that we used to train. Training is a prolonged and compute heavy job that require a different of compute and memory requirements that hosting typically do not. We can choose any type of instance we want to host the model. In our case we chose the `ml.p3.2xlarge` instance to train, but we choose to host the model on the less expensive cpu instance, `ml.m4.xlarge`. The endpoint deployment can be accomplished as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector = incr_od_model2.deploy(initial_instance_count = 1,\n",
    "                                 instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.create_endpoint_config_from_existing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now that the trained model is deployed at an endpoint that is up-and-running, we can use this endpoint for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './data/jpeg_images/1127.jpeg'\n",
    "file_name = './data/jpeg_chips/1127_22.jpeg'\n",
    "\n",
    "with open(file_name, 'rb') as image:\n",
    "    f = image.read()\n",
    "    b = bytearray(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use our endpoint to try to detect objects within this image. Since the image is `jpeg`, we use the appropriate `content_type` to run the prediction job. The endpoint returns a JSON file that we can simply load and peek into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "object_detector.content_type = 'image/jpeg'\n",
    "results = object_detector.predict(b)\n",
    "detections = json.loads(results)\n",
    "print (detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are in a format that is similar to the input .lst file (See [RecordIO Notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb) for more details on the .lst file definition. )with an addition of a confidence score for each detected object. The format of the output can be represented as `[class_index, confidence_score, xmin, ymin, xmax, ymax]`. Typically, we don't consider low-confidence predictions.\n",
    "\n",
    "We have provided additional script to easily visualize the detection outputs. You can visulize the high-confidence preditions with bounding box by filtering out low-confidence detections using the script below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_chip(img_file, dets, classes=[]):\n",
    "        \"\"\"\n",
    "        visualize detections in one image\n",
    "        Parameters:\n",
    "        ----------\n",
    "        img : numpy.array\n",
    "            image, in bgr format\n",
    "        dets : numpy.array\n",
    "            ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n",
    "            each row is one object\n",
    "        classes : tuple or list of str\n",
    "            class names\n",
    "        thresh : float\n",
    "            score threshold\n",
    "        \"\"\"\n",
    "        import random\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.image as mpimg\n",
    "\n",
    "        img=mpimg.imread(img_file)\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        height = 1#img.shape[0]\n",
    "        width = 1#img.shape[1]\n",
    "        colors = dict()\n",
    "        for det in dets['annotations']:\n",
    "            print(det)\n",
    "            klass = det['class_id']\n",
    "            y0 = det['top']\n",
    "            x0 = det['left']\n",
    "            width = det['width']\n",
    "            height = det['height']\n",
    "            cls_id = int(klass)\n",
    "            if cls_id not in colors:\n",
    "                colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "\n",
    "            rect = plt.Rectangle((x0, y0), width,\n",
    "                                 height, fill=False,\n",
    "                                 edgecolor='red',\n",
    "                                 linewidth=3.5)\n",
    "            plt.gca().add_patch(rect)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "N=62\n",
    "# define the colormap\n",
    "cmap = plt.cm.jet\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "# create the new map\n",
    "cmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detection(img_file, dets, classes=[], thresh=0.5):\n",
    "        \"\"\"\n",
    "        visualize detections in one image\n",
    "        Parameters:\n",
    "        ----------\n",
    "        img : numpy.array\n",
    "            image, in bgr format\n",
    "        dets : numpy.array\n",
    "            ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n",
    "            each row is one object\n",
    "        classes : tuple or list of str\n",
    "            class names\n",
    "        thresh : float\n",
    "            score threshold\n",
    "        \"\"\"\n",
    "        import random\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.image as mpimg\n",
    "\n",
    "        img=mpimg.imread(img_file)\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        colors = dict()\n",
    "        for det in dets:\n",
    "            print(det)\n",
    "            (klass, score, x0, y0, x1, y1) = det\n",
    "            if score < thresh:\n",
    "                continue\n",
    "            cls_id = int(klass)\n",
    "            if cls_id not in colors:\n",
    "                colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "                #colors[cls_id] = cmaplist[cls_id][1:]\n",
    "            xmin = int(x0 * width)\n",
    "            ymin = int(y0 * height)\n",
    "            xmax = int(x1 * width)\n",
    "            ymax = int(y1 * height)\n",
    "#             rect = plt.Rectangle((xmin, ymin), xmax - xmin,\n",
    "#                                  ymax - ymin, fill=False,\n",
    "#                                  edgecolor=colors[cls_id],\n",
    "#                                  linewidth=3.5)\n",
    "            rect = plt.Rectangle((xmin, ymin), xmax - xmin,\n",
    "                                 ymax - ymin, fill=False,\n",
    "                                 edgecolor=colors[cls_id],\n",
    "                                 linewidth=3.5)\n",
    "            plt.gca().add_patch(rect)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training(img_file, img_class, img_bbox, classesclasses=[]):\n",
    "        \"\"\"\n",
    "        visualize detections in one image\n",
    "        Parameters:\n",
    "        ----------\n",
    "        img : numpy.array\n",
    "            image, in bgr format\n",
    "        dets : numpy.array\n",
    "            ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n",
    "            each row is one object\n",
    "        classes : tuple or list of str\n",
    "            class names\n",
    "        \"\"\"\n",
    "        import random\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.image as mpimg\n",
    "        \n",
    "        img=mpimg.imread(img_file)\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        height = 1#img.shape[0]\n",
    "        width = 1#img.shape[1]\n",
    "        colors = dict()\n",
    "        \n",
    "        for p,det in enumerate(img_bbox):\n",
    "            (x0, y0, x1, y1) = det\n",
    "            klass = img_class[p]\n",
    "            cls_id = int(klass)\n",
    "            if cls_id not in colors:\n",
    "                colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "            xmin = int(x0 * width)\n",
    "            ymin = int(y0 * height)\n",
    "            xmax = int(x1 * width)\n",
    "            ymax = int(y1 * height)\n",
    "            rect = plt.Rectangle((xmin, ymin), xmax - xmin,\n",
    "                                 ymax - ymin, fill=False,\n",
    "                                 edgecolor=colors[cls_id],\n",
    "                                 linewidth=3.5)\n",
    "            plt.gca().add_patch(rect)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve better detection results, you can try to use the more data from the dataset and train the model for more epochs. Tuning the hyperparameters, such as `mini_batch_size`, `learning_rate`, and `optimizer`, also helps to get a better detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3f9be4fa8ea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/jpeg_images/1127.jpeg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchips\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.tif'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchips\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.tif'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvisualize_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "file_name = './data/jpeg_images/1127.jpeg'\n",
    "fclasses = classes[chips==file_name.split('/')[-1].strip('.jpeg') + '.tif'].astype(np.int64)\n",
    "fbbox = coords[chips==file_name.split('/')[-1].strip('.jpeg') + '.tif']\n",
    "visualize_training(file_name, fclasses, fbbox, list(rlist.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './data/jpeg_chips/1127_22.jpeg'\n",
    "\n",
    "import json\n",
    "with open('./data/generated/1127_22.json') as f:\n",
    "  chip_data = json.load(f)\n",
    "\n",
    "visualize_chip(file_name, chip_data, list(rlist.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Setting a threshold 0.20 will only plot detection results that have a confidence score greater than 0.20.\n",
    "threshold = 0.16\n",
    "\n",
    "# Visualize the detections.\n",
    "visualize_detection(file_name, detections['prediction'], list(rlist.values()), threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(object_detector.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training from a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Given the base estimator, create a new one for incremental training\n",
    "\n",
    "incr_od_model2 = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=2, \n",
    "                                         train_instance_type='ml.p3.8xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000, \n",
    "                                         input_mode = 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess,\n",
    "                                         model_uri=incr_od_model.model_data)  # This parameter will ingest the previous job's model as a new channel\n",
    "\n",
    "incr_od_model2.set_hyperparameters(base_network='resnet-50',\n",
    "                             use_pretrained_model=0,\n",
    "                             num_classes=62,\n",
    "                             mini_batch_size=8,\n",
    "                             epochs=100,\n",
    "                             learning_rate=0.001,\n",
    "                             lr_scheduler_step='10',\n",
    "                             lr_scheduler_factor=0.1,\n",
    "                             optimizer='sgd',\n",
    "                             momentum=0.9,\n",
    "                             weight_decay=0.0005,\n",
    "                             overlap_threshold=0.5,\n",
    "                             nms_threshold=0.45,\n",
    "                             image_shape=500,\n",
    "                             label_width=6000,\n",
    "                             num_training_samples=11738)\n",
    "\n",
    "incr_od_model2.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
