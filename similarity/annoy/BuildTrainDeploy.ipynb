{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4044ee",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook and associated files will be used to train and deploy a custom container in SageMaker.  This notebook uses the Q&A dataset from Kaggle where the objective is to find duplicate questions in the dataset.  We'll use the SimHash library to encode the text into a 64 bit representation and the Spotify Annoy library to build an index.  Once the index is built we'll deploy the index for nearest neighbor search via SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8da84212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simhash in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from simhash) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "# install simhash\n",
    "!pip install simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "05e23e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from simhash import Simhash\n",
    "import re\n",
    "import boto3\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "16c3f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket_name = session.default_bucket()\n",
    "prefix = 'simhash-annoy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc088fb0",
   "metadata": {},
   "source": [
    "# Training and Inference Container\n",
    "\n",
    "To build the custom continer we'll use a Dockerfile, training script, and inference script.  We'll build the container on the notebook instance and upload to ECR.  We are following documentation - https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers-create.html\n",
    "\n",
    "\n",
    "First create the ECR repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "08f62fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = 'simhash-annoy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "07ecba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'simhash-annoy' already exists in the registry with id '431615879134'\n"
     ]
    }
   ],
   "source": [
    "!aws ecr create-repository --repository-name {repo_name} --region {boto3.Session().region_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f49b86",
   "metadata": {},
   "source": [
    "\n",
    "Next, build the container using docker, tag the continer, and push it to the repo above.  \n",
    "\n",
    "*Update the code below with the correct repo and version name*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9ebefbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431615879134\n",
      "Sending build context to Docker daemon    240MB\n",
      "Step 1/15 : FROM python:3.7\n",
      " ---> 9b5e75b69a4f\n",
      "Step 2/15 : ENV PATH=\"/root/miniconda3/bin:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 03da77366816\n",
      "Step 3/15 : ARG PATH=\"/root/miniconda3/bin:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 98632610c687\n",
      "Step 4/15 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 676785a28291\n",
      "Step 5/15 : RUN apt-get install -y wget && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 6030d4c921d9\n",
      "Step 6/15 : RUN wget     https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh     && mkdir /root/.conda     && bash Miniconda3-latest-Linux-x86_64.sh -b     && rm -f Miniconda3-latest-Linux-x86_64.sh\n",
      " ---> Using cache\n",
      " ---> 21ce0c332477\n",
      "Step 7/15 : RUN conda --version\n",
      " ---> Using cache\n",
      " ---> 5d2192b628d2\n",
      "Step 8/15 : RUN pip3 install --no-cache numpy pandas\n",
      " ---> Using cache\n",
      " ---> 5391e01be4ad\n",
      "Step 9/15 : RUN pip3 install --no-cache simhash\n",
      " ---> Using cache\n",
      " ---> d30c0e4e40f6\n",
      "Step 10/15 : RUN conda install -c conda-forge python-annoy\n",
      " ---> Using cache\n",
      " ---> 75f07dcd4936\n",
      "Step 11/15 : RUN pip3 install --no-cache flask\n",
      " ---> Using cache\n",
      " ---> d28eb3cb4a8f\n",
      "Step 12/15 : COPY annoy-train.py /usr/bin/train\n",
      " ---> Using cache\n",
      " ---> 309a1f7dc88b\n",
      "Step 13/15 : COPY annoy-serve.py /usr/bin/serve\n",
      " ---> fde10fff53d9\n",
      "Step 14/15 : RUN chmod 755 /usr/bin/train /usr/bin/serve\n",
      " ---> Running in c22872e49e0f\n",
      "Removing intermediate container c22872e49e0f\n",
      " ---> 0ffcd27df5a1\n",
      "Step 15/15 : EXPOSE 8080\n",
      " ---> Running in 6158b25a3b62\n",
      "Removing intermediate container 6158b25a3b62\n",
      " ---> cac0b446f6b0\n",
      "Successfully built cac0b446f6b0\n",
      "Successfully tagged simhash-annoy:traindeploy\n",
      "Login Succeeded\n",
      "The push refers to repository [431615879134.dkr.ecr.us-east-1.amazonaws.com/simhash-annoy]\n",
      "4f94176522ee: Preparing\n",
      "46cdc0be0b23: Preparing\n",
      "276203ca5840: Preparing\n",
      "52443a2586ca: Preparing\n",
      "6cf763a628e8: Preparing\n",
      "720bf1344079: Preparing\n",
      "923bf2e4078d: Preparing\n",
      "18ba0cba8b8b: Preparing\n",
      "1e382bea8322: Preparing\n",
      "6f0166c14b6b: Preparing\n",
      "c167acc9a8e9: Preparing\n",
      "577d182336e4: Preparing\n",
      "2147747beb86: Preparing\n",
      "1591bf7ec708: Preparing\n",
      "dd3097cd7909: Preparing\n",
      "685934357c89: Preparing\n",
      "ccb9b68523fd: Preparing\n",
      "00bcea93703b: Preparing\n",
      "688e187d6c79: Preparing\n",
      "720bf1344079: Waiting\n",
      "923bf2e4078d: Waiting\n",
      "18ba0cba8b8b: Waiting\n",
      "1e382bea8322: Waiting\n",
      "6f0166c14b6b: Waiting\n",
      "c167acc9a8e9: Waiting\n",
      "577d182336e4: Waiting\n",
      "2147747beb86: Waiting\n",
      "1591bf7ec708: Waiting\n",
      "dd3097cd7909: Waiting\n",
      "685934357c89: Waiting\n",
      "ccb9b68523fd: Waiting\n",
      "00bcea93703b: Waiting\n",
      "688e187d6c79: Waiting\n",
      "46cdc0be0b23: Layer already exists\n",
      "4f94176522ee: Layer already exists\n",
      "6cf763a628e8: Layer already exists\n",
      "276203ca5840: Layer already exists\n",
      "52443a2586ca: Layer already exists\n",
      "720bf1344079: Layer already exists\n",
      "923bf2e4078d: Layer already exists\n",
      "6f0166c14b6b: Layer already exists\n",
      "18ba0cba8b8b: Layer already exists\n",
      "c167acc9a8e9: Layer already exists\n",
      "1e382bea8322: Layer already exists\n",
      "577d182336e4: Layer already exists\n",
      "2147747beb86: Layer already exists\n",
      "1591bf7ec708: Layer already exists\n",
      "dd3097cd7909: Layer already exists\n",
      "685934357c89: Layer already exists\n",
      "ccb9b68523fd: Layer already exists\n",
      "00bcea93703b: Layer already exists\n",
      "688e187d6c79: Layer already exists\n",
      "traindeploy: digest: sha256:8a093b559eed2f9011e315733f5307aa5ce03f6396e26d6c9501695618606290 size: 4315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "export REGION=us-east-1\n",
    "export ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text`\n",
    "export IMAGE_ID=`docker images -q simhash-annoy:traindeploy`\n",
    "echo $ACCOUNT_ID\n",
    "docker build -t simhash-annoy:traindeploy -f Dockerfile ./  \n",
    "docker tag $IMAGE_ID $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/simhash-annoy:traindeploy\n",
    "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/simhash-annoy:traindeploy\n",
    "docker push $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/simhash-annoy:traindeploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d47f2a",
   "metadata": {},
   "source": [
    "# Data\n",
    "Let's prepare our dataset for training and inference.  This dataset is from Kaggle.  We are taking the first 200k question pairs.  Note that not all question pairs are duplicates.  In total we'll use 400k questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "08d00b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df = pd.read_csv('questions.csv')\n",
    "data = np.concatenate((q_df['question1'].values[0:200000], q_df['question2'].values[0:200000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6f7fd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save text array as compressed numpy format.  Compression is reducing the file size by ~1/3\n",
    "np.savez('data.npz',data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2ce9c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data to s3\n",
    "data_path = session.upload_data('data.npz',bucket=bucket_name,key_prefix=prefix+'/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f63f68b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the step by step guide to invest in share market in india?'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142de084",
   "metadata": {},
   "source": [
    "# Estimator\n",
    "\n",
    "Let's define the estimator using our custom container and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e38e02b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f's3://{bucket_name}/{prefix}/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3158fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    image_uri='431615879134.dkr.ecr.us-east-1.amazonaws.com/simhash-annoy:traindeploy',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output=output_path,\n",
    "    hyperparameters={\n",
    "        'dimension': 64,\n",
    "        'distance': 'hamming',\n",
    "        'topk': 25,\n",
    "        'numtrees': 10\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "58ced0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-03 16:38:17 Starting - Starting the training job...\n",
      "2021-06-03 16:38:41 Starting - Launching requested ML instancesProfilerReport-1622738296: InProgress\n",
      "......\n",
      "2021-06-03 16:39:41 Starting - Preparing the instances for training......\n",
      "2021-06-03 16:40:42 Downloading - Downloading input data\n",
      "2021-06-03 16:40:42 Training - Downloading the training image......\n",
      "2021-06-03 16:41:42 Training - Training image download completed. Training in progress.....\u001b[34mhyperparameters: 64 hamming 25 10\u001b[0m\n",
      "\u001b[34mhyperparameters parsed\u001b[0m\n",
      "\u001b[34mData loaded from .npz in 0.31 seconds\u001b[0m\n",
      "\u001b[34mData converted to Simhash in 41.53 seconds\u001b[0m\n",
      "\u001b[34mAnnoy Index built in 6.61 seconds\u001b[0m\n",
      "\u001b[34mModel Saved\u001b[0m\n",
      "\n",
      "2021-06-03 16:42:42 Uploading - Uploading generated training model\n",
      "2021-06-03 16:42:42 Completed - Training job completed\n",
      "Training seconds: 128\n",
      "Billable seconds: 128\n",
      "CPU times: user 518 ms, sys: 32.5 ms, total: 551 ms\n",
      "Wall time: 4min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit({'training':data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6105231",
   "metadata": {},
   "source": [
    "# Model Artifacts\n",
    "\n",
    "Now that we've trained our model, we can find the compressed model artifacts in s3.  Let's copy them to the notebook instance for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cb735f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-431615879134/simhash-annoy-2021-06-03-16-38-16-876/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {estimator.model_data} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "56e9ecc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-431615879134/simhash-annoy-2021-06-03-16-38-16-876/output/model.tar.gz'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a0bdd96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.ann\n"
     ]
    }
   ],
   "source": [
    "# uncompress\n",
    "!tar xvf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f3dd9",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "We'll use sagemaker to deploy the model and send some data through the real time endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0e621d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - code to load a model object from the provided trained model\n",
    "# estimator = sagemaker.model.Model(\n",
    "#     model_data='s3://sagemaker-us-east-1-431615879134/simhash-annoy-2021-06-03-16-38-16-876/output/model.tar.gz',\n",
    "#     image_uri='431615879134.dkr.ecr.us-east-1.amazonaws.com/simhash-annoy:traindeploy',\n",
    "#     role=sagemaker.get_execution_role(),\n",
    "#     predictor_cls = sagemaker.predictor.Predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f96742b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "predictor = estimator.deploy(instance_type='ml.m5.xlarge',initial_instance_count=1,serializer=CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7cb50cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - connect to endpoint that has already been deployed\n",
    "# end_point_name = ''\n",
    "# predictor = sagemaker.predictor.Predictor(end_point_name,sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "881edaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# character n-gram function\n",
    "def get_features(s):\n",
    "    width = 3\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^\\w]+', '', s)\n",
    "    return [s[i:i + width] for i in range(max(len(s) - width + 1, 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "74a15237",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = 'What is the step by step guide to invest in share market in india?'\n",
    "text_hash = Simhash(get_features(str(test_text))).value\n",
    "binary_hash = format(int(text_hash), f'0{64}b')\n",
    "vector_hash = ''\n",
    "for i,char in enumerate(binary_hash):\n",
    "    if i != len(binary_hash)-1:\n",
    "        vector_hash = vector_hash + char + \", \"\n",
    "    else:\n",
    "        vector_hash = vector_hash + char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d2003d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Latency 184.72 ms\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "response = predictor.predict(vector_hash)\n",
    "print(f'Prediction Latency {1000*(time.time()-t):.2f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ba0ce418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the step by step guide to invest in share market in india?\n",
      "-------------------------\n",
      "What is the step by step guide to invest in share market?\n",
      "Which movies are the best examples of the Hero's Journey?\n",
      "How can I invest $100 into myself?\n",
      "How can I invest $100 into myself?\n",
      "How can I invest $100 into myself?\n",
      "What is the biggest animal with more than four legs?\n",
      "Which is the best way to invest in stock market?\n",
      "Which is the best way to invest in stock market?\n",
      "What are the best places to visit in San Francisco at night?\n",
      "What are the best long term investment options in India?\n",
      "Which are the best love story novels?\n",
      "Which are the best love story novels?\n",
      "What are the best musical venues in San Francisco?\n",
      "What are the best long term investment options in India?\n",
      "How can I cultivate patience?\n",
      "Which are the best love story novels?\n",
      "What are the best ways to trade in share markets in India?\n",
      "Which are the best love story novels?\n",
      "What are the best examples of \"girls will be girls\"?\n",
      "Which are the best universities in Germany for MS in automotive and aerospace engineering?\n",
      "Where can I tryout various Sennheiser headphones in San Francisco?\n",
      "Which is the best mutual funds to invest in India?\n",
      "Which is the best mutual funds to invest in India?\n",
      "Who are some of the best web design firms in Los Angeles?\n"
     ]
    }
   ],
   "source": [
    "for q,res in enumerate(json.loads(response.decode(encoding='utf8'))):\n",
    "    print(data[res])\n",
    "    if q == 0: print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de839e",
   "metadata": {},
   "source": [
    "# Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dae9128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping docker: \u001b[60G[\u001b[0;32m  OK  \u001b[0;39m]\n",
      "Starting docker:\t.\u001b[60G[\u001b[0;32m  OK  \u001b[0;39m]\n"
     ]
    }
   ],
   "source": [
    "# used if docker gets in a strange state\n",
    "!sudo service docker restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2b8006e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
