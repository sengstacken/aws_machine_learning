{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook provides an overview on how to get started with natural language processing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is NLP?\n",
    "\n",
    "Natural language processing (NLP) is the analysis of written text.  It is a subfield of linguistics, computer science, and statistics.  Historically, the following problems are used within NLP:\n",
    "\n",
    "* Sentiment Classification\n",
    "* Topic Modeling\n",
    "* Language Translation\n",
    "* Text Classification\n",
    "* Language Generation\n",
    "* Speech to Text and Text to Speech\n",
    "* Named Entity Regonition\n",
    "* Part of speech tagging\n",
    "* Text Summarization\n",
    "* Question Answering\n",
    "\n",
    "The field of NLP is rapidly changing and has seen an explosion of interest in the last three years.  This is mainly due to the application of very deep neural networks to NLP problems.  \n",
    "\n",
    "\n",
    "Why is NLP hard?\n",
    "\n",
    "Consider the following:\n",
    "**\"She killed the man with the tie.\"** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Modeling\n",
    "\n",
    "Topic modeling is an unsupervised approach used to determine the topics of text based on the word choice and word frequency.  Let's take a look at traditional topic modeling using linear algebra. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn import decomposition\n",
    "from glob import glob\n",
    "from scipy import linalg\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "from nltk import stem\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Scikit Learn comes with a number of built-in datasets, as well as loading utilities to load several standard external datasets. The datasets include Boston housing prices, face images, patches of forest, diabetes, breast cancer, and more. We will be using the newsgroups dataset.\n",
    "\n",
    "Newsgroups are discussion groups on Usenet, which was popular in the 80s and 90s before the web really took off. This dataset includes 18,000 newsgroups posts with 20 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2034,), (2034,), (1353,), (1353,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.filenames.shape, newsgroups_train.target.shape, newsgroups_test.filenames.shape, newsgroups_test.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Their Hiten engineering-test mission spent a while in a highly eccentric\n",
      "Earth orbit doing lunar flybys, and then was inserted into lunar orbit\n",
      "using some very tricky gravity-assist-like maneuvering.  This meant that\n",
      "it would crash on the Moon eventually, since there is no such thing as\n",
      "a stable lunar orbit (as far as anyone knows), and I believe I recall\n",
      "hearing recently that it was about to happen.\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.DataFrame(newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(categories,target_df[0].value_counts().sort_index().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TAKE AWAY** Data is loaded and the distribution of data is nearly balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we prepare the data?\n",
    "\n",
    "We need a way to convert the text data into a numerical representation.  This is done via Tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "Bags of words is a common model / approach for text processing.  Bags of words disregards the order / structure of the text but keeps the multiplicity.  This approach is commonly used in document classification where the frequency of each word is important in the classifying documents.  The bag of words approach is useful to convey which words were in the text and how often did each word get used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    'John and Mary jumped in the pool.',\n",
    "    'Tom went to John\\'s house to use his pool this summer.',\n",
    "    'The basketball players jumped.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x17 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(docs)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 1 1 0 1 0 0 0 1 1 0 1 2 1 1 1]\n",
      " [0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'basketball',\n",
       " 'his',\n",
       " 'house',\n",
       " 'in',\n",
       " 'john',\n",
       " 'jumped',\n",
       " 'mary',\n",
       " 'players',\n",
       " 'pool',\n",
       " 'summer',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to',\n",
       " 'tom',\n",
       " 'use',\n",
       " 'went']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "N-grams are just sequences of words.  N represents the number of words in each sequence.  2-grams or bigrams are two word sequences, 3-grams or trigrams are three word sequences.  N-grams have historically been used in early language models to assign a probablity of the occurance of an N-gram.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bigram_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and mary',\n",
       " 'basketball players',\n",
       " 'his pool',\n",
       " 'house to',\n",
       " 'in the',\n",
       " 'john and',\n",
       " 'john house',\n",
       " 'jumped in',\n",
       " 'mary jumped',\n",
       " 'players jumped',\n",
       " 'pool this',\n",
       " 'the basketball',\n",
       " 'the pool',\n",
       " 'this summer',\n",
       " 'to john',\n",
       " 'to use',\n",
       " 'tom went',\n",
       " 'use his',\n",
       " 'went to']"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more commonly when higher order n-grams are used the lower n-grams are also included.  So for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bigram_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about words that are similar?\n",
    "\n",
    "We can use Lemmatization or Stemming to group similar words together.  Lemmatization considers the word context and converts the word into the meanful base form (lemmas).  Stemming is a \"poor man's version\" of Lemmatization where only the stem of the word is kept and the remainder is discarded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = stem.WordNetLemmatizer()\n",
    "porter = stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_list = ['organize','organizes','organizing','organized']\n",
    "word_list = ['fly', 'flies', 'flying']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fly', 'fly', 'flying']"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Lemmatization')\n",
    "[wnl.lemmatize(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fli', 'fli', 'fli']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Stemming')\n",
    "[porter.stem(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE!** Stemming and Lemmatization are language dependent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about common words?\n",
    "\n",
    "Common words in NLP are typically referred to 'Stop Words'.  These can be removed by using various stop word lists.  Also note that many tokenizers include options for stop word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "sorted(list(stop_words.ENGLISH_STOP_WORDS))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We've shown some of the common preprocessing steps that are used in NLP applications - stop word removal, stemming, lemmitization, and tokenization.  Let's use our dataset to model the topics of the text.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "To utilize text in machine learning we need to convert text (strings) into numbers.  There are numerous ways to do this.  The generic process of converting text into numbers is called Tokenization\n",
    "\n",
    "A tokenizer will first split a given text in words (or part of words, punctuation symbols, etc.) usually called tokens. Then it will convert those tokens into numbers, to be able to build a tensor out of them and feed them to the model.\n",
    "\n",
    "**NOTE:** If you plan on using a pretrained model, itâ€™s important to use the associated pretrained tokenizer: it will split the text you give it in tokens the same way for the pretraining corpus, and it will use the same correspondence token to index (that we usually call a vocab) as during pretraining. This is similar to preprocessing on pretrained computer vision models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=2000, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Their Hiten engineering-test mission spent a while in a highly eccentric\n",
      "Earth orbit doing lunar flybys, and then was inserted into lunar orbit\n",
      "using some very tricky gravity-assist-like maneuvering.  This meant that\n",
      "it would crash on the Moon eventually, since there is no such thing as\n",
      "a stable lunar orbit (as far as anyone knows), and I believe I recall\n",
      "hearing recently that it was about to happen.\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2397, 15375, 1424, 3752, 118, 2774, 2862, 2097, 170, 1229, 1107, 170, 3023, 20276, 2746, 8895, 1833, 15383, 4689, 2665, 1116, 117, 1105, 1173, 1108, 13137, 1154, 15383, 8895, 1606, 1199, 1304, 7959, 1183, 9926, 118, 6043, 118, 1176, 19844, 1158, 119, 1188, 2318, 1115, 1122, 1156, 5683, 1113, 1103, 5148, 2028, 117, 1290, 1175, 1110, 1185, 1216, 1645, 1112, 170, 6111, 15383, 8895, 113, 1112, 1677, 1112, 2256, 3520, 114, 117, 1105, 146, 2059, 146, 9148, 4510, 3055, 1115, 1122, 1108, 1164, 1106, 3333, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(newsgroups_train.data[8])\n",
    "print(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Their Hiten engineering - test mission spent a while in a highly eccentric Earth orbit doing lunar flybys, and then was inserted into lunar orbit using some very tricky gravity - assist - like maneuvering. This meant that it would crash on the Moon eventually, since there is no such thing as a stable lunar orbit ( as far as anyone knows ), and I believe I recall hearing recently that it was about to happen. [SEP]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if len(t) >= 2 and re.match(\"[a-z].*\",t) \n",
    "                and re.match(token_pattern, t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and counting, this may take a few minutes...\n",
      "CPU times: user 7.2 s, sys: 0 ns, total: 7.2 s\n",
      "Wall time: 7.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab_size = 2000\n",
    "print('Tokenizing and counting, this may take a few minutes...')\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english',\n",
    "                             tokenizer=LemmaTokenizer(), max_features=vocab_size, max_df=0.95, min_df=2)\n",
    "#vectorizer = CountVectorizer(stop_words='english',max_features=vocab_size, min_df=2)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 2000)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    for v in vocab:\n",
    "        f.write(v+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['archive-name', 'area', 'argue', 'argument', 'ariane',\n",
       "       'arithmetic', 'arm', 'army', 'array', 'art', 'article', 'ascii',\n",
       "       'aside', 'ask', 'asked', 'asking', 'aspect', 'assembly',\n",
       "       'assertion', 'assist'], dtype='<U17')"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Negative Factorization (NMF)\n",
    "\n",
    "Similar factorization technique as singualr value decomposition (SVD) but instead of orthonormal, the factors are constrained to be non-negative.  Most consider NMF to be more interpretable than SVD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n=vectors.shape\n",
    "d=4  # num topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = decomposition.NMF(n_components=d, random_state=1)\n",
    "\n",
    "W1 = clf.fit_transform(vectors)\n",
    "H1 = clf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 2.33796219e-01, 6.28239986e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.28996576e-04],\n",
       "       [1.09611259e-01, 6.75114209e-02, 0.00000000e+00, ...,\n",
       "        2.70023897e-02, 4.23334157e-02, 0.00000000e+00],\n",
       "       [6.19737942e-02, 2.18224097e-01, 0.00000000e+00, ...,\n",
       "        2.13591256e-01, 5.63505050e-02, 1.35730824e-01],\n",
       "       [1.77196875e-02, 2.10536258e-01, 6.69453375e-01, ...,\n",
       "        6.42269509e-03, 1.49264527e-02, 1.94308389e-02]])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words=8\n",
    "\n",
    "def show_topics(a,vocab):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jpeg image file gif format color version program',\n",
       " 'image data available file package graphic ftp format',\n",
       " 'wa jesus god atheist ha people matthew doe',\n",
       " 'space launch satellite wa commercial year market ha']"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H1,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Topic Frequency-Inverse Document Frequency (TF-IDF) is a way to normalize term counts by taking into account how often they appear in a document, how long the document is, and how commmon/rare the term is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english', min_df=2)\n",
    "vectorizer_tfidf = TfidfVectorizer(input='content', analyzer='word', stop_words='english',\n",
    "                             tokenizer=LemmaTokenizer(), max_df=0.95, min_df=2)\n",
    "vectors_tfidf = vectorizer_tfidf.fit_transform(newsgroups_train.data) # (documents, vocab)\n",
    "vocab_tfidf = np.array(vectorizer_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=4  # num topics\n",
    "clf = decomposition.NMF(n_components=d, random_state=1)\n",
    "W1 = clf.fit_transform(vectors_tfidf)\n",
    "H1 = clf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wa think people just did like ha say',\n",
       " 'file image thanks format program graphic know color',\n",
       " 'space launch nasa satellite orbit shuttle mission moon',\n",
       " 'god jesus christian bible believe belief doe atheism']"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H1,vocab_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker NTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, boto3\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectors.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2034x2000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 73608 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buf = io.BytesIO()\n",
    "smac.write_spmatrix_to_sparse_tensor(buf, vectors, None)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'topic-modeling'\n",
    "train_key = 'usenet.protobuf'\n",
    "obj = f'{prefix}/{train_key}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "s3_train_path = f's3://{bucket}/{obj}'\n",
    "s3_output = f's3://{bucket}/{prefix}/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_aux_path = sess.upload_data(path='vocab.txt',bucket=bucket,key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-431615879134/topic-modeling/vocab.txt'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_aux_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-431615879134/topic-modeling/usenet.protobuf\n",
      "s3://sagemaker-us-east-1-431615879134/topic-modeling/output/\n",
      "s3://sagemaker-us-east-1-431615879134/topic-modeling/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "print(s3_train_path)\n",
    "print(s3_output)\n",
    "print(s3_aux_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_container = sagemaker.image_uris.retrieve('ntm','us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm = sagemaker.estimator.Estimator(ntm_container,\n",
    "                                   role = sagemaker.get_execution_role(),\n",
    "                                   instance_count = 1,\n",
    "                                   instance_type = 'ml.p3.2xlarge',\n",
    "                                   output_path = s3_output,\n",
    "                                   sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm.set_hyperparameters(num_topics=5, \n",
    "                        feature_dim=len(vocab),\n",
    "                        optimizer='adam', \n",
    "                        mini_batch_size=256,\n",
    "                        epochs=100,\n",
    "                        num_patience_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-11 23:17:06 Starting - Starting the training job...\n",
      "2021-01-11 23:17:30 Starting - Launching requested ML instancesProfilerReport-1610407026: InProgress\n",
      "......\n",
      "2021-01-11 23:18:31 Starting - Preparing the instances for training......\n",
      "2021-01-11 23:19:33 Downloading - Downloading input data...\n",
      "2021-01-11 23:19:52 Training - Downloading the training image..\n",
      "2021-01-11 23:20:33 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:21 INFO 140226321442624] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:21 INFO 140226321442624] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'10', u'optimizer': u'adam', u'num_topics': u'5', u'epochs': u'100', u'feature_dim': u'2000', u'mini_batch_size': u'256'}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:21 INFO 140226321442624] Final configuration: {u'optimizer': u'adam', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'2000', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'10', u'epochs': u'100', u'mini_batch_size': u'256', u'num_topics': u'5', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:21 INFO 140226321442624] nvidia-smi took: 0.075453042984 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:21 INFO 140226321442624] Using default worker.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:21 INFO 140226321442624] Initializing\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:21 INFO 140226321442624] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:21 INFO 140226321442624] vocab.txt\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:21 INFO 140226321442624] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:21 INFO 140226321442624] Loading pre-trained token embedding vectors from /opt/amazon/lib/python2.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:34 WARNING 140226321442624] 21 out of 2000 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:34 INFO 140226321442624] Vocab embedding shape\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:34 INFO 140226321442624] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Create Store: device\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1610407242.554996, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1610407242.554957}\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.555] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 21417, \"num_examples\": 1, \"num_bytes\": 66196}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.597] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 41, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 1 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 7.22635018826\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.00314957042065\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 7.22320067883\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 7.22635018826\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=7.22635018826\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.05s, val: 0.00s, epoch: 0.05s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Total Records Seen\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1610407242.60226, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1610407242.555266}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=43185.5262316 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.629] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 2 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 7.0648189187\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.030379985692\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 7.0344388485\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 7.0648189187\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=7.0648189187\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}, \"Total Records Seen\": {\"count\": 1, \"max\": 4068, \"sum\": 4068.0, \"min\": 4068}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1610407242.633425, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1610407242.602521}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=65477.6527799 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.660] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 3 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 6.9300403595\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.0585723505355\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 6.87146800756\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 6.9300403595\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=6.9300403595\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}, \"Total Records Seen\": {\"count\": 1, \"max\": 6102, \"sum\": 6102.0, \"min\": 6102}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1610407242.665888, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1610407242.633731}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=62964.7088832 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.695] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 4 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 6.84845250845\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.049124410376\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 6.79932808876\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 6.84845250845\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=6.84845250845\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}, \"Total Records Seen\": {\"count\": 1, \"max\": 8136, \"sum\": 8136.0, \"min\": 8136}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1610407242.699812, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1610407242.666141}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60143.7769992 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.729] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 29, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 5 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 6.79723972082\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.0356571811717\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 6.76158255339\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 6.79723972082\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=6.79723972082\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}, \"Total Records Seen\": {\"count\": 1, \"max\": 10170, \"sum\": 10170.0, \"min\": 10170}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1610407242.733592, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1610407242.700103}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60465.6134721 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.763] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 29, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 6 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 6.78317773342\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.0310471793637\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 6.75213062763\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 6.78317773342\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=6.78317773342\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}, \"Total Records Seen\": {\"count\": 1, \"max\": 12204, \"sum\": 12204.0, \"min\": 12204}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1610407242.768192, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1610407242.733874}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=59026.4739712 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.797] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 7 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 6.77443259954\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.0266157924198\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 6.74781668186\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 6.77443259954\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=6.77443259954\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}, \"Total Records Seen\": {\"count\": 1, \"max\": 14238, \"sum\": 14238.0, \"min\": 14238}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1610407242.801872, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1610407242.768434}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60620.7185055 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.830] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 8 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 6.75306683779\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.0307638838422\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 6.72230309248\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 6.75306683779\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=6.75306683779\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}, \"Total Records Seen\": {\"count\": 1, \"max\": 16272, \"sum\": 16272.0, \"min\": 16272}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1610407242.834117, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1610407242.802123}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=63307.1953339 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.863] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 9 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 6.7484819293\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.0263706163969\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 6.72211134434\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 6.7484819293\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=6.7484819293\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}, \"Total Records Seen\": {\"count\": 1, \"max\": 18306, \"sum\": 18306.0, \"min\": 18306}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1610407242.867426, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1610407242.83437}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=61268.5329675 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.896] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 10 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 6.74359619617\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.0234696359839\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 6.72012645006\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 6.74359619617\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=6.74359619617\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}, \"Total Records Seen\": {\"count\": 1, \"max\": 20340, \"sum\": 20340.0, \"min\": 20340}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1610407242.900079, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1610407242.867705}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=62561.6128479 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.927] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 11 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 6.73389780521\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.025924856076\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 6.70797282457\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 6.73389780521\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=6.73389780521\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] patience losses:[7.22635018825531, 7.064818918704987, 6.93004035949707, 6.848452508449554, 6.797239720821381, 6.783177733421326, 6.774432599544525, 6.753066837787628, 6.748481929302216, 6.743596196174622] min patience loss:6.74359619617 current loss:6.73389780521 absolute loss difference:0.00969839096069\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 88, \"sum\": 88.0, \"min\": 88}, \"Total Records Seen\": {\"count\": 1, \"max\": 22374, \"sum\": 22374.0, \"min\": 22374}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1610407242.931753, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1610407242.900346}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=64465.3412928 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.960] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 12 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 6.72818046808\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.0228877868503\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 6.70529264212\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 6.72818046808\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=6.72818046808\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] patience losses:[7.064818918704987, 6.93004035949707, 6.848452508449554, 6.797239720821381, 6.783177733421326, 6.774432599544525, 6.753066837787628, 6.748481929302216, 6.743596196174622, 6.733897805213928] min patience loss:6.73389780521 current loss:6.72818046808 absolute loss difference:0.0057173371315\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}, \"Total Records Seen\": {\"count\": 1, \"max\": 24408, \"sum\": 24408.0, \"min\": 24408}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1610407242.965095, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1610407242.932013}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=61257.5346526 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:42.993] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Finished training epoch 13 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) total: 6.72776544094\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) kld: 0.0220922818407\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) recons: 6.70567321777\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Loss (name: value) logppx: 6.72776544094\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=6.72776544094\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] patience losses:[6.93004035949707, 6.848452508449554, 6.797239720821381, 6.783177733421326, 6.774432599544525, 6.753066837787628, 6.748481929302216, 6.743596196174622, 6.733897805213928, 6.728180468082428] min patience loss:6.72818046808 current loss:6.72776544094 absolute loss difference:0.000415027141571\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 104, \"sum\": 104.0, \"min\": 104}, \"Total Records Seen\": {\"count\": 1, \"max\": 26442, \"sum\": 26442.0, \"min\": 26442}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1610407242.997676, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1610407242.965355}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=62687.1111895 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:42 INFO 140226321442624] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.026] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 14 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.71895760298\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0230110110715\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.695946455\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.71895760298\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=6.71895760298\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.848452508449554, 6.797239720821381, 6.783177733421326, 6.774432599544525, 6.753066837787628, 6.748481929302216, 6.743596196174622, 6.733897805213928, 6.728180468082428, 6.727765440940857] min patience loss:6.72776544094 current loss:6.71895760298 absolute loss difference:0.0088078379631\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 112, \"sum\": 112.0, \"min\": 112}, \"Total Records Seen\": {\"count\": 1, \"max\": 28476, \"sum\": 28476.0, \"min\": 28476}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1610407243.030406, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1610407242.997937}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=62419.7134516 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.060] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 29, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 15 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.71147680283\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0238487164024\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.68762820959\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.71147680283\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=6.71147680283\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.797239720821381, 6.783177733421326, 6.774432599544525, 6.753066837787628, 6.748481929302216, 6.743596196174622, 6.733897805213928, 6.728180468082428, 6.727765440940857, 6.718957602977753] min patience loss:6.71895760298 current loss:6.71147680283 absolute loss difference:0.00748080015182\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 120, \"sum\": 120.0, \"min\": 120}, \"Total Records Seen\": {\"count\": 1, \"max\": 30510, \"sum\": 30510.0, \"min\": 30510}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1610407243.064266, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1610407243.030655}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60281.0430457 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.092] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 16 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.71002435684\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0214118373115\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.68861269951\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.71002435684\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=6.71002435684\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.783177733421326, 6.774432599544525, 6.753066837787628, 6.748481929302216, 6.743596196174622, 6.733897805213928, 6.728180468082428, 6.727765440940857, 6.718957602977753, 6.711476802825928] min patience loss:6.71147680283 current loss:6.71002435684 absolute loss difference:0.00145244598389\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 128, \"sum\": 128.0, \"min\": 128}, \"Total Records Seen\": {\"count\": 1, \"max\": 32544, \"sum\": 32544.0, \"min\": 32544}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1610407243.096661, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1610407243.064531}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=63068.0441783 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.125] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 17 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.70427906513\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0200139684603\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.68426525593\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.70427906513\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=6.70427906513\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.774432599544525, 6.753066837787628, 6.748481929302216, 6.743596196174622, 6.733897805213928, 6.728180468082428, 6.727765440940857, 6.718957602977753, 6.711476802825928, 6.710024356842041] min patience loss:6.71002435684 current loss:6.70427906513 absolute loss difference:0.0057452917099\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 136, \"sum\": 136.0, \"min\": 136}, \"Total Records Seen\": {\"count\": 1, \"max\": 34578, \"sum\": 34578.0, \"min\": 34578}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1610407243.129743, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1610407243.096933}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=61765.3420212 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.158] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 18 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.70305305719\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0227509411052\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.6803022027\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.70305305719\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=6.70305305719\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.753066837787628, 6.748481929302216, 6.743596196174622, 6.733897805213928, 6.728180468082428, 6.727765440940857, 6.718957602977753, 6.711476802825928, 6.710024356842041, 6.704279065132141] min patience loss:6.70427906513 current loss:6.70305305719 absolute loss difference:0.00122600793839\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 144, \"sum\": 144.0, \"min\": 144}, \"Total Records Seen\": {\"count\": 1, \"max\": 36612, \"sum\": 36612.0, \"min\": 36612}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1610407243.163994, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1610407243.130027}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=59634.2371749 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.192] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 19 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.69954490662\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0206790862139\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.67886579037\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.69954490662\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=6.69954490662\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.748481929302216, 6.743596196174622, 6.733897805213928, 6.728180468082428, 6.727765440940857, 6.718957602977753, 6.711476802825928, 6.710024356842041, 6.704279065132141, 6.703053057193756] min patience loss:6.70305305719 current loss:6.69954490662 absolute loss difference:0.00350815057755\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 152, \"sum\": 152.0, \"min\": 152}, \"Total Records Seen\": {\"count\": 1, \"max\": 38646, \"sum\": 38646.0, \"min\": 38646}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1610407243.197561, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1610407243.164258}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60854.6567944 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.223] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 25, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 20 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.69131433964\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.020046363119\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.67126786709\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.69131433964\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=6.69131433964\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.743596196174622, 6.733897805213928, 6.728180468082428, 6.727765440940857, 6.718957602977753, 6.711476802825928, 6.710024356842041, 6.704279065132141, 6.703053057193756, 6.699544906616211] min patience loss:6.69954490662 current loss:6.69131433964 absolute loss difference:0.00823056697845\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 160, \"sum\": 160.0, \"min\": 160}, \"Total Records Seen\": {\"count\": 1, \"max\": 40680, \"sum\": 40680.0, \"min\": 40680}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1610407243.229072, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1610407243.197822}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=64816.5135959 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.255] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 21 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.69282799959\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0204665458295\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.6723613739\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.69282799959\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=6.69282799959\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.733897805213928, 6.728180468082428, 6.727765440940857, 6.718957602977753, 6.711476802825928, 6.710024356842041, 6.704279065132141, 6.703053057193756, 6.699544906616211, 6.691314339637756] min patience loss:6.69131433964 current loss:6.69282799959 absolute loss difference:0.00151365995407\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 168, \"sum\": 168.0, \"min\": 168}, \"Total Records Seen\": {\"count\": 1, \"max\": 42714, \"sum\": 42714.0, \"min\": 42714}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1610407243.256745, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1610407243.229321}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=73784.1134713 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.284] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 22 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.68642550707\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0220696944743\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.66435581446\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.68642550707\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=6.68642550707\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.728180468082428, 6.727765440940857, 6.718957602977753, 6.711476802825928, 6.710024356842041, 6.704279065132141, 6.703053057193756, 6.699544906616211, 6.691314339637756, 6.692827999591827] min patience loss:6.69131433964 current loss:6.68642550707 absolute loss difference:0.00488883256912\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 176, \"sum\": 176.0, \"min\": 176}, \"Total Records Seen\": {\"count\": 1, \"max\": 44748, \"sum\": 44748.0, \"min\": 44748}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1610407243.289337, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1610407243.256988}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=62642.4625777 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.317] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 23 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.68202829361\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0208362352569\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.6611918807\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.68202829361\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=6.68202829361\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.727765440940857, 6.718957602977753, 6.711476802825928, 6.710024356842041, 6.704279065132141, 6.703053057193756, 6.699544906616211, 6.691314339637756, 6.692827999591827, 6.686425507068634] min patience loss:6.68642550707 current loss:6.68202829361 absolute loss difference:0.00439721345901\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 184, \"sum\": 184.0, \"min\": 184}, \"Total Records Seen\": {\"count\": 1, \"max\": 46782, \"sum\": 46782.0, \"min\": 46782}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1610407243.32161, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1610407243.289589}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=63277.6129712 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.348] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 24 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.67890912294\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0223631642293\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.65654581785\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.67890912294\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=6.67890912294\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.718957602977753, 6.711476802825928, 6.710024356842041, 6.704279065132141, 6.703053057193756, 6.699544906616211, 6.691314339637756, 6.692827999591827, 6.686425507068634, 6.682028293609619] min patience loss:6.68202829361 current loss:6.67890912294 absolute loss difference:0.00311917066574\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 192, \"sum\": 192.0, \"min\": 192}, \"Total Records Seen\": {\"count\": 1, \"max\": 48816, \"sum\": 48816.0, \"min\": 48816}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1610407243.352556, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1610407243.321876}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=66040.7825919 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.378] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 25, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 25 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.67538321018\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0237583713606\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.65162485838\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.67538321018\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=6.67538321018\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.711476802825928, 6.710024356842041, 6.704279065132141, 6.703053057193756, 6.699544906616211, 6.691314339637756, 6.692827999591827, 6.686425507068634, 6.682028293609619, 6.678909122943878] min patience loss:6.67890912294 current loss:6.67538321018 absolute loss difference:0.00352591276169\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 200, \"sum\": 200.0, \"min\": 200}, \"Total Records Seen\": {\"count\": 1, \"max\": 50850, \"sum\": 50850.0, \"min\": 50850}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1610407243.383352, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1610407243.35281}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=66314.3953734 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.410] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 26 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.67097353935\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0228580699768\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.6481153965\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.67097353935\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=6.67097353935\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.710024356842041, 6.704279065132141, 6.703053057193756, 6.699544906616211, 6.691314339637756, 6.692827999591827, 6.686425507068634, 6.682028293609619, 6.678909122943878, 6.67538321018219] min patience loss:6.67538321018 current loss:6.67097353935 absolute loss difference:0.00440967082977\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 208, \"sum\": 208.0, \"min\": 208}, \"Total Records Seen\": {\"count\": 1, \"max\": 52884, \"sum\": 52884.0, \"min\": 52884}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1610407243.415241, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1610407243.383613}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=64014.5144143 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.444] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 29, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 27 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.66967582703\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0222909089644\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.64738482237\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.66967582703\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=6.66967582703\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.704279065132141, 6.703053057193756, 6.699544906616211, 6.691314339637756, 6.692827999591827, 6.686425507068634, 6.682028293609619, 6.678909122943878, 6.67538321018219, 6.670973539352417] min patience loss:6.67097353935 current loss:6.66967582703 absolute loss difference:0.00129771232605\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 216, \"sum\": 216.0, \"min\": 216}, \"Total Records Seen\": {\"count\": 1, \"max\": 54918, \"sum\": 54918.0, \"min\": 54918}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1610407243.449177, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1610407243.415526}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60215.0942341 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.478] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 28 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.66818708181\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0250264836941\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.6431607008\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.66818708181\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=6.66818708181\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.703053057193756, 6.699544906616211, 6.691314339637756, 6.692827999591827, 6.686425507068634, 6.682028293609619, 6.678909122943878, 6.67538321018219, 6.670973539352417, 6.669675827026367] min patience loss:6.66967582703 current loss:6.66818708181 absolute loss difference:0.00148874521255\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 224, \"sum\": 224.0, \"min\": 224}, \"Total Records Seen\": {\"count\": 1, \"max\": 56952, \"sum\": 56952.0, \"min\": 56952}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1610407243.482598, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 27}, \"StartTime\": 1610407243.449452}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=61156.5351188 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.511] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 29 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.66431236267\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0222946403082\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.64201778173\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.66431236267\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=6.66431236267\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.699544906616211, 6.691314339637756, 6.692827999591827, 6.686425507068634, 6.682028293609619, 6.678909122943878, 6.67538321018219, 6.670973539352417, 6.669675827026367, 6.668187081813812] min patience loss:6.66818708181 current loss:6.66431236267 absolute loss difference:0.00387471914291\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 232, \"sum\": 232.0, \"min\": 232}, \"Total Records Seen\": {\"count\": 1, \"max\": 58986, \"sum\": 58986.0, \"min\": 58986}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1610407243.51623, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 28}, \"StartTime\": 1610407243.482855}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60703.5366411 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.544] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 30 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.66393762827\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0247120463755\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.63922566175\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.66393762827\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=6.66393762827\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.691314339637756, 6.692827999591827, 6.686425507068634, 6.682028293609619, 6.678909122943878, 6.67538321018219, 6.670973539352417, 6.669675827026367, 6.668187081813812, 6.664312362670898] min patience loss:6.66431236267 current loss:6.66393762827 absolute loss difference:0.000374734401703\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 240, \"sum\": 240.0, \"min\": 240}, \"Total Records Seen\": {\"count\": 1, \"max\": 61020, \"sum\": 61020.0, \"min\": 61020}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1610407243.550146, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 29}, \"StartTime\": 1610407243.516482}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60229.1228556 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.576] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 31 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.65722155571\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0246702926233\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.63255131245\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.65722155571\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=6.65722155571\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.692827999591827, 6.686425507068634, 6.682028293609619, 6.678909122943878, 6.67538321018219, 6.670973539352417, 6.669675827026367, 6.668187081813812, 6.664312362670898, 6.663937628269196] min patience loss:6.66393762827 current loss:6.65722155571 absolute loss difference:0.00671607255936\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 248, \"sum\": 248.0, \"min\": 248}, \"Total Records Seen\": {\"count\": 1, \"max\": 63054, \"sum\": 63054.0, \"min\": 63054}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1610407243.581835, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 30}, \"StartTime\": 1610407243.550342}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=64305.958844 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.611] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 29, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 32 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.65684008598\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0251216725446\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.63171833754\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.65684008598\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=6.65684008598\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.686425507068634, 6.682028293609619, 6.678909122943878, 6.67538321018219, 6.670973539352417, 6.669675827026367, 6.668187081813812, 6.664312362670898, 6.663937628269196, 6.657221555709839] min patience loss:6.65722155571 current loss:6.65684008598 absolute loss difference:0.000381469726562\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 256, \"sum\": 256.0, \"min\": 256}, \"Total Records Seen\": {\"count\": 1, \"max\": 65088, \"sum\": 65088.0, \"min\": 65088}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1610407243.617412, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 31}, \"StartTime\": 1610407243.582101}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=57425.2792504 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.645] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 33 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.6520537734\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0266227459069\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.6254311204\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.6520537734\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=6.6520537734\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.682028293609619, 6.678909122943878, 6.67538321018219, 6.670973539352417, 6.669675827026367, 6.668187081813812, 6.664312362670898, 6.663937628269196, 6.657221555709839, 6.656840085983276] min patience loss:6.65684008598 current loss:6.6520537734 absolute loss difference:0.00478631258011\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 264, \"sum\": 264.0, \"min\": 264}, \"Total Records Seen\": {\"count\": 1, \"max\": 67122, \"sum\": 67122.0, \"min\": 67122}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1610407243.649922, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 32}, \"StartTime\": 1610407243.617615}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=62721.2158391 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.676] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 34 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.65290099382\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0266475214157\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.62625360489\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.65290099382\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=6.65290099382\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.678909122943878, 6.67538321018219, 6.670973539352417, 6.669675827026367, 6.668187081813812, 6.664312362670898, 6.663937628269196, 6.657221555709839, 6.656840085983276, 6.652053773403168] min patience loss:6.6520537734 current loss:6.65290099382 absolute loss difference:0.000847220420837\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 272, \"sum\": 272.0, \"min\": 272}, \"Total Records Seen\": {\"count\": 1, \"max\": 69156, \"sum\": 69156.0, \"min\": 69156}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1610407243.678077, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 33}, \"StartTime\": 1610407243.650189}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=72642.5552916 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.706] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 35 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.64803260565\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0278861222323\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.62014657259\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.64803260565\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=6.64803260565\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.67538321018219, 6.670973539352417, 6.669675827026367, 6.668187081813812, 6.664312362670898, 6.663937628269196, 6.657221555709839, 6.656840085983276, 6.652053773403168, 6.652900993824005] min patience loss:6.6520537734 current loss:6.64803260565 absolute loss difference:0.00402116775513\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 280, \"sum\": 280.0, \"min\": 280}, \"Total Records Seen\": {\"count\": 1, \"max\": 71190, \"sum\": 71190.0, \"min\": 71190}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1610407243.710438, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 34}, \"StartTime\": 1610407243.678274}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=63028.9044727 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.739] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 36 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.64539122581\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0259182893205\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.6194729805\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.64539122581\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=6.64539122581\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.670973539352417, 6.669675827026367, 6.668187081813812, 6.664312362670898, 6.663937628269196, 6.657221555709839, 6.656840085983276, 6.652053773403168, 6.652900993824005, 6.648032605648041] min patience loss:6.64803260565 current loss:6.64539122581 absolute loss difference:0.00264137983322\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 288, \"sum\": 288.0, \"min\": 288}, \"Total Records Seen\": {\"count\": 1, \"max\": 73224, \"sum\": 73224.0, \"min\": 73224}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1610407243.744045, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 35}, \"StartTime\": 1610407243.710672}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60734.2194379 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.772] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 37 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.64173418283\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0285844376776\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.61314964294\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.64173418283\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=6.64173418283\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.669675827026367, 6.668187081813812, 6.664312362670898, 6.663937628269196, 6.657221555709839, 6.656840085983276, 6.652053773403168, 6.652900993824005, 6.648032605648041, 6.645391225814819] min patience loss:6.64539122581 current loss:6.64173418283 absolute loss difference:0.00365704298019\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 296, \"sum\": 296.0, \"min\": 296}, \"Total Records Seen\": {\"count\": 1, \"max\": 75258, \"sum\": 75258.0, \"min\": 75258}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1610407243.777585, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 36}, \"StartTime\": 1610407243.744298}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60860.3004487 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.804] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 38 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.63727033138\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0303772136103\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.60689288378\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.63727033138\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=6.63727033138\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.668187081813812, 6.664312362670898, 6.663937628269196, 6.657221555709839, 6.656840085983276, 6.652053773403168, 6.652900993824005, 6.648032605648041, 6.645391225814819, 6.641734182834625] min patience loss:6.64173418283 current loss:6.63727033138 absolute loss difference:0.00446385145187\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 304, \"sum\": 304.0, \"min\": 304}, \"Total Records Seen\": {\"count\": 1, \"max\": 77292, \"sum\": 77292.0, \"min\": 77292}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1610407243.808742, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 37}, \"StartTime\": 1610407243.777841}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=65515.8685261 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.837] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 39 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.63662046194\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0296632661484\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.60695701838\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.63662046194\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=39, train total_loss <loss>=6.63662046194\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.664312362670898, 6.663937628269196, 6.657221555709839, 6.656840085983276, 6.652053773403168, 6.652900993824005, 6.648032605648041, 6.645391225814819, 6.641734182834625, 6.6372703313827515] min patience loss:6.63727033138 current loss:6.63662046194 absolute loss difference:0.000649869441986\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 312, \"sum\": 312.0, \"min\": 312}, \"Total Records Seen\": {\"count\": 1, \"max\": 79326, \"sum\": 79326.0, \"min\": 79326}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}}, \"EndTime\": 1610407243.842397, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 38}, \"StartTime\": 1610407243.80902}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60709.1522992 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.870] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 40 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.62706285715\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0330760860816\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.59398674965\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.62706285715\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=40, train total_loss <loss>=6.62706285715\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.663937628269196, 6.657221555709839, 6.656840085983276, 6.652053773403168, 6.652900993824005, 6.648032605648041, 6.645391225814819, 6.641734182834625, 6.6372703313827515, 6.636620461940765] min patience loss:6.63662046194 current loss:6.62706285715 absolute loss difference:0.00955760478973\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 320, \"sum\": 320.0, \"min\": 320}, \"Total Records Seen\": {\"count\": 1, \"max\": 81360, \"sum\": 81360.0, \"min\": 81360}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}}, \"EndTime\": 1610407243.875469, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 39}, \"StartTime\": 1610407243.84264}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=61707.7100946 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.904] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 41 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.6306527257\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0323524938431\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.59830015898\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.6306527257\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=41, train total_loss <loss>=6.6306527257\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.657221555709839, 6.656840085983276, 6.652053773403168, 6.652900993824005, 6.648032605648041, 6.645391225814819, 6.641734182834625, 6.6372703313827515, 6.636620461940765, 6.6270628571510315] min patience loss:6.62706285715 current loss:6.6306527257 absolute loss difference:0.00358986854553\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 328, \"sum\": 328.0, \"min\": 328}, \"Total Records Seen\": {\"count\": 1, \"max\": 83394, \"sum\": 83394.0, \"min\": 83394}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}}, \"EndTime\": 1610407243.90538, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 40}, \"StartTime\": 1610407243.875724}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=68287.409338 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.934] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 42 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.62138491869\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0351055113133\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.58627927303\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.62138491869\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=42, train total_loss <loss>=6.62138491869\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.656840085983276, 6.652053773403168, 6.652900993824005, 6.648032605648041, 6.645391225814819, 6.641734182834625, 6.6372703313827515, 6.636620461940765, 6.6270628571510315, 6.630652725696564] min patience loss:6.62706285715 current loss:6.62138491869 absolute loss difference:0.0056779384613\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 336, \"sum\": 336.0, \"min\": 336}, \"Total Records Seen\": {\"count\": 1, \"max\": 85428, \"sum\": 85428.0, \"min\": 85428}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}}, \"EndTime\": 1610407243.938363, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 41}, \"StartTime\": 1610407243.905658}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=61915.9596769 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.965] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 43 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.6200671792\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0354064549319\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.5846607089\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.6200671792\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=43, train total_loss <loss>=6.6200671792\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] patience losses:[6.652053773403168, 6.652900993824005, 6.648032605648041, 6.645391225814819, 6.641734182834625, 6.6372703313827515, 6.636620461940765, 6.6270628571510315, 6.630652725696564, 6.621384918689728] min patience loss:6.62138491869 current loss:6.6200671792 absolute loss difference:0.00131773948669\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 344, \"sum\": 344.0, \"min\": 344}, \"Total Records Seen\": {\"count\": 1, \"max\": 87462, \"sum\": 87462.0, \"min\": 87462}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}}, \"EndTime\": 1610407243.969979, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 42}, \"StartTime\": 1610407243.938642}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=64661.7627941 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:43.998] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] # Finished training epoch 44 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) total: 6.61625123024\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) kld: 0.0386067330837\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) recons: 6.57764458656\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] Loss (name: value) logppx: 6.61625123024\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:43 INFO 140226321442624] #quality_metric: host=algo-1, epoch=44, train total_loss <loss>=6.61625123024\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.652900993824005, 6.648032605648041, 6.645391225814819, 6.641734182834625, 6.6372703313827515, 6.636620461940765, 6.6270628571510315, 6.630652725696564, 6.621384918689728, 6.6200671792030334] min patience loss:6.6200671792 current loss:6.61625123024 absolute loss difference:0.00381594896317\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 352, \"sum\": 352.0, \"min\": 352}, \"Total Records Seen\": {\"count\": 1, \"max\": 89496, \"sum\": 89496.0, \"min\": 89496}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 88, \"sum\": 88.0, \"min\": 88}}, \"EndTime\": 1610407244.002963, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 43}, \"StartTime\": 1610407243.970236}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=61891.7037456 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.029] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 45 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.615698874\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0397803774104\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.57591855526\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.615698874\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=45, train total_loss <loss>=6.615698874\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.648032605648041, 6.645391225814819, 6.641734182834625, 6.6372703313827515, 6.636620461940765, 6.6270628571510315, 6.630652725696564, 6.621384918689728, 6.6200671792030334, 6.616251230239868] min patience loss:6.61625123024 current loss:6.615698874 absolute loss difference:0.000552356243134\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 360, \"sum\": 360.0, \"min\": 360}, \"Total Records Seen\": {\"count\": 1, \"max\": 91530, \"sum\": 91530.0, \"min\": 91530}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}}, \"EndTime\": 1610407244.034947, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 44}, \"StartTime\": 1610407244.003228}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=63857.8286638 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.065] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 29, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 46 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.60757136345\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.041425076779\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.56614631414\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.60757136345\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=46, train total_loss <loss>=6.60757136345\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.645391225814819, 6.641734182834625, 6.6372703313827515, 6.636620461940765, 6.6270628571510315, 6.630652725696564, 6.621384918689728, 6.6200671792030334, 6.616251230239868, 6.615698873996735] min patience loss:6.615698874 current loss:6.60757136345 absolute loss difference:0.00812751054764\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 368, \"sum\": 368.0, \"min\": 368}, \"Total Records Seen\": {\"count\": 1, \"max\": 93564, \"sum\": 93564.0, \"min\": 93564}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}}, \"EndTime\": 1610407244.06944, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 45}, \"StartTime\": 1610407244.035191}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=59203.4305066 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.096] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 47 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.60597020388\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0417259936221\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.56424427032\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.60597020388\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=47, train total_loss <loss>=6.60597020388\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.641734182834625, 6.6372703313827515, 6.636620461940765, 6.6270628571510315, 6.630652725696564, 6.621384918689728, 6.6200671792030334, 6.616251230239868, 6.615698873996735, 6.607571363449097] min patience loss:6.60757136345 current loss:6.60597020388 absolute loss difference:0.0016011595726\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 376, \"sum\": 376.0, \"min\": 376}, \"Total Records Seen\": {\"count\": 1, \"max\": 95598, \"sum\": 95598.0, \"min\": 95598}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 94, \"sum\": 94.0, \"min\": 94}}, \"EndTime\": 1610407244.100792, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 46}, \"StartTime\": 1610407244.069668}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=65104.8881698 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.129] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 48 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.601339221\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0461568390019\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.55518239737\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.601339221\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=48, train total_loss <loss>=6.601339221\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.6372703313827515, 6.636620461940765, 6.6270628571510315, 6.630652725696564, 6.621384918689728, 6.6200671792030334, 6.616251230239868, 6.615698873996735, 6.607571363449097, 6.605970203876495] min patience loss:6.60597020388 current loss:6.601339221 absolute loss difference:0.00463098287582\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.01s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 384, \"sum\": 384.0, \"min\": 384}, \"Total Records Seen\": {\"count\": 1, \"max\": 97632, \"sum\": 97632.0, \"min\": 97632}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}}, \"EndTime\": 1610407244.136433, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 47}, \"StartTime\": 1610407244.101022}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=57186.2366087 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.168] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 31, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 49 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.59799772501\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0436785584316\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.5543191433\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.59799772501\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=49, train total_loss <loss>=6.59799772501\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.636620461940765, 6.6270628571510315, 6.630652725696564, 6.621384918689728, 6.6200671792030334, 6.616251230239868, 6.615698873996735, 6.607571363449097, 6.605970203876495, 6.601339221000671] min patience loss:6.601339221 current loss:6.59799772501 absolute loss difference:0.00334149599075\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 392, \"sum\": 392.0, \"min\": 392}, \"Total Records Seen\": {\"count\": 1, \"max\": 99666, \"sum\": 99666.0, \"min\": 99666}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 98, \"sum\": 98.0, \"min\": 98}}, \"EndTime\": 1610407244.173027, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 48}, \"StartTime\": 1610407244.136769}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=55873.1430293 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.200] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 50 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.59710639715\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0462265331298\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.55087971687\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.59710639715\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=50, train total_loss <loss>=6.59710639715\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.6270628571510315, 6.630652725696564, 6.621384918689728, 6.6200671792030334, 6.616251230239868, 6.615698873996735, 6.607571363449097, 6.605970203876495, 6.601339221000671, 6.597997725009918] min patience loss:6.59799772501 current loss:6.59710639715 absolute loss difference:0.000891327857971\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"Total Records Seen\": {\"count\": 1, \"max\": 101700, \"sum\": 101700.0, \"min\": 101700}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}}, \"EndTime\": 1610407244.205144, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 49}, \"StartTime\": 1610407244.173298}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=63602.1764504 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 51\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.235] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 152, \"duration\": 29, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 51 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.5924346447\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0496735177003\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.54276108742\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.5924346447\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=51, train total_loss <loss>=6.5924346447\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.630652725696564, 6.621384918689728, 6.6200671792030334, 6.616251230239868, 6.615698873996735, 6.607571363449097, 6.605970203876495, 6.601339221000671, 6.597997725009918, 6.597106397151947] min patience loss:6.59710639715 current loss:6.5924346447 absolute loss difference:0.00467175245285\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 408, \"sum\": 408.0, \"min\": 408}, \"Total Records Seen\": {\"count\": 1, \"max\": 103734, \"sum\": 103734.0, \"min\": 103734}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 102, \"sum\": 102.0, \"min\": 102}}, \"EndTime\": 1610407244.240644, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 50}, \"StartTime\": 1610407244.205428}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=57548.4629125 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 52\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.269] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 155, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 52 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.59216588736\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0499865827151\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.54217922688\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.59216588736\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=52, train total_loss <loss>=6.59216588736\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.621384918689728, 6.6200671792030334, 6.616251230239868, 6.615698873996735, 6.607571363449097, 6.605970203876495, 6.601339221000671, 6.597997725009918, 6.597106397151947, 6.592434644699097] min patience loss:6.5924346447 current loss:6.59216588736 absolute loss difference:0.000268757343292\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.01s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 416, \"sum\": 416.0, \"min\": 416}, \"Total Records Seen\": {\"count\": 1, \"max\": 105768, \"sum\": 105768.0, \"min\": 105768}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 104, \"sum\": 104.0, \"min\": 104}}, \"EndTime\": 1610407244.276636, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 51}, \"StartTime\": 1610407244.240986}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=56802.8120115 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 53\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.307] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 158, \"duration\": 30, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 53 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.59161686897\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0526128779165\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.53900402784\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.59161686897\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=53, train total_loss <loss>=6.59161686897\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.6200671792030334, 6.616251230239868, 6.615698873996735, 6.607571363449097, 6.605970203876495, 6.601339221000671, 6.597997725009918, 6.597106397151947, 6.592434644699097, 6.592165887355804] min patience loss:6.59216588736 current loss:6.59161686897 absolute loss difference:0.000549018383026\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.01s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 424, \"sum\": 424.0, \"min\": 424}, \"Total Records Seen\": {\"count\": 1, \"max\": 107802, \"sum\": 107802.0, \"min\": 107802}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 106, \"sum\": 106.0, \"min\": 106}}, \"EndTime\": 1610407244.314813, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 52}, \"StartTime\": 1610407244.276918}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=53458.7482282 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 54\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.347] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 161, \"duration\": 32, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 54 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.58216792345\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0521744927391\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.52999347448\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.58216792345\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=54, train total_loss <loss>=6.58216792345\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.616251230239868, 6.615698873996735, 6.607571363449097, 6.605970203876495, 6.601339221000671, 6.597997725009918, 6.597106397151947, 6.592434644699097, 6.592165887355804, 6.591616868972778] min patience loss:6.59161686897 current loss:6.58216792345 absolute loss difference:0.00944894552231\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.01s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 432, \"sum\": 432.0, \"min\": 432}, \"Total Records Seen\": {\"count\": 1, \"max\": 109836, \"sum\": 109836.0, \"min\": 109836}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 108, \"sum\": 108.0, \"min\": 108}}, \"EndTime\": 1610407244.354298, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 53}, \"StartTime\": 1610407244.315134}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=51745.0981743 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 55\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.385] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 164, \"duration\": 30, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 55 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.58097136021\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0530135333538\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.52795779705\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.58097136021\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=55, train total_loss <loss>=6.58097136021\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.615698873996735, 6.607571363449097, 6.605970203876495, 6.601339221000671, 6.597997725009918, 6.597106397151947, 6.592434644699097, 6.592165887355804, 6.591616868972778, 6.58216792345047] min patience loss:6.58216792345 current loss:6.58097136021 absolute loss difference:0.00119656324387\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 440, \"sum\": 440.0, \"min\": 440}, \"Total Records Seen\": {\"count\": 1, \"max\": 111870, \"sum\": 111870.0, \"min\": 111870}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 110, \"sum\": 110.0, \"min\": 110}}, \"EndTime\": 1610407244.391292, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 54}, \"StartTime\": 1610407244.354558}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=55126.1604311 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 56\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.423] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 167, \"duration\": 31, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 56 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.57531970739\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0551821640693\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.52013754845\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.57531970739\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=56, train total_loss <loss>=6.57531970739\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.607571363449097, 6.605970203876495, 6.601339221000671, 6.597997725009918, 6.597106397151947, 6.592434644699097, 6.592165887355804, 6.591616868972778, 6.58216792345047, 6.580971360206604] min patience loss:6.58097136021 current loss:6.57531970739 absolute loss difference:0.00565165281296\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.01s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 448, \"sum\": 448.0, \"min\": 448}, \"Total Records Seen\": {\"count\": 1, \"max\": 113904, \"sum\": 113904.0, \"min\": 113904}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 112, \"sum\": 112.0, \"min\": 112}}, \"EndTime\": 1610407244.430163, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 55}, \"StartTime\": 1610407244.391589}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=52517.2324095 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 57\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.461] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 170, \"duration\": 30, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 57 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.57217240334\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0550938253291\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.51707869768\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.57217240334\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=57, train total_loss <loss>=6.57217240334\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.605970203876495, 6.601339221000671, 6.597997725009918, 6.597106397151947, 6.592434644699097, 6.592165887355804, 6.591616868972778, 6.58216792345047, 6.580971360206604, 6.575319707393646] min patience loss:6.57531970739 current loss:6.57217240334 absolute loss difference:0.00314730405807\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 456, \"sum\": 456.0, \"min\": 456}, \"Total Records Seen\": {\"count\": 1, \"max\": 115938, \"sum\": 115938.0, \"min\": 115938}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 114, \"sum\": 114.0, \"min\": 114}}, \"EndTime\": 1610407244.466697, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 56}, \"StartTime\": 1610407244.430457}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=55932.8529956 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 58\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.501] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 173, \"duration\": 34, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 58 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.56810575724\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0553060756065\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.51279956102\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.56810575724\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=58, train total_loss <loss>=6.56810575724\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.601339221000671, 6.597997725009918, 6.597106397151947, 6.592434644699097, 6.592165887355804, 6.591616868972778, 6.58216792345047, 6.580971360206604, 6.575319707393646, 6.572172403335571] min patience loss:6.57217240334 current loss:6.56810575724 absolute loss difference:0.00406664609909\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.04s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 464, \"sum\": 464.0, \"min\": 464}, \"Total Records Seen\": {\"count\": 1, \"max\": 117972, \"sum\": 117972.0, \"min\": 117972}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 116, \"sum\": 116.0, \"min\": 116}}, \"EndTime\": 1610407244.506706, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 57}, \"StartTime\": 1610407244.466939}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=50936.576188 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 59\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.542] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 176, \"duration\": 35, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 59 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.5638808012\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.058237854857\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.50564301014\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.5638808012\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=59, train total_loss <loss>=6.5638808012\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.597997725009918, 6.597106397151947, 6.592434644699097, 6.592165887355804, 6.591616868972778, 6.58216792345047, 6.580971360206604, 6.575319707393646, 6.572172403335571, 6.568105757236481] min patience loss:6.56810575724 current loss:6.5638808012 absolute loss difference:0.00422495603561\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.04s, val: 0.01s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 472, \"sum\": 472.0, \"min\": 472}, \"Total Records Seen\": {\"count\": 1, \"max\": 120006, \"sum\": 120006.0, \"min\": 120006}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 118, \"sum\": 118.0, \"min\": 118}}, \"EndTime\": 1610407244.549615, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 58}, \"StartTime\": 1610407244.507028}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=47563.1632305 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 60\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.581] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 179, \"duration\": 31, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 60 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.56446594\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0570007404312\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.50746524334\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.56446594\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=60, train total_loss <loss>=6.56446594\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.597106397151947, 6.592434644699097, 6.592165887355804, 6.591616868972778, 6.58216792345047, 6.580971360206604, 6.575319707393646, 6.572172403335571, 6.568105757236481, 6.563880801200867] min patience loss:6.5638808012 current loss:6.56446594 absolute loss difference:0.00058513879776\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 480, \"sum\": 480.0, \"min\": 480}, \"Total Records Seen\": {\"count\": 1, \"max\": 122040, \"sum\": 122040.0, \"min\": 122040}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 120, \"sum\": 120.0, \"min\": 120}}, \"EndTime\": 1610407244.582647, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 59}, \"StartTime\": 1610407244.549958}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=61972.6308541 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 61\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.615] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 182, \"duration\": 32, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 61 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.56272363663\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0593087961897\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.50341486931\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.56272363663\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=61, train total_loss <loss>=6.56272363663\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.592434644699097, 6.592165887355804, 6.591616868972778, 6.58216792345047, 6.580971360206604, 6.575319707393646, 6.572172403335571, 6.568105757236481, 6.563880801200867, 6.564465939998627] min patience loss:6.5638808012 current loss:6.56272363663 absolute loss difference:0.00115716457367\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 488, \"sum\": 488.0, \"min\": 488}, \"Total Records Seen\": {\"count\": 1, \"max\": 124074, \"sum\": 124074.0, \"min\": 124074}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 122, \"sum\": 122.0, \"min\": 122}}, \"EndTime\": 1610407244.62048, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 60}, \"StartTime\": 1610407244.582901}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=53896.4447056 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 62\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.654] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 185, \"duration\": 33, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 62 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.55624252558\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0599707500078\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.49627190828\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.55624252558\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=62, train total_loss <loss>=6.55624252558\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.592165887355804, 6.591616868972778, 6.58216792345047, 6.580971360206604, 6.575319707393646, 6.572172403335571, 6.568105757236481, 6.563880801200867, 6.564465939998627, 6.562723636627197] min patience loss:6.56272363663 current loss:6.55624252558 absolute loss difference:0.00648111104965\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 496, \"sum\": 496.0, \"min\": 496}, \"Total Records Seen\": {\"count\": 1, \"max\": 126108, \"sum\": 126108.0, \"min\": 126108}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}}, \"EndTime\": 1610407244.659742, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 61}, \"StartTime\": 1610407244.620795}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=51985.9989031 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 63\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.704] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 188, \"duration\": 37, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 63 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.55808597803\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0614260463044\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.49666005373\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.55808597803\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=63, train total_loss <loss>=6.55808597803\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.591616868972778, 6.58216792345047, 6.580971360206604, 6.575319707393646, 6.572172403335571, 6.568105757236481, 6.563880801200867, 6.564465939998627, 6.562723636627197, 6.556242525577545] min patience loss:6.55624252558 current loss:6.55808597803 absolute loss difference:0.00184345245361\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.05s, val: 0.00s, epoch: 0.05s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 504, \"sum\": 504.0, \"min\": 504}, \"Total Records Seen\": {\"count\": 1, \"max\": 128142, \"sum\": 128142.0, \"min\": 128142}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 126, \"sum\": 126.0, \"min\": 126}}, \"EndTime\": 1610407244.706349, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 62}, \"StartTime\": 1610407244.660046}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=43772.0398356 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 64\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.743] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 191, \"duration\": 36, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 64 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.55224186182\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0635442868806\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.48869770765\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.55224186182\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=64, train total_loss <loss>=6.55224186182\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.58216792345047, 6.580971360206604, 6.575319707393646, 6.572172403335571, 6.568105757236481, 6.563880801200867, 6.564465939998627, 6.562723636627197, 6.556242525577545, 6.5580859780311584] min patience loss:6.55624252558 current loss:6.55224186182 absolute loss difference:0.00400066375732\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.04s, val: 0.01s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 512, \"sum\": 512.0, \"min\": 512}, \"Total Records Seen\": {\"count\": 1, \"max\": 130176, \"sum\": 130176.0, \"min\": 130176}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 128, \"sum\": 128.0, \"min\": 128}}, \"EndTime\": 1610407244.750437, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 63}, \"StartTime\": 1610407244.706656}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=46264.2180454 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 65\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.783] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 194, \"duration\": 32, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 65 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.55152881145\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0642293114215\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.48729938269\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.55152881145\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=65, train total_loss <loss>=6.55152881145\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.580971360206604, 6.575319707393646, 6.572172403335571, 6.568105757236481, 6.563880801200867, 6.564465939998627, 6.562723636627197, 6.556242525577545, 6.5580859780311584, 6.552241861820221] min patience loss:6.55224186182 current loss:6.55152881145 absolute loss difference:0.000713050365448\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 520, \"sum\": 520.0, \"min\": 520}, \"Total Records Seen\": {\"count\": 1, \"max\": 132210, \"sum\": 132210.0, \"min\": 132210}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 130, \"sum\": 130.0, \"min\": 130}}, \"EndTime\": 1610407244.789435, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 64}, \"StartTime\": 1610407244.750801}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=52429.444413 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 66\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.821] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 197, \"duration\": 31, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 66 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.54996734858\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0647115907632\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.48525571823\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.54996734858\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=66, train total_loss <loss>=6.54996734858\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.575319707393646, 6.572172403335571, 6.568105757236481, 6.563880801200867, 6.564465939998627, 6.562723636627197, 6.556242525577545, 6.5580859780311584, 6.552241861820221, 6.551528811454773] min patience loss:6.55152881145 current loss:6.54996734858 absolute loss difference:0.00156146287918\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 528, \"sum\": 528.0, \"min\": 528}, \"Total Records Seen\": {\"count\": 1, \"max\": 134244, \"sum\": 134244.0, \"min\": 134244}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 132, \"sum\": 132.0, \"min\": 132}}, \"EndTime\": 1610407244.826373, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 65}, \"StartTime\": 1610407244.789744}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=55276.5334042 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 67\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.857] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 200, \"duration\": 30, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 67 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.54206967354\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0665363497101\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.47553318739\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.54206967354\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=67, train total_loss <loss>=6.54206967354\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.572172403335571, 6.568105757236481, 6.563880801200867, 6.564465939998627, 6.562723636627197, 6.556242525577545, 6.5580859780311584, 6.552241861820221, 6.551528811454773, 6.549967348575592] min patience loss:6.54996734858 current loss:6.54206967354 absolute loss difference:0.00789767503738\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 536, \"sum\": 536.0, \"min\": 536}, \"Total Records Seen\": {\"count\": 1, \"max\": 136278, \"sum\": 136278.0, \"min\": 136278}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 134, \"sum\": 134.0, \"min\": 134}}, \"EndTime\": 1610407244.862329, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 66}, \"StartTime\": 1610407244.826699}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=56809.2422473 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 68\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.893] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 203, \"duration\": 30, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 68 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.54168134928\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0646904832684\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.47699081898\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.54168134928\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=68, train total_loss <loss>=6.54168134928\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.568105757236481, 6.563880801200867, 6.564465939998627, 6.562723636627197, 6.556242525577545, 6.5580859780311584, 6.552241861820221, 6.551528811454773, 6.549967348575592, 6.542069673538208] min patience loss:6.54206967354 current loss:6.54168134928 absolute loss difference:0.000388324260712\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 544, \"sum\": 544.0, \"min\": 544}, \"Total Records Seen\": {\"count\": 1, \"max\": 138312, \"sum\": 138312.0, \"min\": 138312}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 136, \"sum\": 136.0, \"min\": 136}}, \"EndTime\": 1610407244.898436, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 67}, \"StartTime\": 1610407244.862639}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=56564.4121653 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 69\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.928] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 206, \"duration\": 29, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 69 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.5473959446\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0692801158875\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.47811597586\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.5473959446\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=69, train total_loss <loss>=6.5473959446\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.563880801200867, 6.564465939998627, 6.562723636627197, 6.556242525577545, 6.5580859780311584, 6.552241861820221, 6.551528811454773, 6.549967348575592, 6.542069673538208, 6.541681349277496] min patience loss:6.54168134928 current loss:6.5473959446 absolute loss difference:0.00571459531784\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 552, \"sum\": 552.0, \"min\": 552}, \"Total Records Seen\": {\"count\": 1, \"max\": 140346, \"sum\": 140346.0, \"min\": 140346}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 138, \"sum\": 138.0, \"min\": 138}}, \"EndTime\": 1610407244.930147, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 68}, \"StartTime\": 1610407244.898731}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=64440.5074138 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 70\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:44.963] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 209, \"duration\": 33, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Finished training epoch 70 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) total: 6.53828620911\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) kld: 0.0676555531099\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) recons: 6.47063064575\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Loss (name: value) logppx: 6.53828620911\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #quality_metric: host=algo-1, epoch=70, train total_loss <loss>=6.53828620911\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] patience losses:[6.564465939998627, 6.562723636627197, 6.556242525577545, 6.5580859780311584, 6.552241861820221, 6.551528811454773, 6.549967348575592, 6.542069673538208, 6.541681349277496, 6.547395944595337] min patience loss:6.54168134928 current loss:6.53828620911 absolute loss difference:0.00339514017105\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] Timing: train: 0.03s, val: 0.01s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 560, \"sum\": 560.0, \"min\": 560}, \"Total Records Seen\": {\"count\": 1, \"max\": 142380, \"sum\": 142380.0, \"min\": 142380}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 140, \"sum\": 140.0, \"min\": 140}}, \"EndTime\": 1610407244.970506, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 69}, \"StartTime\": 1610407244.930419}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=50540.3692891 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:44 INFO 140226321442624] # Starting training for epoch 71\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.004] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 212, \"duration\": 33, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 71 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.53476113081\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0714279916137\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.46333318949\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.53476113081\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=71, train total_loss <loss>=6.53476113081\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.562723636627197, 6.556242525577545, 6.5580859780311584, 6.552241861820221, 6.551528811454773, 6.549967348575592, 6.542069673538208, 6.541681349277496, 6.547395944595337, 6.538286209106445] min patience loss:6.53828620911 current loss:6.53476113081 absolute loss difference:0.00352507829666\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 568, \"sum\": 568.0, \"min\": 568}, \"Total Records Seen\": {\"count\": 1, \"max\": 144414, \"sum\": 144414.0, \"min\": 144414}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 142, \"sum\": 142.0, \"min\": 142}}, \"EndTime\": 1610407245.010768, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 70}, \"StartTime\": 1610407244.970797}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=50686.3026011 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 72\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.039] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 215, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 72 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.53218227625\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0698788464069\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.46230334044\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.53218227625\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=72, train total_loss <loss>=6.53218227625\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.556242525577545, 6.5580859780311584, 6.552241861820221, 6.551528811454773, 6.549967348575592, 6.542069673538208, 6.541681349277496, 6.547395944595337, 6.538286209106445, 6.534761130809784] min patience loss:6.53476113081 current loss:6.53218227625 absolute loss difference:0.00257885456085\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 576, \"sum\": 576.0, \"min\": 576}, \"Total Records Seen\": {\"count\": 1, \"max\": 146448, \"sum\": 146448.0, \"min\": 146448}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 144, \"sum\": 144.0, \"min\": 144}}, \"EndTime\": 1610407245.044837, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 71}, \"StartTime\": 1610407245.011089}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60012.6222135 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 73\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.076] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 218, \"duration\": 30, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 73 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.52695804834\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0728715006262\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.45408666134\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.52695804834\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=73, train total_loss <loss>=6.52695804834\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.5580859780311584, 6.552241861820221, 6.551528811454773, 6.549967348575592, 6.542069673538208, 6.541681349277496, 6.547395944595337, 6.538286209106445, 6.534761130809784, 6.532182276248932] min patience loss:6.53218227625 current loss:6.52695804834 absolute loss difference:0.00522422790527\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.04s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 584, \"sum\": 584.0, \"min\": 584}, \"Total Records Seen\": {\"count\": 1, \"max\": 148482, \"sum\": 148482.0, \"min\": 148482}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 146, \"sum\": 146.0, \"min\": 146}}, \"EndTime\": 1610407245.081414, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 72}, \"StartTime\": 1610407245.045101}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=55805.5283174 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 74\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.110] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 221, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 74 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.52239972353\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0719436416402\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.45045596361\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.52239972353\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=74, train total_loss <loss>=6.52239972353\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.552241861820221, 6.551528811454773, 6.549967348575592, 6.542069673538208, 6.541681349277496, 6.547395944595337, 6.538286209106445, 6.534761130809784, 6.532182276248932, 6.5269580483436584] min patience loss:6.52695804834 current loss:6.52239972353 absolute loss difference:0.00455832481384\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 592, \"sum\": 592.0, \"min\": 592}, \"Total Records Seen\": {\"count\": 1, \"max\": 150516, \"sum\": 150516.0, \"min\": 150516}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 148, \"sum\": 148.0, \"min\": 148}}, \"EndTime\": 1610407245.115948, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 73}, \"StartTime\": 1610407245.081671}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=59112.7717796 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 75\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.142] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 224, \"duration\": 25, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 75 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.51722443104\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0716056860983\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.44561874866\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.51722443104\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=75, train total_loss <loss>=6.51722443104\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.551528811454773, 6.549967348575592, 6.542069673538208, 6.541681349277496, 6.547395944595337, 6.538286209106445, 6.534761130809784, 6.532182276248932, 6.5269580483436584, 6.522399723529816] min patience loss:6.52239972353 current loss:6.51722443104 absolute loss difference:0.00517529249191\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Total Records Seen\": {\"count\": 1, \"max\": 152550, \"sum\": 152550.0, \"min\": 152550}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 150, \"sum\": 150.0, \"min\": 150}}, \"EndTime\": 1610407245.147237, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 74}, \"StartTime\": 1610407245.116199}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=65307.2320412 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 76\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.174] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 227, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 76 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.52211940289\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0731498692185\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.44896942377\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.52211940289\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=76, train total_loss <loss>=6.52211940289\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.549967348575592, 6.542069673538208, 6.541681349277496, 6.547395944595337, 6.538286209106445, 6.534761130809784, 6.532182276248932, 6.5269580483436584, 6.522399723529816, 6.517224431037903] min patience loss:6.51722443104 current loss:6.52211940289 absolute loss difference:0.00489497184753\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 608, \"sum\": 608.0, \"min\": 608}, \"Total Records Seen\": {\"count\": 1, \"max\": 154584, \"sum\": 154584.0, \"min\": 154584}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 152, \"sum\": 152.0, \"min\": 152}}, \"EndTime\": 1610407245.175799, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 75}, \"StartTime\": 1610407245.147487}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=71511.197378 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 77\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.204] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 230, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 77 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.51861220598\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0789403682575\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.43967187405\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.51861220598\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=77, train total_loss <loss>=6.51861220598\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.542069673538208, 6.541681349277496, 6.547395944595337, 6.538286209106445, 6.534761130809784, 6.532182276248932, 6.5269580483436584, 6.522399723529816, 6.517224431037903, 6.522119402885437] min patience loss:6.51722443104 current loss:6.51861220598 absolute loss difference:0.00138777494431\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 616, \"sum\": 616.0, \"min\": 616}, \"Total Records Seen\": {\"count\": 1, \"max\": 156618, \"sum\": 156618.0, \"min\": 156618}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 154, \"sum\": 154.0, \"min\": 154}}, \"EndTime\": 1610407245.205731, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 76}, \"StartTime\": 1610407245.176063}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=68271.0152448 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 78\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.233] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 233, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 78 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.51361614466\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0754855694249\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.43813049793\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.51361614466\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=78, train total_loss <loss>=6.51361614466\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.541681349277496, 6.547395944595337, 6.538286209106445, 6.534761130809784, 6.532182276248932, 6.5269580483436584, 6.522399723529816, 6.517224431037903, 6.522119402885437, 6.518612205982208] min patience loss:6.51722443104 current loss:6.51361614466 absolute loss difference:0.00360828638077\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 624, \"sum\": 624.0, \"min\": 624}, \"Total Records Seen\": {\"count\": 1, \"max\": 158652, \"sum\": 158652.0, \"min\": 158652}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 156, \"sum\": 156.0, \"min\": 156}}, \"EndTime\": 1610407245.237953, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 77}, \"StartTime\": 1610407245.205996}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=63399.8776475 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 79\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.264] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 236, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 79 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.50794887543\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0774953169748\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.4304536581\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.50794887543\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=79, train total_loss <loss>=6.50794887543\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.547395944595337, 6.538286209106445, 6.534761130809784, 6.532182276248932, 6.5269580483436584, 6.522399723529816, 6.517224431037903, 6.522119402885437, 6.518612205982208, 6.513616144657135] min patience loss:6.51361614466 current loss:6.50794887543 absolute loss difference:0.00566726922989\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 632, \"sum\": 632.0, \"min\": 632}, \"Total Records Seen\": {\"count\": 1, \"max\": 160686, \"sum\": 160686.0, \"min\": 160686}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 158, \"sum\": 158.0, \"min\": 158}}, \"EndTime\": 1610407245.269932, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 78}, \"StartTime\": 1610407245.238199}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=63823.9095071 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 80\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.298] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 239, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 80 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.51124328375\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.079643920064\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.43159931898\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.51124328375\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=80, train total_loss <loss>=6.51124328375\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.538286209106445, 6.534761130809784, 6.532182276248932, 6.5269580483436584, 6.522399723529816, 6.517224431037903, 6.522119402885437, 6.518612205982208, 6.513616144657135, 6.507948875427246] min patience loss:6.50794887543 current loss:6.51124328375 absolute loss difference:0.00329440832138\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 640, \"sum\": 640.0, \"min\": 640}, \"Total Records Seen\": {\"count\": 1, \"max\": 162720, \"sum\": 162720.0, \"min\": 162720}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 160, \"sum\": 160.0, \"min\": 160}}, \"EndTime\": 1610407245.299259, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 79}, \"StartTime\": 1610407245.270182}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=69676.1243048 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 81\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.328] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 242, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 81 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.50881433487\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0815184311941\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.42729592323\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.50881433487\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=81, train total_loss <loss>=6.50881433487\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.534761130809784, 6.532182276248932, 6.5269580483436584, 6.522399723529816, 6.517224431037903, 6.522119402885437, 6.518612205982208, 6.513616144657135, 6.507948875427246, 6.511243283748627] min patience loss:6.50794887543 current loss:6.50881433487 absolute loss difference:0.000865459442139\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 648, \"sum\": 648.0, \"min\": 648}, \"Total Records Seen\": {\"count\": 1, \"max\": 164754, \"sum\": 164754.0, \"min\": 164754}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 162, \"sum\": 162.0, \"min\": 162}}, \"EndTime\": 1610407245.329441, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 80}, \"StartTime\": 1610407245.299506}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=67682.8035257 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 82\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.358] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 245, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 82 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.50604510307\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0830592596903\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.42298591137\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.50604510307\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=82, train total_loss <loss>=6.50604510307\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.532182276248932, 6.5269580483436584, 6.522399723529816, 6.517224431037903, 6.522119402885437, 6.518612205982208, 6.513616144657135, 6.507948875427246, 6.511243283748627, 6.508814334869385] min patience loss:6.50794887543 current loss:6.50604510307 absolute loss difference:0.00190377235413\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 656, \"sum\": 656.0, \"min\": 656}, \"Total Records Seen\": {\"count\": 1, \"max\": 166788, \"sum\": 166788.0, \"min\": 166788}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 164, \"sum\": 164.0, \"min\": 164}}, \"EndTime\": 1610407245.362677, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 81}, \"StartTime\": 1610407245.329691}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=61410.9871581 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 83\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.391] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 248, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 83 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.49812269211\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0805782312527\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.41754460335\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.49812269211\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=83, train total_loss <loss>=6.49812269211\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.5269580483436584, 6.522399723529816, 6.517224431037903, 6.522119402885437, 6.518612205982208, 6.513616144657135, 6.507948875427246, 6.511243283748627, 6.508814334869385, 6.50604510307312] min patience loss:6.50604510307 current loss:6.49812269211 absolute loss difference:0.00792241096497\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 664, \"sum\": 664.0, \"min\": 664}, \"Total Records Seen\": {\"count\": 1, \"max\": 168822, \"sum\": 168822.0, \"min\": 168822}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 166, \"sum\": 166.0, \"min\": 166}}, \"EndTime\": 1610407245.395603, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 82}, \"StartTime\": 1610407245.362932}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=62027.1509088 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 84\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.423] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 251, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 84 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.50218468904\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0812732465565\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.42091143131\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.50218468904\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=84, train total_loss <loss>=6.50218468904\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.522399723529816, 6.517224431037903, 6.522119402885437, 6.518612205982208, 6.513616144657135, 6.507948875427246, 6.511243283748627, 6.508814334869385, 6.50604510307312, 6.498122692108154] min patience loss:6.49812269211 current loss:6.50218468904 absolute loss difference:0.0040619969368\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 672, \"sum\": 672.0, \"min\": 672}, \"Total Records Seen\": {\"count\": 1, \"max\": 170856, \"sum\": 170856.0, \"min\": 170856}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 168, \"sum\": 168.0, \"min\": 168}}, \"EndTime\": 1610407245.424806, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 83}, \"StartTime\": 1610407245.395858}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=69976.1666721 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 85\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.451] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 254, \"duration\": 25, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 85 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.49874591827\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0839116033167\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.41483443975\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.49874591827\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=85, train total_loss <loss>=6.49874591827\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.517224431037903, 6.522119402885437, 6.518612205982208, 6.513616144657135, 6.507948875427246, 6.511243283748627, 6.508814334869385, 6.50604510307312, 6.498122692108154, 6.502184689044952] min patience loss:6.49812269211 current loss:6.49874591827 absolute loss difference:0.000623226165771\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 680, \"sum\": 680.0, \"min\": 680}, \"Total Records Seen\": {\"count\": 1, \"max\": 172890, \"sum\": 172890.0, \"min\": 172890}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 170, \"sum\": 170.0, \"min\": 170}}, \"EndTime\": 1610407245.452528, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 84}, \"StartTime\": 1610407245.425052}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=73727.3628374 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 86\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.481] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 257, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 86 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.49071800709\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0826846910641\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.40803319216\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.49071800709\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=86, train total_loss <loss>=6.49071800709\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.522119402885437, 6.518612205982208, 6.513616144657135, 6.507948875427246, 6.511243283748627, 6.508814334869385, 6.50604510307312, 6.498122692108154, 6.502184689044952, 6.498745918273926] min patience loss:6.49812269211 current loss:6.49071800709 absolute loss difference:0.00740468502045\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 688, \"sum\": 688.0, \"min\": 688}, \"Total Records Seen\": {\"count\": 1, \"max\": 174924, \"sum\": 174924.0, \"min\": 174924}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 172, \"sum\": 172.0, \"min\": 172}}, \"EndTime\": 1610407245.486074, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 85}, \"StartTime\": 1610407245.452766}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=60827.3215974 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 87\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.514] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 260, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 87 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.4916369915\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0851339623332\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.4065027833\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.4916369915\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=87, train total_loss <loss>=6.4916369915\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.518612205982208, 6.513616144657135, 6.507948875427246, 6.511243283748627, 6.508814334869385, 6.50604510307312, 6.498122692108154, 6.502184689044952, 6.498745918273926, 6.4907180070877075] min patience loss:6.49071800709 current loss:6.4916369915 absolute loss difference:0.000918984413147\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 696, \"sum\": 696.0, \"min\": 696}, \"Total Records Seen\": {\"count\": 1, \"max\": 176958, \"sum\": 176958.0, \"min\": 176958}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 174, \"sum\": 174.0, \"min\": 174}}, \"EndTime\": 1610407245.515307, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 86}, \"StartTime\": 1610407245.48632}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=69858.1282324 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 88\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.542] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 263, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 88 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.49007558823\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0872616739944\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.40281385183\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.49007558823\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=88, train total_loss <loss>=6.49007558823\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.513616144657135, 6.507948875427246, 6.511243283748627, 6.508814334869385, 6.50604510307312, 6.498122692108154, 6.502184689044952, 6.498745918273926, 6.4907180070877075, 6.4916369915008545] min patience loss:6.49071800709 current loss:6.49007558823 absolute loss difference:0.000642418861389\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 704, \"sum\": 704.0, \"min\": 704}, \"Total Records Seen\": {\"count\": 1, \"max\": 178992, \"sum\": 178992.0, \"min\": 178992}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 176, \"sum\": 176.0, \"min\": 176}}, \"EndTime\": 1610407245.547076, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 87}, \"StartTime\": 1610407245.515546}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=64281.247587 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 89\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.575] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 266, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 89 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.48527050018\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0895720031112\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.39569848776\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.48527050018\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=89, train total_loss <loss>=6.48527050018\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.507948875427246, 6.511243283748627, 6.508814334869385, 6.50604510307312, 6.498122692108154, 6.502184689044952, 6.498745918273926, 6.4907180070877075, 6.4916369915008545, 6.490075588226318] min patience loss:6.49007558823 current loss:6.48527050018 absolute loss difference:0.00480508804321\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 712, \"sum\": 712.0, \"min\": 712}, \"Total Records Seen\": {\"count\": 1, \"max\": 181026, \"sum\": 181026.0, \"min\": 181026}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 178, \"sum\": 178.0, \"min\": 178}}, \"EndTime\": 1610407245.579996, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 88}, \"StartTime\": 1610407245.547339}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=62036.6228376 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 90\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.607] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 269, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 90 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.48203521967\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0865286840126\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.39550644159\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.48203521967\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=90, train total_loss <loss>=6.48203521967\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.511243283748627, 6.508814334869385, 6.50604510307312, 6.498122692108154, 6.502184689044952, 6.498745918273926, 6.4907180070877075, 6.4916369915008545, 6.490075588226318, 6.4852705001831055] min patience loss:6.48527050018 current loss:6.48203521967 absolute loss difference:0.00323528051376\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 720, \"sum\": 720.0, \"min\": 720}, \"Total Records Seen\": {\"count\": 1, \"max\": 183060, \"sum\": 183060.0, \"min\": 183060}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 180, \"sum\": 180.0, \"min\": 180}}, \"EndTime\": 1610407245.612137, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 89}, \"StartTime\": 1610407245.580262}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=63542.4872337 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 91\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.638] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 272, \"duration\": 25, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 91 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.48030352592\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0873538469896\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.39294993877\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.48030352592\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=91, train total_loss <loss>=6.48030352592\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.508814334869385, 6.50604510307312, 6.498122692108154, 6.502184689044952, 6.498745918273926, 6.4907180070877075, 6.4916369915008545, 6.490075588226318, 6.4852705001831055, 6.482035219669342] min patience loss:6.48203521967 current loss:6.48030352592 absolute loss difference:0.00173169374466\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 728, \"sum\": 728.0, \"min\": 728}, \"Total Records Seen\": {\"count\": 1, \"max\": 185094, \"sum\": 185094.0, \"min\": 185094}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 182, \"sum\": 182.0, \"min\": 182}}, \"EndTime\": 1610407245.643685, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 90}, \"StartTime\": 1610407245.612386}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=64758.4567668 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 92\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.670] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 275, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 92 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.47915756702\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0861009610817\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.39305657148\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.47915756702\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=92, train total_loss <loss>=6.47915756702\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.50604510307312, 6.498122692108154, 6.502184689044952, 6.498745918273926, 6.4907180070877075, 6.4916369915008545, 6.490075588226318, 6.4852705001831055, 6.482035219669342, 6.480303525924683] min patience loss:6.48030352592 current loss:6.47915756702 absolute loss difference:0.00114595890045\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 736, \"sum\": 736.0, \"min\": 736}, \"Total Records Seen\": {\"count\": 1, \"max\": 187128, \"sum\": 187128.0, \"min\": 187128}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 184, \"sum\": 184.0, \"min\": 184}}, \"EndTime\": 1610407245.674439, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 91}, \"StartTime\": 1610407245.643898}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=66371.1457779 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 93\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.701] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 278, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 93 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.47549051046\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.090723413974\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.38476711512\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.47549051046\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=93, train total_loss <loss>=6.47549051046\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.498122692108154, 6.502184689044952, 6.498745918273926, 6.4907180070877075, 6.4916369915008545, 6.490075588226318, 6.4852705001831055, 6.482035219669342, 6.480303525924683, 6.479157567024231] min patience loss:6.47915756702 current loss:6.47549051046 absolute loss difference:0.00366705656052\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 744, \"sum\": 744.0, \"min\": 744}, \"Total Records Seen\": {\"count\": 1, \"max\": 189162, \"sum\": 189162.0, \"min\": 189162}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 186, \"sum\": 186.0, \"min\": 186}}, \"EndTime\": 1610407245.70704, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 92}, \"StartTime\": 1610407245.674688}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=62659.9462068 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 94\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.735] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 281, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 94 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.47129273415\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0893879877403\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.38190478086\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.47129273415\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=94, train total_loss <loss>=6.47129273415\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.502184689044952, 6.498745918273926, 6.4907180070877075, 6.4916369915008545, 6.490075588226318, 6.4852705001831055, 6.482035219669342, 6.480303525924683, 6.479157567024231, 6.475490510463715] min patience loss:6.47549051046 current loss:6.47129273415 absolute loss difference:0.0041977763176\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 752, \"sum\": 752.0, \"min\": 752}, \"Total Records Seen\": {\"count\": 1, \"max\": 191196, \"sum\": 191196.0, \"min\": 191196}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 188, \"sum\": 188.0, \"min\": 188}}, \"EndTime\": 1610407245.741817, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 93}, \"StartTime\": 1610407245.707252}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=58616.4524299 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 95\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.770] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 284, \"duration\": 27, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 95 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.46843236685\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0876920539886\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.38074028492\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.46843236685\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=95, train total_loss <loss>=6.46843236685\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.498745918273926, 6.4907180070877075, 6.4916369915008545, 6.490075588226318, 6.4852705001831055, 6.482035219669342, 6.480303525924683, 6.479157567024231, 6.475490510463715, 6.471292734146118] min patience loss:6.47129273415 current loss:6.46843236685 absolute loss difference:0.00286036729813\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 760, \"sum\": 760.0, \"min\": 760}, \"Total Records Seen\": {\"count\": 1, \"max\": 193230, \"sum\": 193230.0, \"min\": 193230}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 190, \"sum\": 190.0, \"min\": 190}}, \"EndTime\": 1610407245.774856, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 94}, \"StartTime\": 1610407245.742115}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=61840.5591388 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 96\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.801] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 287, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 96 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.46559226513\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0886815004051\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.3769108057\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.46559226513\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=96, train total_loss <loss>=6.46559226513\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.4907180070877075, 6.4916369915008545, 6.490075588226318, 6.4852705001831055, 6.482035219669342, 6.480303525924683, 6.479157567024231, 6.475490510463715, 6.471292734146118, 6.468432366847992] min patience loss:6.46843236685 current loss:6.46559226513 absolute loss difference:0.0028401017189\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 768, \"sum\": 768.0, \"min\": 768}, \"Total Records Seen\": {\"count\": 1, \"max\": 195264, \"sum\": 195264.0, \"min\": 195264}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 192, \"sum\": 192.0, \"min\": 192}}, \"EndTime\": 1610407245.806476, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 95}, \"StartTime\": 1610407245.775128}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=64614.2580718 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 97\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.833] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 290, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 97 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.46369022131\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0925294738263\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.37116092443\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.46369022131\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=97, train total_loss <loss>=6.46369022131\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.4916369915008545, 6.490075588226318, 6.4852705001831055, 6.482035219669342, 6.480303525924683, 6.479157567024231, 6.475490510463715, 6.471292734146118, 6.468432366847992, 6.465592265129089] min patience loss:6.46559226513 current loss:6.46369022131 absolute loss difference:0.00190204381943\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 776, \"sum\": 776.0, \"min\": 776}, \"Total Records Seen\": {\"count\": 1, \"max\": 197298, \"sum\": 197298.0, \"min\": 197298}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 194, \"sum\": 194.0, \"min\": 194}}, \"EndTime\": 1610407245.838653, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 96}, \"StartTime\": 1610407245.806709}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=63396.1086126 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 98\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.864] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 293, \"duration\": 25, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 98 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.46288830042\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.093824153766\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.36906415224\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.46288830042\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=98, train total_loss <loss>=6.46288830042\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.490075588226318, 6.4852705001831055, 6.482035219669342, 6.480303525924683, 6.479157567024231, 6.475490510463715, 6.471292734146118, 6.468432366847992, 6.465592265129089, 6.463690221309662] min patience loss:6.46369022131 current loss:6.46288830042 absolute loss difference:0.000801920890808\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 784, \"sum\": 784.0, \"min\": 784}, \"Total Records Seen\": {\"count\": 1, \"max\": 199332, \"sum\": 199332.0, \"min\": 199332}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 196, \"sum\": 196.0, \"min\": 196}}, \"EndTime\": 1610407245.868978, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 97}, \"StartTime\": 1610407245.838914}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=67355.2371388 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 99\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.895] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 296, \"duration\": 26, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 99 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.45905387402\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0939803309739\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.36507356167\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.45905387402\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=99, train total_loss <loss>=6.45905387402\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.4852705001831055, 6.482035219669342, 6.480303525924683, 6.479157567024231, 6.475490510463715, 6.471292734146118, 6.468432366847992, 6.465592265129089, 6.463690221309662, 6.462888300418854] min patience loss:6.46288830042 current loss:6.45905387402 absolute loss difference:0.00383442640305\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 792, \"sum\": 792.0, \"min\": 792}, \"Total Records Seen\": {\"count\": 1, \"max\": 201366, \"sum\": 201366.0, \"min\": 201366}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 198, \"sum\": 198.0, \"min\": 198}}, \"EndTime\": 1610407245.90024, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 98}, \"StartTime\": 1610407245.869246}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=65374.7927998 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] \u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Starting training for epoch 100\u001b[0m\n",
      "\u001b[34m[2021-01-11 23:20:45.929] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 299, \"duration\": 28, \"num_examples\": 8, \"num_bytes\": 505236}\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] # Finished training epoch 100 on 2034 examples from 8 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) total: 6.45743685961\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) kld: 0.0919363759458\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) recons: 6.36550045013\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Loss (name: value) logppx: 6.45743685961\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #quality_metric: host=algo-1, epoch=100, train total_loss <loss>=6.45743685961\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] patience losses:[6.482035219669342, 6.480303525924683, 6.479157567024231, 6.475490510463715, 6.471292734146118, 6.468432366847992, 6.465592265129089, 6.463690221309662, 6.462888300418854, 6.459053874015808] min patience loss:6.45905387402 current loss:6.45743685961 absolute loss difference:0.00161701440811\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Total Batches Seen\": {\"count\": 1, \"max\": 800, \"sum\": 800.0, \"min\": 800}, \"Total Records Seen\": {\"count\": 1, \"max\": 203400, \"sum\": 203400.0, \"min\": 203400}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2034, \"sum\": 2034.0, \"min\": 2034}, \"Reset Count\": {\"count\": 1, \"max\": 200, \"sum\": 200.0, \"min\": 200}}, \"EndTime\": 1610407245.933295, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 99}, \"StartTime\": 1610407245.900489}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] #throughput_metric: host=algo-1, train throughput=61738.9698803 records/second\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 WARNING 140226321442624] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Best model based on early stopping at epoch 100. Best loss: 6.45743685961\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Topics from epoch:final (num_topics:5) [wetc 0.25, tu 0.74]:\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] [0.24, 0.66] viewer card format vesa window pointer mpeg anybody hello compiled converter advance vga hi pc animation library shareware byte driver\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] [0.26, 0.64] window advance format gif appreciated hi unix vga file graphic shareware package mail utility ftp anybody email card rgb looking\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] [0.19, 0.59] mpeg version package output window appreciated unix pov rayshade shareware anybody running hi format ftp converter converted and/or appreciate wondering\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] [0.37, 1.00] muslim islam absolute moral temp subjective faith jesus objective definition morality atheism believer god existence statement contradictory belief reject cview\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] [0.21, 0.81] gifs appreciate greatly wanted joke message animation update thanks pov iii faq wondering ftp library server database faster searching unix\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Saved checkpoint to \"/tmp/tmpnnmJrj/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[01/11/2021 23:20:45 INFO 140226321442624] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 24956.610918045044, \"sum\": 24956.610918045044, \"min\": 24956.610918045044}, \"finalize.time\": {\"count\": 1, \"max\": 35.65192222595215, \"sum\": 35.65192222595215, \"min\": 35.65192222595215}, \"initialize.time\": {\"count\": 1, \"max\": 21414.844036102295, \"sum\": 21414.844036102295, \"min\": 21414.844036102295}, \"model.serialize.time\": {\"count\": 1, \"max\": 3.2939910888671875, \"sum\": 3.2939910888671875, \"min\": 3.2939910888671875}, \"setuptime\": {\"count\": 1, \"max\": 90.14606475830078, \"sum\": 90.14606475830078, \"min\": 90.14606475830078}, \"early_stop.time\": {\"count\": 100, \"max\": 5.7849884033203125, \"sum\": 335.21318435668945, \"min\": 0.14901161193847656}, \"update.time\": {\"count\": 100, \"max\": 46.80991172790527, \"sum\": 3334.1140747070312, \"min\": 27.294158935546875}, \"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}}, \"EndTime\": 1610407245.973661, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1610407221.13658}\n",
      "\u001b[0m\n",
      "\n",
      "2021-01-11 23:20:58 Uploading - Uploading generated training model\n",
      "2021-01-11 23:20:58 Completed - Training job completed\n",
      "Training seconds: 85\n",
      "Billable seconds: 85\n"
     ]
    }
   ],
   "source": [
    "ntm.fit(inputs={'train': s3_train_path,\n",
    "                'auxiliary': s3_aux_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge',serializer = sagemaker.serializers.IdentitySerializer('application/x-recordio-protobuf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = vectorizer.transform(newsgroups_test.data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.space \n",
      "\n",
      " TRry the SKywatch project in  Arizona. \n",
      "\n",
      "comp.graphics \n",
      "\n",
      " The Vatican library recently made a tour of the US.\n",
      " Can anyone help me in finding a FTP site where this collection is \n",
      " available. \n",
      "\n",
      "comp.graphics \n",
      "\n",
      " Hi there,\n",
      "\n",
      "I am here looking for some help.\n",
      "\n",
      "My friend is a interior decor designer. He is from Thailand. He is\n",
      "trying to find some graphics software on PC. Any suggestion on which\n",
      "software to buy,where to buy and how much it costs ? He likes the most\n",
      "sophisticated \n",
      "software(the more features it has,the better) \n",
      "\n",
      "comp.graphics \n",
      "\n",
      " RFD\n",
      "                          Request For Discussion\n",
      "                                for the\n",
      "                          OPEN  TELEMATIC GROUP\n",
      "\n",
      "                                  OTG\n",
      "\n",
      "I have proposed the forming of a consortium/task force for the\n",
      "promotion of NAPLPS/JPEG, FIF to openly discuss ways, method,\n",
      "procedures,algorythms, applications, implementation, extensions of\n",
      "NAPLPS/JPEG standards.  These standards should facilitate the creation\n",
      "of REAL_TIME Online applications that make use of Voice, Video,\n",
      "Telecommuting, HiRes graphics, Conferencing, Distant Learning, Online\n",
      "order entry, Fax,in addition these dicussion would assist all to\n",
      "better understand how SGML, CALS, ODA, MIME, OODBMS, JPEG, MPEG,\n",
      "FRACTALS, SQL, CDrom, cdromXA, Kodak PhotoCD, TCL, V.FAST, and\n",
      "EIA/TIA562, can best be incorporated and implemented to develop\n",
      "TELEMATIC/Multimedia applications.\n",
      "\n",
      "We want to be able to support DOS, UNIX, MAC, WINDOWS, NT, OS/2\n",
      "platforms.  It is our hope that individuals, developers, corporations,\n",
      "Universities, R & D labs would join in in supporting such an endeavor.\n",
      "\n",
      "This would be a NOT_FOR_PROFIT group with bylaws and charter. Already\n",
      "many corporations have decided to support OTG (Open TELEMATIC Group) so\n",
      "do not delay joining if you are a developer\n",
      "\n",
      "An RFD has been posted to form a usenet newsgroup and a FAQ will soon\n",
      "be be composed to start promulgating what is known on the subject.  If\n",
      "you would like to be added to the maillist send email or mail to the\n",
      "address below.\n",
      "\n",
      "This group would publish an electronic quarterly NAPLPS/JPEG\n",
      "newsletter as well as a hardcopy version.  We urge all who wants to\n",
      "see CMCs HiRes based applications & the NAPLPS/JPEG G R O W, decide to\n",
      "join and mutually benefit from this NOT-FOR_PROFIT endeavor.\n",
      "\n",
      "NOTE: Telematic has been defined by Mr. James Martin as the marriage\n",
      "      of Voice, Video, Hi-res Graphics, Fax, IVR, Music over telephone\n",
      "      lines/LAN.\n",
      "\n",
      "If you would like to get involve write to me at:\n",
      "\n",
      "  IMG Inter-Multimedia Group| Internet:  epimntl@world.std.com\n",
      "  P.O. Box 95901            |            ed.pimentel@gisatl.fidonet.org\n",
      "  Atlanta, Georgia, US      | CIS     :  70611,3703\n",
      "                            | FidoNet :  1:133/407\n",
      "                            | BBS     :  +1-404-985-1198 zyxel 14.4k \n",
      "\n",
      "comp.graphics \n",
      "\n",
      " I am interested in finding 3D animation programs for the Mac.\n",
      "I am especially interested in any programs that don't exist\n",
      "in a PC port and are so good that they would make me go buy\n",
      "a Mac.  Do any such exist? \n",
      "\n",
      "comp.graphics \n",
      "\n",
      " \n",
      "\n",
      "I'm also interested in such a program. But most of all I'd like to know \n",
      "wich program is able to convert GIF or PCX to DXF !!! When I have this \n",
      "program, I can scan pictures and frase (or something like that !) them.\n",
      "This will be beyond the limit !!! \n",
      "\n",
      "sci.space \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Or how about:\n",
      "    \"End light pollution now!!\"\n",
      "\n",
      "Your banner would have no effect on its subject, but my banner would.\n",
      " \n",
      "\n",
      "sci.space \n",
      "\n",
      " : While I'm sure Sagan considers it sacrilegious, that wouldn't be\n",
      ": because of his doubtfull credibility as an astronomer. Modern, \n",
      ": ground-based, visible light astronomy (what these proposed\n",
      ": orbiting billboards would upset) is already a dying field: The\n",
      ": opacity and distortions caused by the atmosphere itself have\n",
      ": driven most of the field to use radio, far infrared or space-based\n",
      ": telescopes.\n",
      "\n",
      "Hardly.  The Keck telescope in Hawaii has taken its first pictures; they're\n",
      "nearly as good as Hubble for a tiny fraction of the cost.\n",
      "\n",
      ": In any case, a bright point of light passing through\n",
      ": the field doesn't ruin observations. If that were the case, the\n",
      ": thousands of existing satellites would have already done so (satelliets\n",
      ": might not seem so bright to the eyes, but as far as astronomy is concerned,\n",
      ": they are extremely bright.)\n",
      "\n",
      "I believe that this orbiting space junk will be FAR brighter still;\n",
      "more like the full moon.  The moon upsets deep-sky observation all\n",
      "over the sky (and not just looking at it) because of scattered light.\n",
      "\n",
      "This is a known problem, but of course two weeks out of every four are\n",
      "OK.  What happens when this billboard circles every 90 minutes?  What\n",
      "would be a good time then?\n",
      "\n",
      ":                                              Frank Crary\n",
      ":                                              CU Boulder\n",
      " \n",
      "\n",
      "alt.atheism \n",
      "\n",
      " \n",
      "Not if you show that these hypothetical atheists are gullible, excitable\n",
      "and easily led from some concrete cause.   In that case we would also\n",
      "have to discuss if that concrete cause, rather than atheism, was the\n",
      "factor that caused their subsequent behaviour. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(newsgroups_test.target_names[newsgroups_test.target[i]],'\\n\\n',newsgroups_test.data[i],'\\n') for i in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buf = io.BytesIO()\n",
    "smac.write_spmatrix_to_sparse_tensor(buf, test_vectors, None)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ntm_predictor.predict(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = json.loads(response.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21096699 0.18487486 0.21297348 0.1556932  0.23549148]\n",
      " [0.1494506  0.54590786 0.14265324 0.05187792 0.11011035]\n",
      " [0.13456063 0.57404399 0.13980454 0.03729982 0.11429098]\n",
      " [0.1329252  0.48300558 0.1318415  0.15262295 0.09960476]\n",
      " [0.14594652 0.5330019  0.14863576 0.04931984 0.12309601]\n",
      " [0.1346754  0.53007102 0.11470658 0.14414591 0.07640111]\n",
      " [0.19331904 0.16369125 0.19723308 0.22267568 0.22308101]\n",
      " [0.19968891 0.18436807 0.20451064 0.19965564 0.21177676]\n",
      " [0.09354258 0.07975934 0.0996239  0.6138199  0.11325429]\n",
      " [0.19406945 0.1759121  0.19490488 0.23710455 0.19800907]]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array([prediction['topic_weights'] for prediction in response['predictions']])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Topic ID')"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fs = 12\n",
    "df=pd.DataFrame(predictions)\n",
    "df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "plt.xlabel('Topic ID', fontsize=fs+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Speech and Language Processing, by Dan Jurafsky and James H. Martin (free PDF)\n",
    "* Natural Language Processing with PyTorch by Brian McMahan and Delip Rao \n",
    "* Fastai NLP Course\n",
    "* Sebastian Ruder's blog\n",
    "* Stephen Merity blog\n",
    "* Rachael Tatman blog\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
